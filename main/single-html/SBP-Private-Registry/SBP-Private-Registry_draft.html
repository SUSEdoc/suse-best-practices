<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>SUSE Private Registry Powered by Harbor 2.1</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.2" /><meta name="description" content="This guide provides instructions how to deploy and maintain a private container registry using Harbor 2.1. Disclaimer: Documents published as part of the SUSE Best Practices series have been contributed voluntarily by SUSE employees and third parties. They are meant to serve as examples of how particular actions can be performed. They have been compiled with utmost attention to detail. However, this does not guarantee complete accuracy. SUSE cannot verify that actions described in these documents do what is claimed or whether actions described have unintended consequences. SUSE LLC, its affiliates, the authors, and the translators may not be held liable for possible errors or the consequences thereof." /><style type="text/css">
body { background-image: url('static/images/draft.png');
       background-repeat: no-repeat;
       background-position: top left;
       /* The following properties make the watermark "fixed" on the page. */
       /* I think that's just a bit too distracting for the reader... */
       /* background-attachment: fixed; */
       /* background-position: center center; */
     }</style></head><body onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div xml:lang="en" class="article" lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="id1337"></a>SUSE Private Registry Powered by Harbor 2.1</h2></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><p>This guide provides instructions how to deploy and maintain a private container registry using Harbor 2.1.</p><p>
        <span class="strong"><strong>Disclaimer: </strong></span>
        Documents published as part of the SUSE Best Practices series have been contributed voluntarily
        by SUSE employees and third parties. They are meant to serve as examples of how particular
        actions can be performed. They have been compiled with utmost attention to detail. However,
        this does not guarantee complete accuracy. SUSE cannot verify that actions described in these
        documents do what is claimed or whether actions described have unintended consequences.
        SUSE LLC, its affiliates, the authors, and the translators may not be held liable for possible errors
        or the consequences thereof.
    </p></div></div></div><hr /></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#id-introduction">Introduction</a></span></dt><dt><span class="section"><a href="#requirements">Requirements</a></span></dt><dd><dl><dt><span class="section"><a href="#id-kubernetes-distribution">Kubernetes Distribution</a></span></dt><dt><span class="section"><a href="#id-helm">Helm</a></span></dt><dt><span class="section"><a href="#id-storage">Storage</a></span></dt><dt><span class="section"><a href="#requirements-ingress">Ingress</a></span></dt><dt><span class="section"><a href="#requirements-tls">TLS Certificates</a></span></dt><dt><span class="section"><a href="#requirements-external-postgres">External PostgreSQL Database</a></span></dt><dt><span class="section"><a href="#requirements-redis-external">External Redis</a></span></dt></dl></dd><dt><span class="section"><a href="#high-availability">High Availability</a></span></dt><dt><span class="section"><a href="#id-deployment-of-suse-private-registry">Deployment of SUSE Private Registry</a></span></dt><dd><dl><dt><span class="section"><a href="#id-preparation">Preparation</a></span></dt><dt><span class="section"><a href="#id-installation-steps">Installation Steps</a></span></dt></dl></dd><dt><span class="section"><a href="#id-deployment-in-an-air-gapped-environment">Deployment in an Air-Gapped Environment </a></span></dt><dd><dl><dt><span class="section"><a href="#id-introduction-2">Introduction</a></span></dt><dt><span class="section"><a href="#id-use-cases">Use Cases</a></span></dt><dt><span class="section"><a href="#id-requirements">Requirements</a></span></dt><dt><span class="section"><a href="#id-deployment">Deployment</a></span></dt></dl></dd><dt><span class="section"><a href="#troubleshooting">Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#id-possible-problems">Possible problems</a></span></dt></dl></dd><dt><span class="section"><a href="#install-tls-security">Transport Layer Security (TLS) Setup</a></span></dt><dt><span class="section"><a href="#administration">Administration</a></span></dt><dd><dl><dt><span class="section"><a href="#id-setup-administration-access">Setup Administration Access</a></span></dt><dt><span class="section"><a href="#admin-configure-authentication">Configuring Authentication</a></span></dt><dt><span class="section"><a href="#id-registry-deployment-configuration-changes">Registry Deployment Configuration Changes</a></span></dt><dt><span class="section"><a href="#id-installing-maintenance-updates">Installing Maintenance Updates</a></span></dt><dt><span class="section"><a href="#id-backup-and-restore">Backup and Restore</a></span></dt></dl></dd><dt><span class="section"><a href="#user-guide">User Guide</a></span></dt><dd><dl><dt><span class="section"><a href="#id-accessing-landing-page">Accessing Landing Page</a></span></dt><dt><span class="section"><a href="#id-using-docker-for-managing-images">Using docker for managing images</a></span></dt><dt><span class="section"><a href="#id-manage-helm-charts-with-suse-private-registry">Manage Helm Charts with SUSE Private Registry</a></span></dt><dt><span class="section"><a href="#id-using-suse-private-registry-in-the-kubernetes-cluster">Using SUSE Private Registry in the Kubernetes Cluster</a></span></dt><dt><span class="section"><a href="#id-vulnerability-scanning">Vulnerability Scanning</a></span></dt><dt><span class="section"><a href="#id-image-signing">Image Signing</a></span></dt><dt><span class="section"><a href="#id-using-suse-private-registry-as-a-proxy-cache">Using SUSE Private Registry as a Proxy Cache</a></span></dt></dl></dd><dt><span class="section"><a href="#id-appendix">Appendix</a></span></dt><dd><dl><dt><span class="section"><a href="#id-glossary">Glossary</a></span></dt><dt><span class="section"><a href="#id-limitations">Limitations</a></span></dt><dt><span class="section"><a href="#id-supported-deployment-scenarios">Supported Deployment Scenarios</a></span></dt><dt><span class="section"><a href="#id-supported-features">Supported Features</a></span></dt><dt><span class="section"><a href="#id-authors">Authors</a></span></dt></dl></dd><dt><span class="section"><a href="#id-legal-notice">Legal Notice</a></span></dt><dt><span class="section"><a href="#id-gnu-free-documentation-license">GNU Free Documentation License</a></span></dt></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-introduction"></a>Introduction</h2></div></div></div><p>Cloud Native Application Development and Delivery methods require a registry and a CI/CD (Continuous Integration/Delivery) instance for the development and production of container images.
In many cases, using a public container registry and CI/CD does not allow for enough control. A <a class="link" href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry" target="_top">private registry</a> can provide that level of control.</p><p>SUSE Private Registry Powered by Harbor 2.1 is a fully OCI-compliant container-image registry solution that gives the development team a place to manage, deploy and sign container images (using <a class="link" href="https://docs.docker.com/notary/getting_started/" target="_top">Notary</a>), fully control access to OCI container images and helm charts via RBAC (role-based access control), and do vulnerability scanning with <a class="link" href="https://github.com/aquasecurity/trivy" target="_top">Trivy</a>.
It is based on <a class="link" href="https://goharbor.io/docs/2.1.0/" target="_top">Harbor</a> on top of SUSE Linux Enterprise Server.</p><p>See <a class="xref" href="#supported-features" title="Table 2. Supported Features">Table 2, “Supported Features”</a> for a complete list of supported features.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="requirements"></a>Requirements</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-kubernetes-distribution"></a>Kubernetes Distribution</h3></div></div></div><p>SUSE Private Registry Powered by Harbor 2.1 can only be deployed on top of a Kubernetes cluster.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Kubernetes 1.17 or higher</p></li><li class="listitem"><p>SUSE CaaS Platform 4.5.x (or higher)</p></li><li class="listitem"><p>K3s</p></li><li class="listitem"><p>RKE2</p><p>or</p></li><li class="listitem"><p>A supported public Kubernetes cloud provider (see <a class="xref" href="#supported-deployment" title="Table 1. Supported deployment options">Table 1, “Supported deployment options”</a> Options)</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-helm"></a>Helm</h3></div></div></div><p>SUSE Private Registry can only be deployed via the Helm chart.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Helm 3.2 (or higher) required</p><p>Use <a class="link" href="https://software.opensuse.org/package/helm" target="_top">https://software.opensuse.org/package/helm</a> to find a build for your distribution.</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-storage"></a>Storage</h3></div></div></div><p>The following types of storage are required for SUSE Private Registry Powered by Harbor 2.1:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Kubernetes persistent volumes</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>SUSE Private Registry internal stateful components require persistent volumes to store their data in a way that survives pod restarts.
A default Kubernetes <code class="literal">StorageClass</code> is needed to dynamically provision the volumes. Alternatively, explicit <code class="literal">StorageClass</code> values may be configured for each component.</p></li></ul></div></li><li class="listitem"><p>Kubernetes persistent shared volumes</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>In addition to regular persistent volumes, a Kubernetes <code class="literal">StorageClass</code> that supports <code class="literal">ReadWriteMany</code> access mode is required to deploy the <code class="literal">registry</code> component - the component providing the OCI registry API and responsible for storing OCI artifacts - in a highly-available and scalable setup. If such a <code class="literal">StorageClass</code> isn’t available in your Kubernetes cluster, an external storage backend may be used instead (see next point). Ultimately, a regular <code class="literal">ReadWriteOnce</code> Kubernetes <code class="literal">StorageClass</code> may still be used for the <code class="literal">registry</code> component, but with HA and scalability features selectively disabled.</p></li></ul></div></li><li class="listitem"><p>External storage for OCI artifacts</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>SUSE Private Registry may optionally be configured to store OCI artifacts (e.g. container images and charts) in an external storage backend such as Azure Blob Storage or Amazon S3, instead of the Kubernetes provided persistent volumes.</p></li></ul></div></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>SUSE CaaS Platform 4.5 provides only very limited options of supported <code class="literal">StorageClass</code> configurations.
Refer to <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_storage.html" target="_top">SUSE CaaS Platform 4.5 Administration Guide: Storage</a>.</p></div><p>For public cloud deployments it is recommended to use the available hosted solutions:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Azure Blob Storage or Amazon S3 for storing OCI artifacts</p></li><li class="listitem"><p>The Azure File Share or Amazon EFS Kubernetes <code class="literal">StorageClass</code> for the <code class="literal">registry</code> component, unless external storage is used to store OCI artifacts</p></li><li class="listitem"><p>The Azure Managed Disk or Amazon EBS Kubernetes <code class="literal">StorageClass</code> for other components that require persistent storage</p></li></ul></div><p>Refer to the table in <a class="xref" href="#supported-deployment" title="Table 1. Supported deployment options">Table 1, “Supported deployment options”</a> for more details.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="requirements-ingress"></a>Ingress</h3></div></div></div><p>The default and recommended way of installing SUSE Private Registry using the helm chart is to employ an ingress service to expose the Harbor service. The Kubernetes cluster where the chart is being deployed needs to have an ingress controller enabled:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>For SUSE CaaS Platform v4.5, check how to deploy <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-admin/#nginx-ingress" target="_top">Nginx based Ingress controller with SUSE CaaS Platform</a>.</p></li><li class="listitem"><p>For K3s, traefik ingress controller is deployed by default. It is also possible to disable traefik when installing K3s and install different ingress controller, e.g. Nginx based Ingress controller the same way as for SUSE CaaS Platform.</p></li><li class="listitem"><p>For AKS, the Ingress controller recommended for SUSE Private Registry is the NGINX ingress controller.
Follow the documentation on <a class="link" href="https://docs.microsoft.com/en-us/azure/aks/ingress-basic" target="_top">Creating an Ingress Controller in AKS</a>, or <a class="link" href="https://docs.microsoft.com/en-us/azure/aks/ingress-tls" target="_top">Creating an HTTPS ingress controller on AKS</a>.
The latter also provides extended instructions about providing support for resolvable FQDNs and generating TLS certificates, which are also SUSE Private Registry requirements.</p></li><li class="listitem"><p>For EKS, the Ingress controller recommended for SUSE Private Registry is the ALB ingress controller. Follow the documentation on <a class="link" href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html" target="_top">Creating an ALB Ingress Controller on EKS</a>.</p></li></ul></div><p>Alternatively, the Harbor service may be exposed directly via a dedicated Kubernetes LoadBalancer, without requiring an Ingress controller.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-resolvable-fqdns"></a>Resolvable FQDNs</h4></div></div></div><p>If the Kubernetes Ingress is the chosen option for exposing the SUSE Private Registry services, it is also required to associate FQDN entries with the exposed SUSE Private Registry endpoints: one FQDN for the Harbor UI/API and another one for the image signing (notary) service, if enabled.</p><p>Both these FQDNs need to be resolvable to the external IP address allocated for the Kubernetes Ingress controller and they must not have the same value. Setting this up depends largely on the host infrastructure and the Kubernetes distribution on top of which SUSE Private Registry is installed. Providing complete instructions on allocating FQDNs and configuring external DNS services to resolve them is outside the scope of this document, but some suggestions are provided in this section.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-nip-io"></a>nip.io</h5></div></div></div><p>For simple deployments, an external public service such as <a class="link" href="https://nip.io" target="_top">nip.io</a> may be used to provide quick DNS mapping for IP addresses without requiring additional services or infrastructure configuration changes. For example, if the external IP address allocated to the Kubernetes Ingress Controller is 10.86.0.237, then the FQDNs used for the Harbor UI/API and notary endpoints might be:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>harbor.10.86.0.237.nip.io</p></li><li class="listitem"><p>notary.10.86.0.237.nip.io</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-azure-aks-use-a-dns-zone"></a>Azure AKS - Use a DNS Zone</h5></div></div></div><p>For AKS, a DNS zone may be configured and used as a custom domain for all FQDNs associated with the Ingress.
Follow the steps documented in <a class="link" href="https://docs.microsoft.com/en-us/azure/aks/ingress-tls" target="_top">Creating an HTTPS ingress controller on AKS</a> on associating an Azure DNS zone with the external IP allocated for an Ingress Controller.</p><p>For example, if the Azure DNS zone mapped to the external IP allocated to the Kubernetes Ingress controller is <code class="literal">MY_CUSTOM_DOMAIN</code>, then the FQDNs used for the Harbor UI/API and notary endpoints might be:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>harbor.MY_CUSTOM_DOMAIN</p></li><li class="listitem"><p>notary.MY_CUSTOM_DOMAIN</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-amazon-eks-use-the-alb-ingress-controller"></a>Amazon EKS - Use the ALB Ingress Controller</h5></div></div></div><p>For EKS, the ALB ingress controller already includes a DNS zone that is automatically used to derive FQDNs for every Ingress instance.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="requirements-tls"></a>TLS Certificates</h3></div></div></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Detailed instructions on generating TLS certificates are outside the scope of this document, but some suggestions are provided in the installation section.</p></div><p>By default, SUSE Private Registry is configured to auto-generate TLS certificates associated with the Harbor UI/API and Notary API FQDNs, independently of the way used to expose the services. While this option simplifies the installation process, it also uses self-signed CA certificates that must be explicitly installed on every machine where clients will interact with the registry. For situation when this is not desirable, such as production deployments, custom certificates need to be generated ahead of time and provided during installation.</p><p>The custom certificates must reflect the FQDNs, domain name, or external IP address that will be used by clients to access the SUSE Private Registry services. For example, if a Kubernetes Ingress is used, one TLS certificate will be required for the Harbor UI/API FQDN and another one for the Notary FQDN. The second option is to create a single certificate to be used for both services, with the SAN value configured to match both FQDNs. A third option is to create a single certificate for the entire subdomain used to derive both FQDNs.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-kubernetes-cert-manager"></a>Kubernetes cert-manager</h4></div></div></div><p>The <a class="link" href="https://cert-manager.io/" target="_top">cert-manager</a> open source solution could be leveraged to manage certificates.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-nginx-ingress-controller"></a>NGINX Ingress Controller</h4></div></div></div><p>This type of Kubernetes Ingress Controller supports configuring a default SSL certificate that may be used in common for all services that use it.
Detailed instructions on how to do that are available in <a class="link" href="https://kubernetes.github.io/ingress-nginx/user-guide/tls/" target="_top">the official NGINX Ingress Controller documentation</a>.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-eks-aws-certificate-manager"></a>EKS - AWS Certificate Manager</h4></div></div></div><p>AWS includes an <a class="link" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html" target="_top">AWS Certificate Manager</a> service that can be used to manage certificates.
The ALB Ingress Controller also supports configuring a default SSL certificate associated with the underlying Application Load Balancer.
Detailed instructions on how to do that are available in the <a class="link" href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates" target="_top">Creating an HTTPS Listener for an ALB</a> documentation.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="requirements-external-postgres"></a>External PostgreSQL Database</h3></div></div></div><p>An internal database is provided with the SUSE Private Registry helm chart, but this builtin service is not highly available nor scalable.
For production deployments running in public cloud, it is recommended that a highly-available and scalable managed PostgreSQL database instance be created and configured as an external database for SUSE Private Registry.
This section covers only high-level instructions on creating a managed PostgreSQL database instance for Azure and AWS.
For detailed instructions, please refer to the official documentation:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Azure: <a class="link" href="https://docs.microsoft.com/en-us/azure/postgresql/" target="_top">Azure Database for PostgreSQL</a></p></li><li class="listitem"><p>AWS: <a class="link" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html" target="_top">PostgreSQL on Amazon RDS</a></p></li></ul></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-azure-postgresql"></a>Azure PostgreSQL</h4></div></div></div><p>To deploy Azure PosgreSQL server, use <code class="literal">az</code> command line client like this:</p><pre class="screen">az postgres server create --location germanywestcentral --resource-group &lt;azure-resource-group&gt; --name &lt;azure-postgres-server&gt; --admin-user &lt;admin-user&gt; --admin-password &lt;admin-password&gt; --sku-name B_Gen5_2</pre><p>Alternatively, the database server can be created from the Azure Portal page.</p><p>After the server creation, it is necessary to manually create the following empty databases: <code class="literal">registry</code>, <code class="literal">notary_server</code> and <code class="literal">notary_signer</code>.</p><p>This can be achieved using the <code class="literal">az</code> command line client like this:</p><pre class="screen">az postgres db create --resource-group &lt;azure-resource-group&gt; --server-name &lt;azure-postgres-server&gt; -name registry
az postgres db create --resource-group &lt;azure-resource-group&gt; --server-name &lt;azure-postgres-server&gt; -name notary_server
az postgres db create --resource-group &lt;azure-resource-group&gt; --server-name &lt;azure-postgres-server&gt; -name notary_signer</pre><p>Do not forget to set up correct firewall rules so that other services are able to access the database.</p><p>For example, one can setup a rule to enable access from all other Azure services using the command line client:</p><pre class="screen">az postgres server firewall-rule create --resource-group &lt;azure-resource-group&gt; --server-name &lt;azure-postgres-server&gt; -n azure-services-rule --start-ip-address 0.0.0.0 --end-ip-address 0.0.0.0</pre><p>When accessing the database from an on-premise Kubernetes cluster, set up the firewall rules accordingly.
You can adapt Firewall rules from Azure Portal as well.</p><p>Find out the URL of the database as well as the username and password that was used when creating the database server.</p><p>For Azure, the host name will likely look like <code class="literal">&lt;postgres-server&gt;.postgres.database.azure.com</code>.
These will be required during the SUSE Private Registry installation. You can also find it in the Azure Portal in the <code class="literal">Connection Strings</code> section.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-aws-rds"></a>AWS RDS</h4></div></div></div><p>SUSE Private Registry is compatible with Amazon RDS using the Amazon Aurora-PostgreSQL or PostgreSQL service.</p><p>The RDS database can be created from the AWS RDS Management Console or by using the <code class="literal">aws</code> cli using
the following command:</p><pre class="screen">aws rds create-db-instance
    --engine postgres \
    --allocated-storage 25 \
    --db-instance-class db.t3.medium \
    --db-security-groups &lt;mydbsecuritygroup&gt; \
    --db-subnet-group &lt;mydbsubnetgroup&gt; \
    --master-username &lt;masterawsuser&gt; \
    --master-user-password &lt;masteruserpassword&gt; \
    --backup-retention-period 3</pre><p>When accessing the database from an on-premise Kubernetes cluster, set up the firewall rules accordingly.
You can adapt Firewall rules from AWS Management Console as well.</p><p>Please record the URL of the database as well as the username and password that was used when creating the database server.</p><p>These will be required during the SUSE Private Registry installation.</p><p>After the server creation, it is necessary to manually create the following empty databases: <code class="literal">registry</code>, <code class="literal">notary_server</code> and <code class="literal">notary_signer</code>.
Use your favorite database client tool (like <code class="literal">psql</code>) to connect to the database server and create these databases.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="requirements-redis-external"></a>External Redis</h3></div></div></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Securing connections to the external Redis with TLS/SSL is currently not supported.</p></div><p>An internal Redis service is provided with the SUSE Private Registry helm chart, but this builtin service is not highly available nor scalable.
For production deployments running in public cloud, it is recommended that a highly-available and scalable managed Redis instance be created and configured as an external Redis for SUSE Private Registry.
This section covers only high-level instructions on creating a managed Redis instance for Azure and AWS.</p><p>For detailed instructions, please refer to the official documentation:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Azure: <a class="link" href="https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/" target="_top">Azure Cache for Redis</a></p></li><li class="listitem"><p>AWS: <a class="link" href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html" target="_top">Amazon ElastiCache for Redis</a>.</p></li></ul></div><p>As an alternative, the SUSE Redis operator may be used to deploy a Kubernetes managed Redis service and not rely on a public cloud managed Redis service.
This is covered in the SUSE Private Registry installation instructions.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="requirements-redis-azure"></a>Azure Cache for Redis</h4></div></div></div><p>To deploy Redis Cache in Azure, you can use the command line client and run it like this:</p><pre class="screen">az redis create --location &lt;azure-location&gt; --name &lt;azure-redis-cache&gt; --resource-group &lt;azure-resource-group&gt; --sku Basic --vm-size c0 --enable-non-ssl-port</pre><p>Use the right value with your resource group and location. The option for enabling non-ssl port is necessary, as Harbor does not support SSL connection to Redis.
Use the options for <code class="literal">sku</code> and <code class="literal">vm-size</code> options that fit your needs.
Using the name you will provide as <code class="literal">&lt;azure-redis-chache&gt;</code>, Azure will generate DNS name for your Redis Cache in the form of <code class="literal">&lt;azure-redis-cache&gt;.redis.cache.windows.net</code>.
Use this address for the <code class="literal">addr</code> key when adapting the <code class="literal">harbor-values.yaml</code> file later.
Refer to <a class="xref" href="#install-external-redis">[install-external-redis]</a> in the Deployment section.</p><p>Once the Redis Cache is created, obtain the access key that you will need for connection. List the access keys using the CLI this way:</p><pre class="screen">az redis list-keys --name &lt;azure-redis-cache&gt; --resource-group &lt;azure-resource-group&gt;</pre><p>Alternatively, the Redis Cache can be created from Azure Portal page. The access keys can be found in the Settings/Access keys section.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-amazon-elasticache-for-redis"></a>Amazon ElastiCache for Redis</h4></div></div></div><p>To deploy Redis Cache in AWS, you can use the aws command line client and run it like this:</p><pre class="screen">aws elasticache create-cache-cluster \
--cache-cluster-id my-cluster \
--cache-node-type cache.t3.small \
--engine redis \
--num-cache-nodes 1</pre><p>For enabling High Availability, the Redis Cache needs to be added to a ElastiCache cluster. Please
read the <a class="link" href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Clusters.Create.CLI.html" target="_top">AWS CLI documentation</a> for further details.</p><p>Refer to <a class="xref" href="#install-external-redis">[install-external-redis]</a> in the Deployment section.</p><p>Alternatively, the Redis Cache can be created from AWS Management Console.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="high-availability"></a>High Availability</h2></div></div></div><p>To install SUSE Private Registry with all high availability features fully enabled, or to be able to fully scale out an existing SUSE Private Registry deployment, the number of replicas for every registry component needs to be configured to 2 or more.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Use a ReplicaCount value of 2 or higher</p><p>The replica count can be configured in the Helm chart individually for every component.</p></li><li class="listitem"><p>HA Ingress Controller</p><p>If a Kubernetes Ingress controller is used to expose the SUSE Private Registry services, use a replica count value of 2 or higher for the ingress controller deployment.</p></li><li class="listitem"><p>Use external storage, or a <code class="literal">ReadWriteMany</code> access mode Kubernetes <code class="literal">StorageClass</code> to store OCI artifacts</p><p>If the SUSE Private Registry <code class="literal">registry</code> internal component is configured to use Kubernetes persistent volumes to store OCI artifacts instead of an external storage service such as Azure Blob Storage or Amazon S3, increasing the number of replicas is only possible if the Kubernetes <code class="literal">StorageClass</code> supports the <code class="literal">ReadWriteMany</code> access mode. High availability may be explicitly disabled only for this component, if such a <code class="literal">StorageClass</code> cannot be provided.</p></li><li class="listitem"><p><span class="strong"><strong>The internal database and redis component do not support high availability.</strong></span></p></li><li class="listitem"><p>Database can only be HA when SUSE Private Registry is connected to an external HA database setup</p><p>For the database component, the only supported way to achieve High Availability is to connect SUSE Private Registry to an external hosted database service, such as Amazon RDS/PostgreSQL or Azure Database for PostgreSQL, deployed in a highly available setup.
See <a class="xref" href="#install-external-database">[install-external-database]</a> in the Deployment section.</p></li><li class="listitem"><p>To enable high availability for the Redis component, use one of the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Connect to an external HA redis service (Azure Cache / Amazon ElastiCache)</p><p>Connect SUSE Private Registry to an external hosted redis service, such as Amazon ElastiCache for Redis or Azure Cache for Redis - deployed in a highly available setup. This is the equivalent to the database solution mentioned above.</p><p><span class="strong"><strong>or</strong></span></p></li><li class="listitem"><p>Install the SUSE Redis operator on the same cluster, as the de facto "external" redis service</p><p>Install a highly available Redis cluster via the SUSE Redis operator Helm chart into the same Kubernetes cluster where SUSE Private Registry is running, then connect the SUSE Private Registry to it as an external redis service.
Read more about installing Redis operator in the Deployment section.</p></li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-deployment-of-suse-private-registry"></a>Deployment of SUSE Private Registry</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-preparation"></a>Preparation</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Ensure you have <code class="literal">kubectl</code> and <code class="literal">helm</code> v3 installed and check that you have access to the target Kubernetes cluster where SUSE Private Registry will be installed.</p></li><li class="listitem"><p>Decide between using a Kubernetes Ingress or just a Load Balancer to expose the SUSE Private Registry services.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>If Kubernetes Ingress is the chosen option, ensure that:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: square; "><li class="listitem"><p>You have an Ingress Controller set up in the target Kubernetes cluster.</p></li><li class="listitem"><p>You prepare two resolvable FQDN values:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>One for the Harbor UI/API.</p></li><li class="listitem"><p>One for the Notary API (only if <code class="literal">Notary</code> will be enabled).</p></li></ul></div></li></ul></div></li><li class="listitem"><p>If using just a LoadBalancer, ensure that you have one of the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: square; "><li class="listitem"><p>A predefined external IP address that can be associated with the Load Balancer service used to expose the SUSE Private Registry services.</p></li><li class="listitem"><p>An FQDN value that, later on, can be mapped in the external DNS to the external IP address dynamically allocated to the Load Balancer service during installation.</p></li></ul></div></li></ul></div></li><li class="listitem"><p>For the Harbor UI/API and Notary API, choose between using auto-generated TLS certificates or providing your own custom TLS certificates. If using your own, have the certificates ready.</p></li><li class="listitem"><p>Choose the persistent storage back-end to store OCI artifacts: a Kubernetes <code class="literal">StorageClass</code>, or one of the external storage services available from public-cloud providers.</p></li><li class="listitem"><p>Verify that the target Kubernetes cluster provides the required <code class="literal">StorageClass</code>(es). A <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access mode is required to fully enable  and scalability for the <code class="literal">registry</code> SUSE Private Registry component, unless an external storage service is used to store OCI artifacts.</p></li><li class="listitem"><p>Choose between using an internal or external database service.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>The internal database service does not support high availability and scalability, and is therefore <span class="strong"><strong>not recommended for production</strong></span>.</p></li><li class="listitem"><p>If you use an external database service instead, prepare it separately beforehand.</p></li></ul></div></li><li class="listitem"><p>Choose between using an internal or external Redis service. Similarly to the database:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>The internal Redis service does not support high availability and scalability and <span class="strong"><strong>is not recommended for production</strong></span>.</p></li><li class="listitem"><p>Similarly, if you will use an external, public-cloud-managed Redis service, you must prepare it separately beforehand.</p></li><li class="listitem"><p>If you prefer to use the SUSE <code class="literal">redis-ha</code> operator service, installation instructions are included in the SUSE Private Registry installation steps covered this section.</p></li></ul></div></li><li class="listitem"><p>Optionally, prepare a GitHub personal authentication token, in order to prevent rate-limiting problems when <code class="literal">trivy</code> downloads its vulnerability database.</p></li><li class="listitem"><p>Determine resource requests and limits based on your Kubernetes cluster setup.</p></li><li class="listitem"><p>Optionally, prepare the Service Accounts to use for Harbor components.</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-installation-steps"></a>Installation Steps</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Download the Helm chart from the official SUSE repository:</p><pre class="screen">export HELM_EXPERIMENTAL_OCI=1
# download a chart from public registry
helm chart pull registry.suse.com/harbor/harbor:1.5
# export the chart to local directory
helm chart export registry.suse.com/harbor/harbor:1.5</pre></li><li class="listitem"><p>Make sure <code class="literal">KUBECONFIG</code> is set correctly</p><p>When installing on SUSE CaaS Platform, it is expected that the <code class="literal">KUBECONFIG</code> environment variable is set correctly pointing to the Kubernetes cluster.</p><p>When installing into hosted Kubernetes clusters such as EKS or AKS, configuration must be fetched first so the following <code class="literal">kubectl</code> and helm commands work correctly.</p><p>For AKS, it is possible to use the <code class="literal">az</code> command line tool to get the <code class="literal">kubeconfig</code>:</p><pre class="screen">az aks get-credentials --resource-group &lt;azure-resource-group&gt; --name &lt;aks-cluster-name&gt; --file kubeconfig.yaml
export KUBECONFIG=&lt;full path to kubeconfig.yaml&gt;</pre><p>For EKS, the <code class="literal">aws</code> command line tool can be used to generate the <code class="literal">kubeconfig</code>:</p><pre class="screen">aws eks --region &lt;region-code&gt; update-kubeconfig --name &lt;eks-cluster_name&gt; --kubeconfig kubeconfig.yaml
export KUBECONFIG=&lt;full path to kubeconfig.yaml&gt;</pre></li><li class="listitem"><p>Prepare a <code class="literal">harbor-values.yaml</code> file to specify custom SUSE Private Registry configuration values</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>The default configuration provided with the SUSE Private Registry helm chart is not suited for production use!</p><p>A separate YAML file (referred to as the <code class="literal">harbor-values.yaml</code> file in the following sections) needs to be populated with customized configuration values to be used during installation.
The exact configuration options that can be customized, and the values that they can take, are covered in detail in the next installation steps.</p></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3><p>The full list of configuration options and default values that can be overridden in <code class="literal">harbor-values.yaml</code> is included in the helm chart itself, and can be viewed in YAML format by running the following command:</p><pre class="screen">helm show values harbor</pre><p>It can also be used as a YAML template for configuration values that need to be customized, although it is recommended to keep only the configuration options that are changed from their default values in <code class="literal">harbor-values.yaml</code>, to allow default configuration changes to be introduced during upgrades.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>The <code class="literal">harbor-values.yaml</code> file prepared and used during installation is the source of truth for the SUSE Private Registry configuration.</p><p>It will also be required for some administrative operations, such as subsequent configuration changes and upgrades.
Make sure to preserve this file in a safe place, preferably under version control, and to update it with every configuration change that is subsequently made to the deployed SUSE Private Registry instance.</p></div></li><li class="listitem"><p>(Optional) Disable unnecessary components</p><p>By default, SUSE Private Registry has all supported components enabled. Some components may be disabled in the configuration, if they are not required:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p><code class="literal">trivy</code> - disable if you do not require the security-vulnerability-scanning feature.</p></li><li class="listitem"><p><code class="literal">notary</code> - disable if you do not require the artifact-signing feature.</p></li></ol></div><p>To disable unnecessary components, set the relevant configuration options to false in <code class="literal">harbor-values.yaml</code>:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">trivy:
  enabled: false
notary:
  enabled: false</pre><p>
</p></li><li class="listitem"><p>Configure a way to expose the SUSE Private Registry UI and public APIs</p><p>The default and recommended way to expose the SUSE Private Registry services to be consumed from outside the Kubernetes cluster is to use a Kubernetes Ingress.
This requires that a Kubernetes Ingress controller is already configured in your cluster and resolvable FQDNs to be prepared for the Harbor UI/API and the Notary API services (if enabled).
Alternatively, services may be exposed using a Kubernetes LoadBalancer instead.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Expose SUSE Private Registry using a Kubernetes Ingress</p><p>This option assumes a Kubernetes Ingress Controller is already configured for your Kubernetes cluster, as described in the <a class="xref" href="#requirements-ingress" title="Ingress">the section called “Ingress”</a> section.
Update <code class="literal">harbor-values.yaml</code> with the following configuration values:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
  # Set the way how to expose the service. Default value is "ingress".
  ingress:
    hosts:
      core: "&lt;core_fqdn&gt;"
      notary: "&lt;notary_fqdn&gt;"

# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker/Notary client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: "https://&lt;core_fqdn&gt;"</pre><p>
</p><p>Replace <code class="literal">&lt;core_fqdn&gt;</code> and <code class="literal">&lt;notary_fqdn&gt;</code> values with the resolvable FQDN values that were prepared as detailed in the <a class="xref" href="#requirements" title="Requirements">the section called “Requirements”</a> section.
If the Notary service was not enabled in the configuration, the <code class="literal">&lt;notary_fqdn&gt;</code> entry may be omitted.
The <code class="literal">harbor-values.yaml</code> configuration would look like this, if, for example, a public service like <a class="link" href="https://nip.io" target="_top">nip.io</a> was used to provide FQDNs:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
  ingress:
    hosts:
      core: harbor.10.86.0.237.nip.io
      notary: notary.10.86.0.237.nip.io
externalURL: "https://harbor.10.86.0.237.nip.io"</pre><p>
</p><p>Depending on which Kubernetes Ingress Controller is used, you may need to add additional annotations to the SUSE Private Registry Ingress configuration:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
  ingress:
	...
    annotations:
      # To be used for the nginx ingress on AKS:
      kubernetes.io/ingress.class: nginx
      # To be used for the ALB ingress on EKS:
      kubernetes.io/ingress.class: alb</pre><p>
</p></li><li class="listitem"><p>Expose SUSE Private Registry using a Kubernetes LoadBalancer</p><p>Update the <code class="literal">harbor-values.yaml</code> configuration file with the following configuration values:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
  type: loadBalancer
  loadBalancer:
    # Set the IP if the LoadBalancer supports assigning IP
    IP: ""

# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker/Notary client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: "https://&lt;harbor_fqdn_or_ip_addr&gt;"</pre><p>
</p><p>You must set the <code class="literal">&lt;harbor_fqdn_or_ip_addr&gt;</code> value to an FQDN that can be resolved to the external IP address allocated to the Harbor Load Balancer service.
Alternatively, if the LoadBalancer solution used for the underlying Kubernetes distribution supports assigning an IP address beforehand, you can set both the <code class="literal">expose.loadBalancer.IP</code> configuration option and the <code class="literal">&lt;harbor_fqdn&gt;</code> value to a predefined external IP address.</p></li></ol></div></li><li class="listitem"><p>Configure external TLS and certificates</p><p>TLS certificates are required to secure access to the SUSE Private Registry services that are exposed for external consumption - the Harbor UI/API and the Notary API (if Notary is enabled).
These certificates may either be generated automatically during installation (default), or provided as Kubernetes secrets, or configured beforehand as the default TLS certificate for the Kubernetes Ingress Controller used to expose the services, as explained in the TLS Certificates requirements (<a class="xref" href="#requirements-tls" title="TLS Certificates">the section called “TLS Certificates”</a>) section.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Auto-generated certificates</p><p>This is the default helm chart setting. If an Ingress was used to expose the SUSE Private Registry services, the FQDN values configured for the ingress will be used to generate the TLS certificates automatically.
If using a LoadBalancer to expose the services instead of Ingress, please also set the <code class="literal">commonName</code> option to the pre-allocated external IP address or the FQDN value that will be resolved to it:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: auto
    auto:
      # The common name used to generate the certificate, it's necessary
      # when the type isn't "ingress"
      commonName: "&lt;harbor_fqdn_or_ip_addr&gt;"</pre><p>
</p></li><li class="listitem"><p>Custom certificates</p><p>One or two custom certificates are required for exposed SUSE Private Registry services: one for the Harbor UI/API and another one for the Notary API (required only if Notary is enabled). The certificates need to reflect the FQDN values or external IP address values used at the previous step to configure the Kubernete Ingress or LoadBalancer service exposure settings. The helm chart also supports using a single certificate instead of two, as long as the CN or SAN certificate field values match both FQDNs. The certificates need to be supplied in the form of Kubernetes secrets:</p><pre class="screen">kubectl create secret tls -n registry &lt;harbor-tls-secret&gt; --key ${HARBOR_CERT_KEY_FILE} --cert ${HARBOR_CERT_FILE}
kubectl create secret tls -n registry &lt;notary-tls-secret&gt; --key ${NOTARY_CERT_KEY_FILE} --cert ${NOTARY_CERT_FILE}</pre><p>In case the certificate has intermediate CAs, you can bundle them into the CERT_FILE prior creating the secret, e.g.:</p><pre class="screen">cat $CERT_FILE $bundle_ca_file &gt; bundled_cert_file
kubectl create secret tls -n registry &lt;tls-secret&gt; --key ${KEY_FILE} --cert bundled_cert_file</pre><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: secret
    secret:
      # The name of secret which contains keys named:
      # "tls.crt" - the certificate
      # "tls.key" - the private key
      secretName: "&lt;harbor-tls-secret&gt;"
      # The name of secret which contains keys named:
      # "tls.crt" - the certificate
      # "tls.key" - the private key
      # Only needed when the "expose.type" is "ingress".
      notarySecretName: "&lt;notary-tls-secret&gt;"</pre><p>
</p></li><li class="listitem"><p>Default Ingress certificate</p><p>If a default TLS certificate has been set up for the Kubernetes Ingress Controller earlier, as covered in the TLS Certificates section, certificates need not be explicitly supplied during the SUSE Private Registry installation. It is sufficient to set the <code class="literal">tls.certSource</code> option to <code class="literal">none</code>:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: none</pre><p>
</p></li></ol></div></li><li class="listitem"><p>Configure internal TLS</p><p>In addition to securing external connections to exposed services, SUSE Private Registry also supports using TLS to secure internal communication between its components.
TLS certificates will be generated automatically for this purpose. Enabling internal TLS is optional, but highly recommended:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">internalTLS:
  enabled: true</pre><p>
</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Internal TLS support does not yet cover the internal database and Redis services.</p></div><p>If SUSE Private Registry is deployed in K3s, note that unmodified Traefik (the default K3s ingress controller) will not work with automatically-generated certificates.
You must configure Traefik not to verify the backend SSL certificate (<code class="literal">insecureSkipVerify = true</code> option).
Learn how to modify Traefik settings in the <a class="link" href="https://rancher.com/docs/k3s/latest/en/helm/#customizing-packaged-components-with-helmchartconfig" target="_top">upstream documentation</a>.</p><p>For example, with K3s version 1.19 and newer, it is possible to use this kind of modification for the Traefik helm chart, then place it into the K3s manifest directory:</p><p><strong>traefik-config.yaml. </strong>
</p><pre class="screen">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: traefik
  namespace: kube-system
spec:
  valuesContent: |-
    ssl:
      insecureSkipVerify: true</pre><p>
</p></li><li class="listitem"><p>Configure Persistent Storage</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Configure Persistent Volumes</p><p>By default, persistent volumes are enabled for all stateful components of SUSE Private Registry.
However, a default <code class="literal">StorageClass</code> must be configured in the Kubernetes cluster to be able to provision volumes dynamically.
Alternatively, you can configure explicit <code class="literal">StorageClass</code> values for each component.</p><p>For each component that uses persistent storage, you can configure the following settings:</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p><code class="literal">storageClass</code>: Specify the "storageClass" used to provision the volume; if empty, the default <code class="literal">StorageClass</code> will be used (default: <code class="literal">empty</code>).</p></li><li class="listitem"><p><code class="literal">accessMode</code>: Volumes can be mounted on a container in any way supported by the storage provider. Valid values are:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">ReadWriteOnce</code>: the volume can be mounted as read-write by a single container</p></li><li class="listitem"><p><code class="literal">ReadWriteMany</code>: the volume can be mounted as read-write by many containers. This is only required for the <code class="literal">registry</code> component, when configured in  mode and using a persistent volume to store OCI artifacts. If an external storage service is used to store OCI artifacts, or if a <code class="literal">ReadWriteMany</code> <code class="literal">StorageClass</code> isn’t available in your Kubernetes cluster, you should not use this value.
(default: <code class="literal">ReadWriteOnce</code>)</p></li></ol></div></li><li class="listitem"><p>size: the size of the volume to be provisioned (e.g. 5Gi for 5 gigabytes). Default values vary by component:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>registry: 5Gi</p></li><li class="listitem"><p>database: 1Gi</p></li><li class="listitem"><p>redis: 1Gi</p></li><li class="listitem"><p>trivy: 5Gi</p></li></ol></div><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>The default volume sizes provided by SUSE Private Registry are <span class="strong"><strong>not recommended for production</strong></span>.</p><p>We recommend careful planning and setting the volume sizes according to the expected usage.
Expanding in-use persistent-volume claims is only supported by some storage providers, and in some cases it will require restarting the pods, which will impact service availability.</p></div></li></ol></div><p>For configuring persistent storage, update <code class="literal">harbor-values.yaml</code> with the following configuration, and set the values accordingly:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">persistence:
  persistentVolumeClaim:
    registry:
      storageClass: ""
      accessMode: ReadWriteMany
      size:
    database:
      storageClass: ""
      size:
    redis:
      storageClass: ""
      size:
    trivy:
      storageClass: ""
      size:</pre><p>
</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Using external services</h3><p>The above settings will be ignored and may be omitted for components configured to use an external service (<code class="literal">database</code>, <code class="literal">redis</code>), and for the <code class="literal">registry</code> component when external storage is configured for OCI artifacts.</p></div><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>If a Kubernetes persistent volume is configured to store OCI artifacts instead of an external storage service, and if your Kubernetes cluster does not provide a <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access mode capabilities, then the <code class="literal">updateStrategy.type</code> option must set to <code class="literal">Recreate</code> in the <code class="literal">harbor-values.yaml</code> file. Otherwise, running <code class="literal">helm upgrade</code> to apply subsequent configuration changes or to perform upgrades will result in failure:</p><pre class="screen"># The update strategy for deployments with persistent volumes (registry): "RollingUpdate" or "Recreate"
# Set it as "Recreate" when "RWM" for volumes isn't supported
updateStrategy:
  type: Recreate</pre></div></li><li class="listitem"><p>Configure External Storage for OCI Artifacts</p><p>The default option for storing OCI artifacts, such as container images and helm charts, is using a persistent volume provided by the default <code class="literal">storageClass</code> of your Kubernetes cluster (as described on the previous section).
However, you can configure SUSE Private Registry to use an external storage solution such as Amazon S3 or Azure Blob Storage to store those artifacts.</p><p>For example, for Azure Blob Storage, you must pre-configure an Azure Storage Account and Azure Storage Container.
Using the <code class="literal">az</code> command line client, execute the following commands to create and fetch necessary resources:</p><pre class="screen">az storage account create --resource-group &lt;azure-resource-group&gt; --name &lt;azure-storage-account-name&gt;
az storage account keys list --resource-group &lt;azure-resource-group&gt; --account-name &lt;azure-storage-account-name&gt; -o tsv | head -n 1 | cut -f 3
az storage container create --account-name &lt;azure-storage-account-name&gt; --name &lt;azure-storage-container-name&gt; --auth-mode key</pre><p>Then, you must configure the "imageChartStorage" section in <code class="literal">harbor-values.yaml</code> as follows:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">persistence:
...
  imageChartStorage:
    type: azure
    azure:
      accountname: &lt;azure-storage-account-name&gt;
      accountkey: &lt;azure-storage-account-key&gt;
      container: &lt;azure-storage-container-name&gt;</pre><p>
</p><p>For Amazon S3, the process is similar. The <code class="literal">imageChartStorage</code> section in <code class="literal">harbor-values.yaml</code> will look like this:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">persistence:
...
  imageChartStorage:
    type: s3
      region: &lt;aws-region&gt;
      bucket: &lt;aws-s3-bucket-name&gt;
      accesskey: &lt;aws-account-access-key&gt;
      secretkey: &lt;aws-account-secret-key&gt;</pre><p>
</p></li></ol></div></li><li class="listitem"><p>(Optional) Configure a GitHub authentication token for Trivy</p><p>If the <code class="literal">Trivy</code> security vulnerability scanner service is enabled, we recommend <a class="link" href="https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token" target="_top">generating a GitHub personal authentication token</a> and supplying it in the <code class="literal">harbor-values.yaml</code> trivy configuration section, to prevent issues with <a class="link" href="https://docs.github.com/en/free-pro-team@latest/rest/reference/rate-limit" target="_top">the API rate-limiting that GitHub enforces on unauthenticated requests</a>:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">trivy:
  ...
  gitHubToken: "&lt;github-token-value&gt;"</pre><p>
</p></li><li class="listitem"><p>(Optional) Configure  parameters</p><p>By default, SUSE Private Registry uses a replica count (that is, the number of redundant pods providing the same service) value of 1 for all its components.
To have a highly-available deployment, configure a <code class="literal">ReplicaCount</code> value of at least 2 for enabled services in <code class="literal">harbor-values.yaml</code>:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">portal:
  replicas: 3
core:
  replicas: 3
# Only enabled when using a LoadBalancer instead of Ingress to expose services
nginx:
  replicas: 3
jobservice:
  replicas: 3
registry:
  replicas: 3
trivy:
  replicas: 3
notary:
  server:
    replicas: 3
  signer:
    replicas: 3</pre><p>
</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>You must have a Kubernetes <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access mode to enable  for the SUSE Private Registry <code class="literal">registry</code> component, when a Kubernetes persistent volume is used as the storage back-end for OCI artifacts.</p><p>If a <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access is not available for your Kubernetes cluster, setting the replica count to a value higher than 1 for the <code class="literal">registry</code> component will result in installation failure.
Furthermore, using <code class="literal">helm upgrade</code> to apply subsequent configuration changes or to perform upgrades will also result in failures without a <code class="literal">ReadWriteMany</code> access mode <code class="literal">StorageClass</code>.
To prevent that, ensure the <code class="literal">updateStrategy.type</code> option is set to <code class="literal">Recreate</code> in the <code class="literal">harbor-values.yaml</code> file:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen"># The update strategy for deployments with persistent volumes(registry): "RollingUpdate" or "Recreate"
# Set it as "Recreate" when "RWM" for volumes isn't supported
updateStrategy:
  type: Recreate</pre><p>
</p></div></li><li class="listitem"><p><a id="install-external-database"></a> (Optional) External Database Setup</p><p>We recommend an external database to deploy SUSE Private Registry in a fully highly-available and scalable setup.
This section assumes a managed PostgreSQL database instance has already been setup, either in Azure or AWS, as covered in the <a class="xref" href="#requirements-external-postgres" title="External PostgreSQL Database">the section called “External PostgreSQL Database”</a>.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Connect to an Azure PostgreSQL database</p><p>Add the following section to the <code class="literal">harbor-values.yaml</code> file and fill it with information reflecting the Azure PostgreSQL database instance previously configured as an external database:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">database:
  type: external
  external:
    host: &lt;database-fully-qualified-hostname&gt;
    port: "5432"
    username: &lt;admin-user&gt;@&lt;database-hostname&gt;
    password: &lt;admin-password&gt;
    # "disable" - No SSL
    # "require" - Always SSL (skip verification)
    # "verify-ca" - Always SSL (verify that the certificate presented by the
    # server was signed by a trusted CA)
    # "verify-full" - Always SSL (verify that the certification presented by the
    # server was signed by a trusted CA and the server host name matches the one
    # in the certificate)
    sslmode: "verify-full"</pre><p>
</p></li><li class="listitem"><p>Connect to an AWS PostgreSQL database</p><p>Add the following section to <code class="literal">harbor-values.yaml</code> and fill it with information reflecting the AWS PostgreSQL database instance previously configured as an external database:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">database:
  type: external
  external:
    host: &lt;database-fully-qualified-hostname&gt;
    port: "5432"
    username: &lt;admin-user&gt;@&lt;database-hostname&gt;
    password: &lt;admin-password&gt;
    # "disable" - No SSL
    # "require" - Always SSL (skip verification)
    # "verify-ca" - Always SSL (verify that the certificate presented by the
    # server was signed by a trusted CA)
    # "verify-full" - Always SSL (verify that the certification presented by the
    # server was signed by a trusted CA and the server host name matches the one
    # in the certificate)
    sslmode: "verify-full"</pre><p>
</p></li></ol></div></li><li class="listitem"><p><a id="install-redis-operator"></a> (Optional) Install Redis Operator</p><p>As mentioned above, Redis Operator provides High Availability to the Redis component of SUSE Private Registry. It can be installed into the same Kubernetes cluster as SUSE Private Registry. The installation of Redis operator is also done via a Helm chart, and must happen before the installation of SUSE Private Registry.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Install Redis operator in its own Kubernetes namespace using the Helm chart:</p><pre class="screen">export HELM_EXPERIMENTAL_OCI=1
helm chart pull registry.suse.com/harbor/redis-operator:3.1
helm chart export registry.suse.com/harbor/redis-operator:3.1
kubectl create namespace redis-operator
helm -n redis-operator install harbor-redis ./redisoperator</pre></li><li class="listitem"><p>Configure <code class="literal">RedisFailover</code> object:</p><p>The Redis HA configuration needs to be specified in the <code class="literal">redisfailover</code> section of <code class="literal">harbor-values.yaml</code>.
The following is an example configuration:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redisfailover:
  enabled: true
  name: harbor-redisfailover</pre><p>
</p></li><li class="listitem"><p>Configure SUSE Private Registry to be connected to the external Redis</p><p>Extend the <code class="literal">harbor-values.yaml</code> file with the configuration specified below.</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redis:
  type: external
  external:
    addr: rfs-harbor-redisfailover:26379
    sentinelMasterSet: mymaster <a id="CO1-1"></a><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></a> </p></td><td valign="top" align="left"><p><code class="literal">mymaster</code> is a predefined value of redisfailover deployment and cannot be changed.</p></td></tr></table></div></li><li class="listitem"><p>(Optional) Set up own password</p><p>By default, if no secret and password are provided, the SUSE Private Registry Helm chart will generate a password. A custom password can also be provided:</p><pre class="screen">kubectl -n registry create secret generic redis-auth --from-literal=password="&lt;password-value&gt;"</pre><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redis:
  type: external
  external:
    addr: rfs-harbor-redisfailover:26379
    sentinelMasterSet: mymaster
    password: &lt;password-value&gt;</pre><p>
</p></li><li class="listitem"><p>(Optional) Configure Redisfailover deployment</p><p>By default, the Redisfailover deployment has three sentinel replicas, three redis replicas, and will keep the data when the Helm chart is uninstalled. This behavior can be configured in the <code class="literal">redisfailover</code> section.</p></li></ol></div></li><li class="listitem"><p><a id="install-external-redis"></a> (Optional) External Redis Setup</p><p>We recommend an external Redis to deploy SUSE Private Registry in a fully highly-available and scalable setup.
When deployed in AKS or EKS, as an alternative to using the Redis Operator, SUSE Private Registry may instead be connected to a managed Redis instance running in public cloud.
This section assumes a managed Redis instance has already been setup, either in Azure or AWS, as covered in the External Redis requirements section.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Connect to an Azure Cache for Redis instance</p><p>Add the following section to the <code class="literal">harbor-values.yaml</code> file and fill it with information reflecting the Azure Cache for Redis instance previously prepared.
As mentioned above in the <a class="xref" href="#requirements-redis-azure" title="Azure Cache for Redis">the section called “Azure Cache for Redis”</a>, the address will have the form of <code class="literal">&lt;azure-redis-cache&gt;.redis.cache.windows.net</code>.</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redis:
  type: external
  external:
    addr: "192.168.0.2:6379"
    password: access-key <a id="CO2-1"></a><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO2-1"><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></a> </p></td><td valign="top" align="left"><p>Replace <code class="literal">access-key</code> with the access key retrieved after creating the Azure Cache for Redis instance.</p></td></tr></table></div></li><li class="listitem"><p>Connect to an Amazon ElastiCache Redis service</p><p>Add the following section to <code class="literal">harbor-values.yaml</code> and fill it with information reflecting the Amazon ElastiCache Redis instance that you previously prepared:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redis:
  type: external
  external:
    addr: "192.168.0.2:6379"
    password: "" <a id="CO3-1"></a><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-1"><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></a> </p></td><td valign="top" align="left"><p>Add password if configured manually (not the default) in AWS ElastiCache.</p></td></tr></table></div></li></ol></div></li><li class="listitem"><p><a id="install-resource-limits"></a> (Optional) Setup Resource Requests and Limits</p><p>It is a good practice to specify resource requests and limit values.
For each Harbor component, it is possible to specify a minimal resource value — that is, the amount of CPU units and memory it should get — and a limit value, so that Kubernetes knows the resources given to a component cannot exceed the limit.
These per-component values are used for all containers that are created for a given Harbor component.</p><p>For example, add the following section to <code class="literal">harbor-values.yaml</code> to specify that the containers from the core component should get at least 0.1 CPU, 256 MiB of RAM, and not more than 1 CPU and 1 GiB of memory:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">core:
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      cpu: 1
      memory: 1Gi</pre><p>
</p><p>Read more about Resource management in the <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" target="_top">upstream documentation</a>.</p></li><li class="listitem"><p><a id="install-resource-accounts"></a> (Optional) Use distinct Service Accounts</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>You can use distinct Service Accounts for each Harbor component.</p><p>Refer to the <a class="link" href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_top">upstream documentation</a> to find out more about Pod Security Policies.</p></div><p>Without any changes, all created Pods belong to the default Service Account. For better overall cluster security, we recommend creating a Pod Security Policy that restricts the Pods to only  specific actions.
Then you can assign new ServiceAccounts to your Pod Security Policy using Roles and Role Bindings.</p><p>For example, if you created a <code class="literal">suse-registry</code> Service Account, add the following section to the <code class="literal">harbor-values.yaml</code> file so that all Harbor services are associated with it:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">nginx:
  serviceAccountName: "suse-registry"
portal:
  serviceAccountName: "suse-registry"
core:
  serviceAccountName: "suse-registry"
jobservice:
  serviceAccountName: "suse-registry"
registry:
  serviceAccountName: "suse-registry"
trivy:
  serviceAccountName: "suse-registry"
notary:
  server:
    serviceAccountName: "suse-registry"
  signer:
    serviceAccountName: "suse-registry"
database:
  internal:
    serviceAccountName: "suse-registry"
redis:
  internal:
    serviceAccountName: "suse-registry"</pre><p>
</p></li><li class="listitem"><p><a id="install-passwords"></a> Set up the passwords for deployment</p><p>By default, all passwords are automatically generated when installing SUSE Private Registry with the Helm chart. They can be retrieved post-installation from the created Kubernetes secrets objects. For example, to retrieve the Harbor administrator password necessary to log in into the Harbor Portal UI as admin user, run this command after the deployment is finished:</p><pre class="screen">kubectl get secret suse-registry-harbor-core -n registry -o jsonpath="{.data.HARBOR_ADMIN_PASSWORD}" | base64 --decode</pre><p>To set a custom administrator password before the installation, modify your <code class="literal">harbor-values.yaml</code> file like this:</p><p><strong>harbor-config-values.yaml. </strong>
</p><pre class="screen">harborAdminPassword: &lt;password-for-admin-user&gt;</pre><p>
</p><p>Similarly, custom passwords may be set before the installation for the database and Redis services, if configured as internal services:</p><p><strong>harbor-config-values.yaml. </strong>
</p><pre class="screen">database:
  ...
  internal:
    password: &lt;password-for-redis&gt;

redis:
  ...
  internal:
    password: &lt;password-for-redis&gt;</pre><p>
</p></li><li class="listitem"><p>Finally, deploy helm to install SUSE Private Registry</p><p>To install SUSE Private Registry as a <code class="literal">suse-registry</code> release into the registry namespace with the custom configuration prepared in <code class="literal">harbor-values.yaml</code> in the previous steps, run the following command:</p><pre class="screen">helm -n registry install suse-registry ./harbor -f harbor-values.yaml</pre><p>Once the installation is complete, Helm will provide the information about the location of the newly installed registry, e.g.:</p><pre class="screen">NAME: suse-registry
LAST DEPLOYED: Fri Jul 24 10:34:53 2020
NAMESPACE: registry
STATUS: deployed
REVISION: 1
NOTES:
Please wait for several minutes for Harbor deployment to complete.
Then you should be able to visit the Harbor portal at https://core.harbor.domain <a id="CO4-1"></a><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-1"><span><img src="static/images/callouts/1.png" alt="1" border="0" /></span></a> </p></td><td valign="top" align="left"><p>You will see your <code class="literal">&lt;core_fqdn&gt;</code> instead of <code class="literal"><a class="link" href="https://core.harbor.domain" target="_top">https://core.harbor.domain</a></code>.</p></td></tr></table></div></li><li class="listitem"><p>Check the installation</p><p>You can check the status of created artifacts and see if everything is running correctly:</p><pre class="screen">&gt; kubectl -n registry get deployments
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
suse-registry-harbor-core         1/1     1            1           17h
suse-registry-harbor-jobservice   1/1     1            1           17h
suse-registry-harbor-portal       1/1     1            1           17h
suse-registry-harbor-registry     1/1     1            1           17h</pre><pre class="screen">&gt; kubectl -n registry get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
suse-registry-harbor-core-c787885b6-2l7lz             1/1     Running   1          105m
suse-registry-harbor-database-0                       1/1     Running   0          105m
suse-registry-harbor-jobservice-698fb5bb44-88mc5      1/1     Running   1          105m
suse-registry-harbor-nginx-b4f7748c5-8v2rp            1/1     Running   0          105m
suse-registry-harbor-portal-bff5898cc-tt9ss           1/1     Running   0          105m
suse-registry-harbor-redis-0                          1/1     Running   0          105m
suse-registry-harbor-registry-7f65b6f87b-sqhzt        2/2     Running   0          105m
suse-registry-harbor-trivy-0                          1/1     Running   0          105m</pre></li></ol></div><p>After the installation is complete, please proceed with <a class="xref" href="#administration" title="Administration">the section called “Administration”</a> and configure an authentication method.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-deployment-in-an-air-gapped-environment"></a>Deployment in an Air-Gapped Environment </h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-introduction-2"></a>Introduction</h3></div></div></div><p>K3s is a lightweight Kubernetes distribution from Rancher that can be
deployed on a single node. </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-use-cases"></a>Use Cases</h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Testing</span></dt><dd><p>The customer wants to test SUSE Private Registry Powered by Harbor 2.1, but they do not
have a Kubernetes cluster prepared.</p></dd><dt><span class="term">Single-node setup</span></dt><dd><p>A production setup for customers that want to use SUSE Private Registry Powered by Harbor 2.1,
but do not want to deploy CaaSP, Racher’s RKE or any other Kubernetes cluster.</p></dd><dt><span class="term">Air-gapped environments</span></dt><dd><p>K3s can be deployed without access to the Internet.
With SUSE Private Registry Powered by Harbor 2.1 running on such a cluster, it could for example serve as an image
registry for other Kubernetes clusters in the customer’s network.</p></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-requirements"></a>Requirements</h3></div></div></div><p>You should have Helm installed on the machine where you want to install k3s.</p><p>Make sure that you have enough disk space. Rancher does not mention any specific
requirements for disk size, so prepare at least a few gigabytes. It is possible
to put the <code class="literal">/var/lib/rancher</code> directory on any partition or disk where you know
that you have enough space. In case space starts to run out, you might encounter
<a class="link" href="https://github.com/rancher/k3s/issues/1552" target="_top">k3s issue #1552</a>.</p><p>Also check for the latest available <span class="emphasis"><em>stable</em></span> version of k3s by consulting the
<a class="link" href="https://github.com/k3s-io/k3s/releases" target="_top">releases page</a>. We do not recommend
running pre-release versions. If you encounter 404 errors trying to download
packages from GitHub, this is probably due to the release of a newer version.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-deployment"></a>Deployment</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>We recommend that you read
<a class="link" href="https://rancher.com/docs/k3s/latest/en/installation/airgap/#manually-deploy-images-method" target="_top">the upstream guide</a>
on deploying k3s into an air-gapped environment.</p></li><li class="listitem"><p>Before you begin, download all the packages for the installation.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>You will need to download three different files to install k3s.</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p>The k3s install script. You can fetch this with the command:</p><pre class="screen">curl https://get.k3s.io -o install.sh</pre></li><li class="listitem"><p>The k3s binary (called just <code class="literal">k3s</code>).</p></li><li class="listitem"><p>The bundle of images required for basic k3s air-gapped deployment and for
some optional features (such as the Traefik ingress controller). This is called
<code class="literal">k3s-airgap-images-amd64.tar</code>.</p><p>Fetch the latter two files from: <a class="link" href="https://github.com/k3s-io/k3s/releases/latest" target="_top">https://github.com/k3s-io/k3s/releases/latest</a></p></li></ol></div></li><li class="listitem"><p>Fetch the images required for the SUSE Private Registry:</p><pre class="screen">mkdir images
for component in core nginx notary-server notary-signer trivy-adapter registryctl jobservice registry db redis; do
 image="registry.suse.com/harbor/harbor-$component:2.1.2"; \
 docker pull $image; docker save -o "images/harbor-$component-2.1.2.tar" $image; \
done</pre></li><li class="listitem"><p>Transfer all the files onto the air-gapped machine where k3s will run, for
example using a USB key.</p></li><li class="listitem"><p>On that machine, prepare the images and install the binary:</p><pre class="screen">sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
sudo cp k3s /usr/local/bin/k3s
sudo chown +x /usr/local/bin/k3s
sudo chown +x /usr/local/bin/install.sh</pre></li><li class="listitem"><p>Install k3s.  It is not necessary to start the <code class="literal">systemd</code> service yet, if you
want to watch the start process closely for the first time</p><pre class="screen">INSTALL_K3S_SKIP_ENABLE=true INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3><p>The <code class="literal">INSTALL_K3S_SKIP_ENABLE</code> setting means that the <code class="literal">systemd</code> service for
k3s will not be enabled and started, but you can ignore the value and actually
use the service.</p></div></li><li class="listitem"><p>Install the images required for the SUSE Private Registry into the k3s images
directory, so that the server can import them when it starts:</p><pre class="screen">sudo cp images/*.tar /var/lib/rancher/k3s/agent/images/</pre></li><li class="listitem"><p>Start the k3s server as root.</p><pre class="screen">su
k3s server --write-kubeconfig-mode 644</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3><p>The additional option tells <code class="literal">k3s</code> to reset the permissions on
<code class="literal">/etc/rancher/k3s/k3s.yaml</code> as it loads.</p></div><p>To see more information, you can to pass the <code class="literal">--debug</code> option to the command,
but you will receive a <span class="strong"><strong>lot</strong></span> of information. Alternatively, you can start the
corresponding <code class="literal">systemd</code> service:</p><pre class="screen">systemctl start k3s</pre></li><li class="listitem"><p>In another window, check the start-up process. The first time will be slow,
as all the images must be imported. Check the current image list with:</p><pre class="screen">su
k3s crictl images</pre><p>Use the usual <code class="literal">kubectl</code> command to check the state of initial deployment:</p><pre class="screen">su
k3s kubectl get deployments</pre><p>The above command talks to the default Kubernetes instance. If you want to use
your own <code class="literal">kubectl</code> file, point to the appropriate config file first:</p><pre class="screen">export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get deployments</pre></li></ol></div></li><li class="listitem"><p>Prepare the Trivy offline database</p><p>Trivy is the security scanner installed with Harbor. By default, it
downloads the vulnerability database from GitHub. However, it can also be
deployed into the air-gapped environment. Refer to the
<a class="link" href="https://github.com/aquasecurity/trivy/blob/master/docs/air-gap.md" target="_top">upstream guidelines</a>
for such scenarios. Here, we cover the case where Trivy is deployed with Harbor
using a Helm chart.</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>First, on a machine with the internet access, download the offline
database from <a class="link" href="https://github.com/aquasecurity/trivy-db/releases/" target="_top">https://github.com/aquasecurity/trivy-db/releases/</a>
and move it to your air-gapped machine.</p></li><li class="listitem"><p>You must create a Persistent Volume where the database can be kept. For the
purpose of this example, we will use the default <code class="literal">storageClass</code> that is set up
for k3s, the one using the local-path provisioner. This allows us to map the
Persistent Volume to a local directory on the host machine where the k3s node
is running.</p></li><li class="listitem"><p>Example of Persistent Volume and persistentVolumeClaim definitions:</p><p><strong>pv-trivy.yaml. </strong>
</p><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
 finalizers:
 - kubernetes.io/pv-protection
 name: trivy-pv-volume
spec:
 accessModes:
 - ReadWriteOnce
 capacity:
   storage: 2Gi
 hostPath:
   # local path on my machine
   path: /data/trivy-pv
   type: DirectoryOrCreate
 persistentVolumeReclaimPolicy: Retain
 storageClassName: local-path
 volumeMode: Filesystem</pre><p>
</p><p><strong>pvc-trivy.yaml. </strong>
</p><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: trivy-pvc
 namespace: registry
spec:
 accessModes:
 - ReadWriteOnce
 storageClassName: local-path
 resources:
   requests:
     storage: 2Gi
 volumeName: trivy-pv-volume</pre><p>
</p><p>Save these files as <code class="literal">pv-trivy.yaml</code> and <code class="literal">pvc-trivy.yaml</code>.</p></li><li class="listitem"><p>Create the directory <code class="literal">/data/trivy-pv</code> (see the value of <code class="literal">path</code> in the
<code class="literal">pv-trivy.yaml</code> file). Unpack the downloaded Trivy database under the <code class="literal">trivy/db</code>
subdirectory, and change the ownership of the whole directory to user and group
10000:</p><pre class="screen">sudo mkdir -p /data/trivy-pv/trivy/db
sudo tar -zxf trivy-offline.db.tgz -C /data/trivy-pv/trivy/db/
sudo chown -R 10000:10000 /data/trivy-pv</pre></li></ol></div></li><li class="listitem"><p>Install SUSE Private Registry</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Now you can install SUSE Private Registry the usual way. Find out the
external address provided by the default ingress controller:</p><pre class="screen">kubectl get services</pre></li><li class="listitem"><p>Use the IP number to provide correct values for the core components in the Helm
chart and create, for example, <code class="literal">harbor-config-values.yaml</code>. Add the parts to
mount the correct volume with the Trivy database.</p><p><strong>harbor-config-values.yaml. </strong>
</p><pre class="screen">expose:
 # Set the way how to expose the service. Default value is "ingress".
 ingress:
   hosts:
     core: "&lt;ingress_url&gt;"
externalURL: "https://&lt;ingress_url&gt;"
trivy:
 # do not download trivy DB from github:
 skipUpdate: true
# use existing trivy PVC (prepare offline DB there)
persistence:
 persistentVolumeClaim:
   trivy:
     existingClaim: "trivy-pvc"</pre><p>
</p></li><li class="listitem"><p>Fetch the Helm chart and install Harbor into the new namespace.</p><pre class="screen">export HELM_EXPERIMENTAL_OCI=1
helm chart pull registry.suse.com/harbor/harbor:1.5
helm chart export registry.suse.com/harbor/harbor:1.5</pre></li><li class="listitem"><p>Do not forget to create Kubernetes objects for the Trivy database:</p><pre class="screen">kubectl create namespace registry
kubectl apply -n pv-trivy.yaml
kubectl apply -n pvc-trivy.yaml
helm install -n negistry suse-registry ./harbor -f</pre><p> </p></li></ol></div></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="troubleshooting"></a>Troubleshooting</h2></div></div></div><p>The installation using Helm is fully automated and should succeed when the requirements are met.
After a few seconds, the ingress controller configured path should be responding with the login screen of an admin UI, see the <a class="xref" href="#administration" title="Administration">the section called “Administration”</a> section.</p><p>In case of issues, please make sure all pods in registry namespace are running and all deployments are in the Ready state.
If a pod fails to start, investigate the situation with your debugging tool of choice (<a class="link" href="https://k9scli.io/" target="_top">k9s</a>, or kubectl).</p><p>Private registry deploys in the given Kubernetes namespace multiple deployments and one statefulset, and all of them should be healthy.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-possible-problems"></a>Possible problems</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>As mentioned in the requirements, ingress-controller needs to be set up in your Kubernetes cluster</p></li><li class="listitem"><p>When accessing the web UI, the browser complains about unknown certificate.</p><p>Check the <a class="xref" href="#install-tls-security" title="Transport Layer Security (TLS) Setup">the section called “Transport Layer Security (TLS) Setup”</a> for a solution.</p></li><li class="listitem"><p>Images for various SUSE Private Registry components cannot be found, and Pods are not getting created.</p><p>Make sure registry.suse.com is accessible from your network.</p></li><li class="listitem"><p>Pods are not starting, most of them fail with <code class="literal">failed to connect to database</code> pod error.</p><p>Check the errors in the database pod. Does it complain with something like:</p><pre class="screen">FailedScheduling &lt;unknown&gt; default-scheduler pod has unbound immediate `PersistentVolumeClaims`</pre><p>(repeated 2 times)? Possible reason is that the <code class="literal">storageClass</code> is missing or no <code class="literal">storageClass</code> is marked as default.</p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="install-tls-security"></a>Transport Layer Security (TLS) Setup</h2></div></div></div><p>With default setup, the SUSE Private Registry public APIs (the UI and the OCI registry API) will both use an auto-generated certificate.
To avoid root certificate validation errors, the generated CA certificate should be installed on all hosts that will need to connect to it:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>All cluster nodes, if a Kubernetes cluster will be used to pull images from the SUSE Private Registry</p></li><li class="listitem"><p>All machines where supported OCI clients, such as docker engine and helm v3 are running, if those clients will be used to interact with the SUSE Private Registry</p></li></ul></div><p>The generated CA certificate is available for download in the SUSE Private Registry UI, under the Configuration / System Settings section:</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-get-ca-cert.png" width="100%" alt="registry harbor get ca cert" /></td></tr></table></div></div><p>Install the extracted CA certificate system-wide:</p><pre class="screen">su
cp harbor.ca /usr/share/pki/trust/anchors/
update-ca-certificates</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="administration"></a>Administration</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-setup-administration-access"></a>Setup Administration Access</h3></div></div></div><p>SUSE Private Registry User Interface can be accessed from a supported web browser at the location provided as <code class="literal">&lt;core_fqdn&gt;</code> during the installation.
Find out about the initial admin user password in <a class="xref" href="#install-passwords">[install-passwords]</a>.</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-landingpage.png" width="100%" alt="registry harbor landingpage" /></td></tr></table></div></div><p>After the first login, you can change the administrator’s password through the web UI. Select the <span class="strong"><strong>admin</strong></span> tab and select <span class="strong"><strong>Change Password</strong></span>.</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-admin-pw-change.png" width="100%" alt="registry harbor admin pw change" /></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="admin-configure-authentication"></a>Configuring Authentication</h3></div></div></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Switching Authentication Mode</h3><p>Once a user (besides the admin) is registered, or logs in when using LDAP/AD or UAA, SUSE Private Registry is locked in the current authentication mode meaning that it is not possible switch to a different authentication mode.
In that way, an authentication mode should be configured as soon as SUSE Private Registry is deployed.</p></div><p>Harbor supports different modes for authenticating users and managing user accounts. The following authentication modes are supported by SUSE Private Registry:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><span class="strong"><strong>Database (default)</strong></span>: User accounts are created/managed directly in SUSE Private Registry. The user accounts are stored on the SUSE Private Registry database.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP/Active Directory</strong></span>: SUSE Private Registry is configured to use an external external LDAP/Active Directory server for user authentication. The user accounts are created and managed by the LDAP/AD provider.</p></li><li class="listitem"><p><span class="strong"><strong>UAA</strong></span>: SUSE Private Registry is configured to authenticate using an external UAA provider. The user accounts are created and managed by the UAA provider.</p></li></ul></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-database-authentication"></a>Database Authentication</h4></div></div></div><p>In database authentication mode, user accounts are stored in the local database.
By default, only the SUSE Private Registry system administrator can create new user accounts. However, It is also possible to configure SUSE Private Registry to allow self-registration.</p><p>Configuring SUSE Private Registry with Database authentication mode:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Log in to the SUSE Private Registry interface with an account that has system administrator privileges.</p></li><li class="listitem"><p>Under <span class="strong"><strong>Administration</strong></span>, go to <span class="strong"><strong>Configuration</strong></span> and select the <span class="strong"><strong>Authentication</strong></span> tab.</p></li><li class="listitem"><p>Leave <span class="strong"><strong>Auth Mode</strong></span> set to the default <span class="strong"><strong>Database</strong></span> option.</p></li><li class="listitem"><p>(Optionally) Select the <span class="strong"><strong>Allow Self-Registration</strong></span> check box for allowing users to register themselves in SUSE Private Registry.
Self-registration is <span class="emphasis"><em>disabled by default</em></span>. If enabled unregistered users can sign up for a SUSE Private Registry account by clicking <span class="strong"><strong>Sign up for an account</strong></span> on the SUSE Private Registry log in page.</p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-ldapactive-directory-authentication"></a>LDAP/Active Directory Authentication</h4></div></div></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Note that self-registration, creating users, deleting users, changing passwords, and resetting passwords is not supported in LDAP/AD authentication mode as users are managed by LDAP/AD.</p></div><p>When using LDAP/AD authentication, users whose credentials are stored in an external LDAP or AD server can log in to SUSE Private Registry directly.
In this case, it is not necessary to create user accounts in SUSE Private Registry.</p><p>To be able to manage user authentication by using LDAP groups, it is required to enable the <code class="literal">memberof</code> feature on the LDAP/AD server.
With the <code class="literal">memberof</code> feature, the LDAP/AD user entity’s <code class="literal">memberof</code> attribute is updated when the group entity’s member attribute is updated, for example by adding or removing an LDAP/AD user from the LDAP/AD group.</p><p>The following steps describe how to enable LDAP/AD authentication mode:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Log in to the SUSE Private Registry interface with an account that has system administrator privileges.</p></li><li class="listitem"><p>Under <span class="strong"><strong>Administration</strong></span>, go to <span class="strong"><strong>Configuration</strong></span> and select the <span class="strong"><strong>Authentication</strong></span> tab.</p></li><li class="listitem"><p>Use the <span class="strong"><strong>Auth Mode</strong></span> drop-down menu to select <span class="strong"><strong>LDAP</strong></span>.</p></li><li class="listitem"><p>Enter the address of the LDAP server, for example <code class="literal">ldaps://10.84.5.171</code>.</p></li><li class="listitem"><p>Enter information about the LDAP server as follows:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p><span class="strong"><strong>LDAP Search DN</strong></span> and <span class="strong"><strong>LDAP Search Password</strong></span>: When a user logs in to SUSE Private Registry with their LDAP username and password, it uses these values to bind to the LDAP/AD server.
For example, <code class="literal">cn=admin,dc=example.com</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Base DN</strong></span>: SUSE Private Registry looks up the user under the LDAP Base DN entry, including the subtree. For example, <code class="literal">dc=example.com</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Filter</strong></span>: The filter to search for LDAP/AD users. For example, <code class="literal">objectclass=user</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP UID</strong></span>: An attribute, for example uid, or cn, that is used to match a user with the username.
If a match is found, the user’s password is verified by a bind request to the LDAP/AD server.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Scope</strong></span>: The scope to search for LDAP/AD users. Select from <span class="strong"><strong>Subtree</strong></span>, <span class="strong"><strong>Base</strong></span>, and <span class="strong"><strong>OneLevel</strong></span>.</p></li></ol></div></li><li class="listitem"><p>To be able to manage user authentication with LDAP groups, configure the group settings:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p><span class="strong"><strong>LDAP Group Base DN</strong></span>: The base DN from which to lookup a group in LDAP/AD. For example, <code class="literal">ou=groups,dc=example,dc=com</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Group Filter</strong></span>: The filter to search for LDAP/AD groups. For example, <code class="literal">objectclass=groupOfNames</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Group GID</strong></span>: The attribute used to name an LDAP/AD group. For example, <code class="literal">cn</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Group Admin DN</strong></span>: All LDAP/AD users in this group DN have Harbor system administrator privileges.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Group Membership</strong></span>: The user attribute usd to identify a user as a member of a group. By default this is <code class="literal">memberof</code>.</p></li><li class="listitem"><p><span class="strong"><strong>LDAP Scope</strong></span>: The scope to search for LDAP/AD groups. Select from <span class="strong"><strong>Subtree</strong></span>, <span class="strong"><strong>Base</strong></span>, and <span class="strong"><strong>OneLevel</strong></span>.</p></li></ol></div></li><li class="listitem"><p>Uncheck <span class="strong"><strong>LDAP Verify Cert</strong></span> if the LDAP/AD server uses a self-signed or untrusted certificate.</p></li><li class="listitem"><p>Click <span class="strong"><strong>Test LDAP Server</strong></span> to make sure that your configuration is correct.</p></li><li class="listitem"><p>Click <span class="strong"><strong>Save</strong></span> to complete the configuration.</p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-uaa-authentication"></a>UAA Authentication</h4></div></div></div><p>By configuring UAA authentication, users whose credentials are stored in an external UAA server can log in to SUSE Private Registry directly. In this case, it is not necessary to create user accounts in SUSE Private Registry. Note that just like LDAP authentication mode, self-registration, creating users, deleting users, changing passwords, and resetting passwords are not supported in UAA authentication mode as users are managed by UAA.</p><p>The following steps describe how to configure UAA authentication mode:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Register a client account on UAA. For example, using the UAA CLI and assuming the UAA server is available at <code class="literal">http://10.83.7.181:8080</code>:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Configure UAA CLI to target the UAA server and login as admin:</p><pre class="screen">$ uaac target http://10.83.7.181:8080/uaa</pre><pre class="screen">$ uaac token client get admin -s &lt;admin_secret&gt; # replace &lt;admin_secret&gt; with the secret of the admin user</pre></li><li class="listitem"><p>Register a client account for SUSE Private Registry:</p><pre class="screen">$ uaac client add suse_private_registry -s suseprivateregistrysupersecret --scope uaa.user --authorized_grant_types client_credentials,password --authorities oauth.login</pre></li></ol></div></li><li class="listitem"><p>Log in to the SUSE Private Registry interface with an account that has system administrator privileges.</p></li><li class="listitem"><p>Under <span class="strong"><strong>Administration</strong></span>, go to <span class="strong"><strong>Configuration</strong></span> and select the <span class="strong"><strong>Authentication</strong></span> tab.</p></li><li class="listitem"><p>Use the <span class="strong"><strong>Auth Mode</strong></span> drop-down menu to select UAA.</p></li><li class="listitem"><p>Enter the address of the UAA server token endpoint, for example <code class="literal">http://10.83.7.181:8080/uaa/oauth/token</code></p></li><li class="listitem"><p>Enter information about the UAA client account as follows:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p><span class="strong"><strong>UAA Client ID</strong></span>: The client account ID. For example <code class="literal">suse_private_registry</code> as created on step 1.</p></li><li class="listitem"><p><span class="strong"><strong>UAA Client Secret</strong></span>: The client account secret. For example <code class="literal">suseprivateregistrysupersecret</code> as created on step 1.</p></li></ol></div></li><li class="listitem"><p>Uncheck <span class="strong"><strong>UAA Verify Cert</strong></span> if the UAA server uses a self-signed or untrusted certificate.</p></li><li class="listitem"><p>Click <span class="strong"><strong>Save</strong></span> to complete the configuration.</p></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-registry-deployment-configuration-changes"></a>Registry Deployment Configuration Changes</h3></div></div></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Updating the Registry Deployment Configuration</h3><p>Changing the deployment configuration of a running SUSE Private Registry instance involves running <code class="literal">helm upgrade</code> in some form or other. The <code class="literal">harbor-values.yaml</code> file used during installation to provide the initial SUSE Private Registry configuration should be treated as the source of truth during all subsequent deployment configuration changes and upgrade operations.
For all SUSE Private Registry configuration change operations documented in this section, it is therefore highly recommended that the <code class="literal">harbor-values.yaml</code> be updated accordingly, and that the file be supplied to the <code class="literal">helm upgrade</code> command, instead of using additional <code class="literal">--set</code> command line arguments that are not be persisted.</p><p>Disregarding this recommendation may lead to situation in which the configuration of the running SUSE Private Registry installation is no longer in sync with the configuration described in the <code class="literal">harbor-values.yaml</code> file, which will cause unexpected configuration changes during upgrade operations.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Some SUSE Private Registry deployment configuration changes require restarting one or several of the registry components. To reduce service downtime while configuration changes are being applied, it is recommended to run SUSE Private Registry in high-availability mode (see <a class="xref" href="#high-availability" title="High Availability">the section called “High Availability”</a> for more information).</p></div><p>Examples of supported post-installation deployment configuration changes, some of which are further documented in the sub-sections that follow:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Enabling or disabling internal TLS</p></li><li class="listitem"><p>Adding custom CA certificate bundles into the trust store used by SUSE Private Registry components</p></li><li class="listitem"><p>Rotating TLS certificates</p></li><li class="listitem"><p>Increasing the size of Kubernetes persistent volumes used by SUSE Private Registry components</p></li><li class="listitem"><p>Changing passwords, tokens and access keys</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>The password for the database admin</p></li><li class="listitem"><p>The password for the redis default account</p></li><li class="listitem"><p>The github token used by trivy to regularly update the vulnerability database</p></li></ul></div></li><li class="listitem"><p>Changing the update strategy used for rolling updates</p></li><li class="listitem"><p>Changing the scale (replica count) for SUSE Private Registry services</p></li><li class="listitem"><p>Changing the Kubernetes service accounts associated with SUSE Private Registry pods</p></li><li class="listitem"><p>Enable or disable the <code class="literal">notary</code> component</p></li><li class="listitem"><p>Enable or disable <code class="literal">trivy</code> component</p></li></ul></div><p>The following post-installation deployment configuration changes are not supported and require a full SUSE Private Registry re-installation:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Changing the storage type for the OCI artifact storage</p></li><li class="listitem"><p>Changing the storage class for Kubernetes persistent volumes</p></li><li class="listitem"><p>Decreasing the size of Kubernetes persistent volumes used by SUSE Private Registry components</p></li><li class="listitem"><p>Replacing the redis service</p></li><li class="listitem"><p>Replacing the database service</p></li></ul></div><p>Helm configuration changes can usually be applied by updating the SUSE Private Registry <code class="literal">harbor-values.yaml</code> configuration file used during installation with the new configuration values and then running <code class="literal">helm upgrade</code> to apply the changes. Cases that required additional steps are explicitly documented in the sub-sections that follow.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3><p>The full list of deployment configuration options and default values that can overridden in the <code class="literal">harbor-values.yaml</code> file is included in the helm chart itself and can be viewed in YAML format by running the following command:</p><pre class="screen">helm show values harbor</pre><p>The SUSE Private Registry Deployment section also contains extended information on the most relevant helm configuration options.
Those configuration options can be customized not only during installation but also updated post-installation, with the exceptions documented earlier in this section as not supported.</p></div><p>For example, to enable internal TLS, <code class="literal">notary</code> and <code class="literal">trivy</code> in one go (assuming they are all currently disabled), update the <code class="literal">harbor-values.yaml</code> configuration file and run <code class="literal">helm upgrade</code> as follows:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">internalTLS:
  enabled: true
trivy:
  enabled: true
  replicas: 3
  gitHubToken: "&lt;github-auth-token&gt;"
notary:
  enabled: true
  server:
    replicas: 3
  signer:
    replicas: 3</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-changing-the-database-password"></a>Changing the Database Password</h4></div></div></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Caution</h3><p>Changing the database password is an administrative operation that has impact on service availability.</p></div><p>The password for the SUSE Private Registry internal or external database service can be changed in three steps:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>First, shutdown all SUSE Private Registry services that use the database, to eliminate the risk of incomplete or failed transactions:</p><pre class="screen">kubectl -n registry scale deployment -l component=core --replicas=0
kubectl -n registry scale deployment -l component=notary-server --replicas=0
kubectl -n registry scale deployment -l component=notary-signer --replicas=0</pre></li><li class="listitem"><p>Change the password for the database server</p><p>For an external database, use the means available from the public cloud provider to set a new admin password.</p><p>For the internal database, the easiest way to do this is by accessing the database pod via <code class="literal">kubectl exec</code> and running a <code class="literal">psql</code> command to change the password, e.g.:</p><pre class="screen">kubectl -n registry exec -ti harbor-harbor-database-0 -- psql
psql (12.4)
Type "help" for help.

postgres=# \password
Enter new password: &lt;new-password-value&gt;
Enter it again: &lt;new-password-value&gt;
postgres=# \q</pre></li><li class="listitem"><p>Update the SUSE Private Registry <code class="literal">harbor-values.yaml</code> configuration file with the new password value and run <code class="literal">helm upgrade</code> to apply the change and start the services that were stopped at the first step:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">database:
  internal:
    ...
    # use this field for the internal database
    password: "&lt;new password value&gt;"
  external:
    ...
    # use this field for the external database
    password: "&lt;new password value&gt;"</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-changing-the-redis-password"></a>Changing the Redis Password</h4></div></div></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Caution</h3><p>Changing the redis password is an administrative operation that has impact on service availability.</p></div><p>The password for the SUSE Private Registry redis service can be changed in two steps:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>First, change the password for the redis service itself</p><p>For an internal redis service, nothing needs to be done at this step.</p><p>If you’re running an external public cloud redis service, change the external redis password using the means available from the public cloud provider.</p><p>For a redis service deployed using the SUSE redis operator, the password can be changed as follows:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Update the secret created during installation with the new password:</p><pre class="screen">helm -n registry delete secret redis-auth
kubectl -n registry create secret generic redis-auth --from-literal=password="&lt;new-password-value&gt;"</pre></li><li class="listitem"><p>Delete the running redis statefulset to force a configuration update:</p><pre class="screen">helm -n registry delete statefulset -l app.kubernetes.io/component=redis</pre></li></ol></div></li><li class="listitem"><p>Update the SUSE Private Registry <code class="literal">harbor-values.yaml</code> configuration file with the new password value and run <code class="literal">helm upgrade</code> to apply the change:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">redis:
  internal:
    ...
    # use this field for the internal redis
    password: "&lt;new password value&gt;"
  external:
    ...
    # use this field for the external redis
    password: "&lt;new password value&gt;"</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-rotating-autogenerated-tls-certificates"></a>Rotating Autogenerated TLS Certificates</h4></div></div></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Rotate certificates on minor version upgrades</h3><p>Certificate rotation is an administrative operation that impacts service availability.
The certificates auto-generated by helm have a validity of 365 days, sufficient to not require rotating them too frequently.
It is advised that all auto-generated certificates be rotated with every upgrade operation consisting in a minor or major version number change, to avoid loss of operation, but it is not required to do so more frequently.</p></div><p>The SUSE Private Registry helm chart provides the option to auto-generate certificates, if custom certificates aren’t explicitly provided. This applies to the following certificates and their use:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>TLS certificates for the publicly exposed APIs: the Harbor UI/API and the notary API</p><p>A single certificate is generated for both endpoints, if <code class="literal">tls.certSource</code> is set to <code class="literal">auto</code> in the helm chart configuration.</p></li><li class="listitem"><p>TLS certificates for the internal communication</p><p>A certificate is generated for every SUSE Private Registry component that exposes an API consumed internally by other components (<code class="literal">core</code>, <code class="literal">jobservice</code>, <code class="literal">registry</code>, <code class="literal">portal</code> and <code class="literal">trivy</code>), if <code class="literal">internalTLS.enabled</code> is set to <code class="literal">true</code> and <code class="literal">internalTLS.certSource</code> is set to <code class="literal">auto</code> in the helm chart configuration.</p></li><li class="listitem"><p>A TLS certificate is used to secure the <code class="literal">notary-signer</code> internal API</p><p>This is handled independently of the global <code class="literal">internalTLS.enable</code> flag controlling internal TLS for other SUSE Private Registry components, because, for technical reasons, internal TLS cannot be disabled for the <code class="literal">notary-signer</code> component.
A certificate is automatically generated unless <code class="literal">notary.secretName</code> is set to point to a predefined secret providing a custom TLS certificate for this component.</p></li><li class="listitem"><p>A TLS certificate and private key pair are used by the SUSE Private Registry <code class="literal">core</code> component to generate encryption/decryption tokens for use by robot accounts</p><p>A certificate is automatically generated unless <code class="literal">core.secretName</code> is set to point to a predefined secret providing a custom TLS certificate and private key pair for this purpose.</p></li></ul></div><p>By default, auto-generated TLS certificates are created during the initial SUSE Private Registry installation and kept unchanged during subsequent helm runs.
To re-generate these TLS certificates, the relevant <code class="literal">rotateCert</code> configuration flags need to be explicitly set during the helm runs, as detailed in the remainder of this section.</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>Rotating the certificates autogenerated for the Harbor UI/API and notary API will invalidate the CA certificates already configured on the remote hosts where these SUSE Private Registry services are accessed. See the <a class="xref" href="#install-tls-security" title="Transport Layer Security (TLS) Setup">the section called “Transport Layer Security (TLS) Setup”</a> section for details on how to reconfigure these hosts.</p></div><p>To rotate the TLS certificates auto-generated for the publicly exposed APIs, run:</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml --set expose.tls.auto.rotateCert=true</pre><p>This operation can be performed with zero downtime, the SUSE Private Registry services themselves are not impacted by it.</p><p>To rotate the TLS certificates auto-generated for the internal communication (including <code class="literal">notary-server</code>), run:</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml --set internalTLS.rotateCert=true --set notary.rotateCert=true</pre><p>This operation requires all SUSE Private Registry components to be updated</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>Rotating the TLS certificate and private key pair autogenerated for encryption/decryption of tokens for robot accounts will invalidate the existing tokens.</p></div><p>To re-generate the TLS certificate and private key pair used for encryption/decryption of tokens for robot accounts, run:</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml --set core.rotateCert=true</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-scaling-the-registry-services"></a>Scaling the Registry Services</h4></div></div></div><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>A Kubernetes <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access mode is required to enable high-availability for the SUSE Private Registry <code class="literal">registry</code> component, if a Kubernetes persistent volume is used as the storage back-end for OCI artifacts.
If a <code class="literal">StorageClass</code> with <code class="literal">ReadWriteMany</code> access is not configured for these components, setting the replica count to a value higher than 1 for them will result in failure.</p></div><p>To change the scale parameters for the internal components of a running SUSE Private Registry instance, update the <code class="literal">harbor-values.yaml</code> configuration file with new replica values, as desired, and then apply the change by running <code class="literal">helm upgrade</code> with the same parameters used during installation:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">portal:
  replicas: 3
core:
  replicas: 3
# Only enabled when using a LoadBalancer instead of Ingress to expose services
nginx:
  replicas: 3
jobservice:
  replicas: 3
registry:
  replicas: 3
trivy:
  replicas: 3
notary:
  server:
    replicas: 3
  signer:
    replicas: 3</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre><p>Alternatively, <code class="literal">kubectl</code> may be used directly to scale SUSE Private Registry components individually, but special care should be taken to keep the <code class="literal">harbor-values.yaml</code> file updated to reflect the running configuration, otherwise subsequent configuration changes or upgrade operations that require running <code class="literal">helm upgrade</code> will revert the number of replicas back to the known configuration. For example, to scale the <code class="literal">portal</code> component to a new value of 3 pods, the following command may be used:</p><pre class="screen">kubectl -n registry scale deployment -l component=portal --replicas=3</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-expanding-persistent-volumes-claims"></a>Expanding Persistent Volumes Claims</h4></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Expanding Volumes Containing a File System</h3><p>It is only possible to resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.</p><p>When a volume contains a file system, the file system is only resized when a new Pod is using the <code class="literal">PersistentVolumeClaim</code> in <code class="literal">ReadWrite</code> mode.
File system expansion is either done when a Pod is starting up or when a Pod is running and the underlying file system supports online expansion.</p></div><div class="caution" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Risk of Data Loss</h3><p>It is extremely advised to perform a backup of the existing volumes that will be resized before taking any action as there is a risk of permanent data loss.</p></div><p>Only specific storage providers offer support for expanding <code class="literal">PersistentVolumeClaims</code> (PVCs).
Before taking any action, it is recommended to check the documentation of the storage provider available for your Kubernetes cluster and make sure that it supports expanding PVCs.</p><p>To be able to expand a PVC the storage class’s <code class="literal">allowVolumeExpansion</code> field needs to be set to true. For example:</p><pre class="screen">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: persistent
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: kubernetes.io/cinder
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true</pre><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-expanding-volumes-managed-by-deployments-registry-jobservice"></a>Expanding Volumes Managed by Deployments (<code class="literal">registry</code>, <code class="literal">jobservice</code>)</h5></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Storage backend without support for online expansion</h3><p>If the storage backend does not support online expansion, additional steps that impact the service availability are required to conclude the resizing.</p></div><p>To resize the PVC for the registry and the <code class="literal">jobservice</code> components of SUSE Private Registry, update the <code class="literal">harbor-values.yaml</code> configuration file with the new storage sizes for the <code class="literal">registry</code> and <code class="literal">jobservice</code> components, then apply the change by running <code class="literal">helm upgrade</code> with the same parameters used during installation, e.g.:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">persistence:
  persistentVolumeClaim:
    registry:
      ...
      size: 100Gi
    jobservice:
      ...
      size: 5Gi</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre><p>The above command will set the PVC size of the <code class="literal">jobservice</code> component to 5 gigabytes and 100 gigabytes for the <code class="literal">registry</code> PVC.</p><p>If the storage backend supports online expansion the PVCs will be automatically resized and no additional action is needed.
However, If the storage backend does not support online expansion additional steps are required to conclude the volume resize which includes deleting the pods that are using the volume being resized, waiting for the volume to be resized and finally starting new pods. For example, to finalize the resize of the <code class="literal">jobservice</code> PVC when volume online expansion is not supported:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Check the status of the PVC to make sure it is waiting for the volume to be detached to perform the resize:</p><pre class="screen">kubectl -n registry describe pvc -l component=jobservice | sed -n -e '/Conditions/,$p'
Conditions:
  Type       Status  LastProbeTime                     LastTransitionTime                Reason  Message
  ----       ------  -----------------                 ------------------                ------  -------
  Resizing   True    Mon, 01 Jan 0001 00:00:00 +0000   Fri, 23 Oct 2020 17:56:33 +0200
Events:
  Type     Reason                 Age                 From                         Message
  ----     ------                 ----                ----                         -------
  Normal   ProvisioningSucceeded  2m34s               persistentvolume-controller  Successfully provisioned volume pvc-297dfa22-0711-4b43-bea0-cdb3684bc2a0 using kubernetes.io/&lt;plugin&gt;
  Warning  VolumeResizeFailed     31s (x13 over 73s)  volume_expand                error expanding volume "suse-registry/suse-registry-harbor-jobservice" of plugin "kubernetes.io/&lt;plugin&gt;": volume in in-use status can not be expanded, it must be available and not attached to a node</pre></li><li class="listitem"><p>Set the number of replicas of the <code class="literal">jobservice</code> deployment to 0 (this will delete the <code class="literal">jobservice</code> pods and the service will be unavailable):</p><pre class="screen">kubectl -n registry scale deployment -l component=jobservice --replicas=0
deployment.apps/suse-registry-harbor-jobservice scaled</pre></li><li class="listitem"><p>Check the status of the PVC, wait until the volume resize is complete and its just waiting for the pod to start to finish resizing the file system:</p><pre class="screen">kubectl -n registry describe pvc -l component=jobservice | sed -n '/Conditions/,/Events/p'
Conditions:
  Type                      Status  LastProbeTime                     LastTransitionTime                Reason  Message
  ----                      ------  -----------------                 ------------------                ------  -------
  FileSystemResizePending   True    Mon, 01 Jan 0001 00:00:00 +0000   Fri, 23 Oct 2020 18:02:03 +0200           Waiting for user to (re-)start a pod to finish file system resize of volume on node.</pre></li><li class="listitem"><p>Set the number of replicas back to the previous value (1 in this case) to conclude the resize:</p><pre class="screen">kubectl -n registry scale deployment -l component=jobservice --replicas=1
deployment.apps/suse-registry-harbor-jobservice scaled</pre></li><li class="listitem"><p>Confirm that the file system resize has finished successfully:</p><pre class="screen">kubectl -n registry describe pvc -l component=jobservice | sed -n -e '/Events/,$p'
Events:
...
  Normal   FileSystemResizeSuccessful  52s                   kubelet, caasp-worker-eco-caasp4-upd-eco-2                MountVolume.NodeExpandVolume succeeded for volume "pvc-297dfa22-0711-4b43-bea0-cdb3684bc2a0"</pre></li></ol></div><p>The same steps can be followed to conclude expanding the <code class="literal">registry</code> PVC by replacing <code class="literal">component=jobservice</code> with <code class="literal">component=registry</code> on each command.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-expanding-volumes-managed-by-statefulsets-database-redis-and-trivy"></a>Expanding Volumes Managed by <code class="literal">StatefulSets</code> (<code class="literal">database</code>, <code class="literal">redis</code> and <code class="literal">trivy</code>)</h5></div></div></div><p>Kubernetes does not officially support volume expansion through <code class="literal">StatefulSets</code>, trying to do so by using helm with new values for PVC size will throw the following error:</p><pre class="screen">Error: UPGRADE FAILED: cannot patch "suse-registry-release-12-harbor-trivy" with kind StatefulSet: StatefulSet.apps "suse-registry-release-12-harbor-trivy" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden</pre><p>This means that the <code class="literal">volumeClaimTemplates</code> field of a <code class="literal">StatefulSet</code> is immutable and cannot be updated with a new value for size.
In that way, extra actions are required to perform the resize of PVCs managed by <code class="literal">StatefulSets</code>.</p><p>The following steps describe how to expand volumes managed by <code class="literal">SatefulSets</code> using the <code class="literal">trivy</code> component as an example.
The same steps can be performed also for the database and <code class="literal">redis</code> components of SUSE Private Registry just by replacing <code class="literal">trivy</code> for database or <code class="literal">redis</code> on each command:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Delete the StatefulSet while keeping the pods running together with any other resource that was managed by the StatefulSet such as the PVC.
This can be done by setting <code class="literal">--cascade=false</code> to the <code class="literal">kubectl delete</code> command, for example:</p><pre class="screen">kubectl -n registry delete sts --cascade=false -l component=trivy
statefulset.apps "suse-registry-harbor-trivy" deleted</pre></li><li class="listitem"><p>Edit the PVC spec with the new size (10 gigabytes in this example), this can be done in many different ways. For example using <code class="literal">kubectl</code> patch:</p><pre class="screen">NEW_SIZE="10Gi"
NAMESPACE="registry"
# depending on the number of replicas, trivy can have more than one PVC.
for pvc in $(kubectl -n $NAMESPACE get pvc -l component=trivy -o name); do
  kubectl -n $NAMESPACE patch $pvc -p "{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"$NEW_SIZE\"}}}}"
done

persistentvolumeclaim/data-suse-registry-harbor-trivy-0 patched
persistentvolumeclaim/data-suse-registry-harbor-trivy-1 patched</pre></li><li class="listitem"><p>Update the <code class="literal">harbor-values.yaml</code> configuration file with the new storage size for the intended component, then apply the change by running <code class="literal">helm upgrade</code> with the same parameters used during installation, to re-define the <code class="literal">StatefulSets</code> with the new size to keep consistency. For trivy, for example:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">persistence:
  persistentVolumeClaim:
    trivy:
      ...
      size: 10Gi</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre></li></ol></div><p>Just like for deployments, if the storage backend supports online expansion the PVCs will be automatically resized and no additional action is needed. However, If the storage backend does not support online expansion additional steps are required to conclude the volume resize which includes deleting the pods that are using the volume being resized, waiting for the volume to be resized and finally starting new pods. For example, to finalize the resize of the trivy PVC when volume online expansion is not supported:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Check the status of the PVCs to make sure it is waiting for the volume to be detached to perform the resize:</p><pre class="screen">kubectl -n registry describe pvc -l component=trivy | sed -n -e '/Conditions/,$p'
Conditions:
  Type       Status  LastProbeTime                     LastTransitionTime                Reason  Message
  ----       ------  -----------------                 ------------------                ------  -------
  Resizing   True    Mon, 01 Jan 0001 00:00:00 +0000   Mon, 26 Oct 2020 13:29:58 +0100
Events:
  Type     Reason                 Age                   From                         Message
  ----     ------                 ----                  ----                         -------
  Normal   ProvisioningSucceeded  8m8s                  persistentvolume-controller  Successfully provisioned volume pvc-8fe4a4b6-83c8-47d0-a266-f8cdbd4e3918 using kubernetes.io/&lt;plugin&gt;
  Warning  VolumeResizeFailed     28s (x17 over 5m57s)  volume_expand                error expanding volume "suse-registry/data-suse-registry-harbor-trivy-0" of plugin "kubernetes.io/&lt;plugin&gt;": volume in in-use status can not be expanded, it must be available and not attached to a node</pre></li><li class="listitem"><p>Set the number of replicas of the trivy statefulset to 0 (this will delete the trivy pods and the service will be unavailable):</p><pre class="screen">kubectl -n registry scale sts -l component=trivy --replicas=0
statefulset.apps/suse-registry-harbor-trivy scaled</pre></li><li class="listitem"><p>Check the status of the PVC, wait until the volume resize is complete and its just waiting for the pod to start to finish resizing the file system:</p><pre class="screen">kubectl -n registry describe pvc -l component=trivy | sed -n '/Conditions/,/Events/p'
Conditions:
  Type                      Status  LastProbeTime                     LastTransitionTime                Reason  Message
  ----                      ------  -----------------                 ------------------                ------  -------
  FileSystemResizePending   True    Mon, 01 Jan 0001 00:00:00 +0000   Mon, 26 Oct 2020 13:40:55 +0100           Waiting for user to (re-)start a pod to finish file system resize of volume on node.</pre></li><li class="listitem"><p>Set the number of replicas back to the previous value (2 in this case) to conclude the resize:</p><pre class="screen">kubectl -n registry scale sts -l component=trivy --replicas=2
deployment.apps/suse-registry-harbor-jobservice scaled</pre></li><li class="listitem"><p>Confirm that the file system resize has finished successfully:</p><pre class="screen">kubectl -n registry describe pvc -l component=trivy | sed -n -e '/Events/,$p'
Events:
...
  Normal   FileSystemResizeSuccessful  64s                   kubelet, caasp-worker-eco-caasp4-upd-eco-2  MountVolume.NodeExpandVolume succeeded for volume "pvc-8fe4a4b6-83c8-47d0-a266-f8cdbd4e3918"</pre></li></ol></div></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-installing-maintenance-updates"></a>Installing Maintenance Updates</h3></div></div></div><p>SUSE Private Registry maintenance updates containing new helm chart and container image versions are regularly published to address security vulnerabilities and to fix critical bugs.
These updates do not introduce new features and therefore can be applied in a manner similar to regular helm configuration changes, with minimal disruption to service availability.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>It is highly recommended to install SUSE Private Registry maintenance updates regularly and as frequently as possible, to keep your SUSE Private Registry instance up to date with the latest security patches and fixes for functionality impairing issues.</p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Installing SUSE Private Registry maintenance updates require restarting one or several of the registry components.
To minimize service downtime while the update is being applied, it is recommended to run SUSE Private Registry in high-availability mode (see <a class="xref" href="#high-availability" title="High Availability">the section called “High Availability”</a> for more information).</p></div><p>The <code class="literal">harbor-values.yaml</code> file used during installation to provide the initial SUSE Private Registry configuration, as well as during subsequent helm configuration changes is also required to install maintenance updates.</p><p>To check for SUSE Private Registry maintenance updates and subsequently install them:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Download the newest Helm chart version from the official SUSE repository:</p><pre class="screen">export HELM_EXPERIMENTAL_OCI=1
# download a chart from public registry
helm chart pull registry.suse.com/harbor/harbor:1.5
# export the chart to local directory
helm chart export registry.suse.com/harbor/harbor:1.5</pre><p>The output from the <code class="literal">helm chart pull</code> command will indicate the helm chart version that is available in the public registry. Make a note of it:</p><pre class="screen">1.5: Pulling from registry.suse.com/harbor/harbor
ref:     registry.suse.com/harbor/harbor:1.5
digest:  db4731ab843d9837eb83327b735a7c5c19826e225858333a3b9a57668d5d40b8
size:    178.1 KiB
name:    harbor
version: 1.5.2
Status: Chart is up to date for registry.suse.com/harbor/harbor:1.5</pre></li><li class="listitem"><p>Verify the version of the running SUSE Private Registry instance:</p><pre class="screen">&gt; helm -n registry list
NAME         	NAMESPACE    	REVISION	UPDATED                              	STATUS  	CHART       	APP VERSION
suse-registry	registry	6       	2020-11-17 15:20:46.037254 +0200 CEST	deployed	harbor-1.5.1	2.1.1</pre><p>If the helm chart version displayed is the same as the one available from the public registry, your SUSE Private Registry instance is up to date and nothing else needs to be done. Otherwise, maintenance updates are available and can be installed.</p></li><li class="listitem"><p>Trigger the upgrade using the <code class="literal">harbor-values.yaml</code> configuration file:</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre></li><li class="listitem"><p>Check the installation</p><p>The SUSE Private Registry update will take a while to complete, while new pods are being created to replace old pods. You can check the status and see if everything is running correctly, e.g.:</p><pre class="screen">&gt; kubectl -n registry get deployments
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
suse-registry-harbor-core         2/2     2            2           32d
suse-registry-harbor-jobservice   2/2     2            2           32d
suse-registry-harbor-portal       2/2     2            2           32d
suse-registry-harbor-registry     1/1     1            1           32d</pre><pre class="screen">&gt; kubectl -n registry get statefulset
NAME                            READY   AGE
suse-registry-harbor-database   1/1     32d
suse-registry-harbor-redis      1/1     32d
suse-registry-harbor-trivy      2/2     4d1h</pre><pre class="screen">&gt; kubectl -n registry get pod
NAME                                               READY   STATUS    RESTARTS   AGE
suse-registry-harbor-core-85845f9777-5rkbb         1/1     Running   0          24h
suse-registry-harbor-core-85845f9777-krwk2         1/1     Running   0          24h
suse-registry-harbor-database-0                    1/1     Running   0          24h
suse-registry-harbor-jobservice-7f954f9466-66b2r   1/1     Running   0          24h
suse-registry-harbor-jobservice-7f954f9466-6n96t   1/1     Running   0          24h
suse-registry-harbor-portal-76b465644f-4zmxw       1/1     Running   0          24h
suse-registry-harbor-portal-76b465644f-lndlm       1/1     Running   0          24h
suse-registry-harbor-redis-0                       1/1     Running   0          24h
suse-registry-harbor-registry-65854df7bc-mrfnj     2/2     Running   0          24h
suse-registry-harbor-trivy-0                       1/1     Running   1          24h
suse-registry-harbor-trivy-1                       1/1     Running   0          24h</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-backup-and-restore"></a>Backup and Restore</h3></div></div></div><p>Backup and restore for SUSE Private Registry can be performed by <a class="link" href="https://documentation.suse.com/suse-caasp/4.2/html/caasp-admin/_backup_and_restore_with_velero.html" target="_top">Velero</a>. Velero not only handles this, but also disaster recovery, and the migration of Kubernetes cluster resources and persistent volumes.</p><p>When SUSE Private Registry is deployed together with its internal services (database, Redis and filesystem storage for image and artifacts) Velero is able to fully cover the backup and restore. However, as Velero is only responsible for Kubernetes resources, when SUSE Private Registry is deployed using external service providers, such as managed PostgreSQL database or object storage (Amazon S3 or Azure Blob Storage), additional actions must be performed to ensure the backup of data that is external to the Kubernetes cluster.</p><p>In general, the backup and restore process for SUSE Private Registry can be split in two parts:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Backup and restore of Kubernetes resources (performed by Velero):</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Namespace</p></li><li class="listitem"><p>Deployment</p></li><li class="listitem"><p>ReplicaSet</p></li><li class="listitem"><p>StatefulSet</p></li><li class="listitem"><p>Endpoint</p></li><li class="listitem"><p>Service</p></li><li class="listitem"><p>Ingress</p></li><li class="listitem"><p>ConfigMap</p></li><li class="listitem"><p>Secret</p></li><li class="listitem"><p>PersistentVolumeClaim</p></li><li class="listitem"><p>PersistentVolume</p></li><li class="listitem"><p>Pod</p></li><li class="listitem"><p>Other CRDs (e.g. <code class="literal">Certificate</code> when using <code class="literal">cert-manager</code> or <code class="literal">RedisFailover</code> when using <code class="literal">redis-operator</code>)</p></li></ul></div></li><li class="listitem"><p>Backup and restore of external services:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Database</p></li><li class="listitem"><p>Redis</p></li><li class="listitem"><p>Object Storage</p></li></ul></div></li></ul></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-backup-and-restore-of-kubernetes-resources"></a>Backup and restore of Kubernetes resources</h4></div></div></div><p>The following steps describe how to perform the backup and restore of Kubernetes resources using Velero.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-backup"></a>Backup</h5></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install Velero as described in the <a class="link" href="https://documentation.suse.com/suse-caasp/4.2/html/caasp-admin/_backup_and_restore_with_velero.html" target="_top">documentation</a></p></li><li class="listitem"><p>(optional) If you are using a volume provider that does not support volume snapshots or a volume-snapshot API, or does not have Velero-supported storage plugin, then Velero uses <a class="link" href="https://restic.net/" target="_top">Restic</a> as a generic solution to backing and restoring this sort of persistent volume. To perform a Restic backup of persistent volumes, Velero requires the addition of a specific annotation (<code class="literal">backup.velero.io/backup-volumes=&lt;VOLUME_NAME_1&gt;,&lt;VOLUME_NAME_2&gt;,…​</code>) to the pods that have the volumes mounted. This can be achieved by adding the following entries to the SUSE Private Registry’s Helm chart (<code class="literal">harbor-values.yaml</code>), then performing <code class="literal">helm upgrade</code> to apply the annotations:</p><p><strong>harbor-values.yaml. </strong>
</p><pre class="screen">registry:
  podAnnotations:
    backup.velero.io/backup-volumes: registry-data
trivy:
  podAnnotations:
    backup.velero.io/backup-volumes: data
database:
  podAnnotations:
    backup.velero.io/backup-volumes: database-data
redis:
  podAnnotations:
    backup.velero.io/backup-volumes: data</pre><p>
</p><pre class="screen">helm -n registry upgrade suse-registry ./harbor -f harbor-values.yaml</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">External Services</h3><p>If using an external service, such as a managed PostgreSQL database, you do not need to add the <code class="literal">database</code> entry. The same applies for <code class="literal">redis</code> when using an external Redis, and for <code class="literal">registry</code> when using an external storage back-end for storing images and artifacts.</p></div></li><li class="listitem"><p>Back-up the cluster with the command <code class="literal">velero backup create &lt;backup-name&gt; --include-namespaces &lt;namespace-to-backup&gt;</code>. For example:</p><pre class="screen">&gt; velero backup create registry-backup --include-namespaces registry
Backup request "registry-backup" submitted successfully.
Run `velero backup describe registry-backup` or `velero backup logs registry-backup` for more details.</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scope of the backup</h3><p>Velero supports backing up all Kubernetes cluster resources; however, in this case, the backup is performed for the registry namespace only. In some cases, such as when using <code class="literal">cert-manager</code> or <code class="literal">redis-operator</code>, you might need to include those namespaces in the backup.</p></div></li><li class="listitem"><p>To check the backup status and ensure its completion, run <code class="literal">velero backup describe &lt;backup-name&gt;</code>. For example:</p><pre class="screen">&gt; velero backup describe registry-backup
Name:         registry-backup
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  &lt;none&gt;

Phase:  Completed

Namespaces:
  Included:  registry
  Excluded:  &lt;none&gt;

Resources:
  Included:        *
  Excluded:        &lt;none&gt;
  Cluster-scoped:  auto

Label selector:  &lt;none&gt;

Storage Location:  default

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  &lt;none&gt;

Backup Format Version:  1

Started:    2020-12-17 12:00:12 +0100 CET
Completed:  2020-12-17 12:01:36 +0100 CET

Expiration:  2021-01-16 12:00:12 +0100 CET

Persistent Volumes: &lt;none included&gt;

Restic Backups (specify --details for more information):
  Completed:  4</pre></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Backup options</h3><p>For more advanced options, such as scheduling backups and configuring expiration times, check the <a class="link" href="https://documentation.suse.com/suse-caasp/4.2/html/caasp-admin/_backup_and_restore_with_velero.html" target="_top">Velero documentation</a>.</p></div></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id-restore"></a>Restore</h5></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>If you are restoring onto a new Kubernetes cluster, install Velero by following the <a class="link" href="https://documentation.suse.com/suse-caasp/4.2/html/caasp-admin/_backup_and_restore_with_velero.html" target="_top">CaaSP Velero documentation</a>. Make sure to use the same storage back-end used to perform the backup, so that Velero can access the existing backups. If restoring onto the same cluster, make sure to completely delete the existing SUSE Private Registry deployment, including its namespace.</p></li><li class="listitem"><p>To restore the cluster, run the command <code class="literal">velero restore create &lt;restore-name&gt; --from-backup &lt;backup-name&gt;</code>. For example:</p><pre class="screen">&gt; velero restore create registry-restore --from-backup registry-backup
Restore request "registry-restore" submitted successfully.
Run `velero restore describe registry-restore` or `velero restore logs registry-restore` for more details.</pre></li><li class="listitem"><p>To check the restore status and ensure its completion, run <code class="literal">velero restore describe &lt;restore-name&gt;</code>. For example:</p><pre class="screen">&gt; velero restore describe registry-restore
Name:         registry-restore
Namespace:    velero
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Phase:  Completed

Errors:
  Velero:     &lt;none&gt;
  Namespaces: &lt;none&gt;

Backup:  registry-backup

Namespaces:
  Included:  *
  Excluded:  &lt;none&gt;

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  &lt;none&gt;

Label selector:  &lt;none&gt;

Restore PVs:  auto

Restic Restores (specify --details for more information):
  Completed:  4</pre></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Restore options</h3><p>For more advanced options, such restoring from a scheduled backup, see the <a class="link" href="https://documentation.suse.com/suse-caasp/4.2/html/caasp-admin/_backup_and_restore_with_velero.html" target="_top">Velero documentation</a>.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-backup-and-restore-of-external-services"></a>Backup and restore of external services</h4></div></div></div><p>For backup and restore of external services, see the official documentation of the adopted solution. For example:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Database:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><a class="link" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html" target="_top">Backing up and restoring an Amazon RDS DB instance</a></p></li><li class="listitem"><p><a class="link" href="https://docs.microsoft.com/en-us/azure/postgresql/concepts-backup" target="_top">Backup and restore in Azure Database for PostgreSQL</a></p></li></ul></div></li><li class="listitem"><p>Redis:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><a class="link" href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups.html" target="_top">Backup and Restore for ElastiCache for Redis</a></p></li><li class="listitem"><p><a class="link" href="https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-how-to-premium-persistence" target="_top">How to configure data persistence for a Premium Azure Cache for Redis</a></p></li></ul></div></li><li class="listitem"><p>Object storage:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><a class="link" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_top">Amazon Simple Storage Service (S3) Versioning</a></p></li><li class="listitem"><p><a class="link" href="https://docs.microsoft.com/en-us/azure/storage/blobs/versioning-overview" target="_top">Azure Blob Storage versioning</a></p></li></ul></div></li></ul></div></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="user-guide"></a>User Guide</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-accessing-landing-page"></a>Accessing Landing Page</h3></div></div></div><p>After logging in, a harbor installation lands on the Projects page. Project pages are the individual "tenants" or "organisations" that the SUSE Private Registry can host.
For each one of those, Access Control, Security Policies, Webhooks or Accounts can be defined.
Access (push/pull) and other event logs can be accessed. A Project can be populated by replicating another registry, like for example <code class="literal">registry.suse.com</code>.
Here’s how an example harbor installation with mirroring itself from <code class="literal">registry.suse.com</code> would look like:</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-landingpage.png" width="100%" alt="registry harbor landingpage" /></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-using-docker-for-managing-images"></a>Using docker for managing images</h3></div></div></div><p>Assume there’s a "hello-world" image in your current local docker registry:</p><pre class="screen">&gt; docker images
REPOSITORY                                                                        TAG                  IMAGE ID            CREATED             SIZE
hello-world                                                                       latest               bf756fb1ae65        6 months ago        13.3kB</pre><p>Let’s assume the value of <code class="literal">&lt;core_fqdn&gt;</code> is <code class="literal">core.harbor.domain</code>.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Log in to new Harbor based Docker registry. Use the correct administrator password, refer to: <a class="xref" href="#install-passwords">[install-passwords]</a>.</p><pre class="screen">docker login core.harbor.domain -u admin</pre></li><li class="listitem"><p>Tag an existing image:</p><pre class="screen">docker tag hello-world core.harbor.domain/library/hello-world:latest</pre><p>Note the "library" part. That’s a project you will be pushing the image to.
Look how to manage projects in the <a class="link" href="https://goharbor.io/docs/2.1.0/working-with-projects/" target="_top">upstream documentation</a>.</p></li><li class="listitem"><p>Push the image to your Harbor registry:</p><pre class="screen">&gt; docker push core.harbor.domain/library/hello-world:latest
The push refers to repository [core.harbor.domain/library/hello-world]
9c27e219663c: Pushed
latest: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525</pre><p>Now go to Harbor web UI and you can see the new image is stored under library project as library/hello-world</p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-manage-helm-charts-with-suse-private-registry"></a>Manage Helm Charts with SUSE Private Registry</h3></div></div></div><p>Helm Charts in SUSE Private Registry <span class="strong"><strong>requires Helm v3</strong></span>, which is not a default option for SUSE CaaS Platform 4.5.
Helm v3 is a client only solution so you only need it on the machine that you launch the installation from.
Read about installing of Helm v3 in the <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_software_management.html#helm-tiller-install" target="_top">SUSE CaaS Platform Administration Guide</a>.
In order to use helm, you need to Log into OCI-compatible registry of Harbor:</p><pre class="screen">&gt; helm registry login core.harbor.domain
Username: admin
Password:
Login succeeded</pre><p>After logging in, run the helm chart save command to save a chart directory that prepares the artifact for pushing:</p><pre class="screen">helm chart save my-chart core.harbor.domain/library/my-chart</pre><p>Now you can push the chart to the Private Registry:</p><pre class="screen">helm chart push core.harbor.domain/library/my-chart:version</pre><p>Go to the Helm web UI and you can see the chart located under library project.</p><p>Read more in the <a class="link" href="https://goharbor.io/docs/2.1.0/working-with-projects/working-with-images/managing-helm-charts" target="_top">upstream documentation</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-using-suse-private-registry-in-the-kubernetes-cluster"></a>Using SUSE Private Registry in the Kubernetes Cluster</h3></div></div></div><p>You can set up your Kubernetes Cluster to use your newly deployed SUSE Private Registry as a source of Images.
Read more about using SUSE <a class="link" href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry" target="_top">Private Registry for 4</a> cluster.
Following setup is based on <a class="link" href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod" target="_top">Specifying ImagePullSecrets on a Pod</a> section.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Copy your generated CA certificate to all nodes of Kubernetes cluster so that container daemons trust it when pulling images from the private registry.
It’s possible to install them to the configuration directory specific to the container engine you are using, e.g. <code class="literal">/etc/docker/certs.d/</code> for docker.</p><p>Or copy them system-wide - repeat this for all your Kubernetes nodes:</p><pre class="screen">scp harbor.ca root@$kubernetes_node:
ssh root@$kubernetes_node
cp harbor.ca /usr/share/pki/trust/anchors/
update-ca-certificates
systemctl restart crio</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Notice that example above is restarting CRI-Oo service which is the container engine used by SUSE CaaS Platform.</p></div></li><li class="listitem"><p>Create a Secret of <code class="literal">docker-registry</code> type. Use the right own values for docker-server and docker-password, so that they match your environment:</p><pre class="screen">kubectl create secret docker-registry registrycred --docker-server=core.harbor.domain --docker-username=admin --docker-password=Harbor12345</pre><p>Now, you can create pods which reference newly created registrycred secret by adding an imagePullSecrets section to a Pod definition. Like this:</p><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
  - name: foo
    image: core.harbor.domain/library/hello-world:latest
  imagePullSecrets:
  - name: registrycred</pre></li><li class="listitem"><p>Save the code above into some yaml file, e.g. <code class="literal">hello-pod.yaml</code> and create the new pod that pulls the image from your private registry:</p><pre class="screen">kubectl apply -f hello-pod.yaml</pre></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It is possible to add <code class="literal">ImagePullSecrets</code> to a service account, so it is not necessary to pass it to each Pod.
Read about it in the <a class="link" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account" target="_top">upstream documentation</a>.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-vulnerability-scanning"></a>Vulnerability Scanning</h3></div></div></div><p>SUSE Private Registry provides static analysis of vulnerabilities in images through the Open Source project Trivy.
<span class="strong"><strong>No other scanners are supported.</strong></span></p><p><span class="strong"><strong>Trivy is enabled by default</strong></span> when deploying the Helm chart and can be disabled by editing the <code class="literal">harbor-values.yaml</code> before deployment:</p><pre class="screen">trivy:
  # enabled the flag to enable Trivy scanner
  enabled: true</pre><p>Log in to the Harbor interface with an account that has at least project administrator privileges.
To see the vulnerabilities detected in repository artifacts, click the <span class="strong"><strong>Repositories</strong></span> tab and then click on a repository.
For each artifact in the repository, the <span class="strong"><strong>Vulnerabilities</strong></span> column displays the vulnerability scanning status and related information.</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-vulnerability-scanning.png" width="100%" alt="registry harbor vulnerability scanning" /></td></tr></table></div></div><p>To run a vulnerability scan, select the artifacts to scan and then click the Scan button.
You can optionally select the checkbox at the top to select all artifacts in the repository.</p><p>Read more about vulnerability scanning in the <a class="link" href="https://goharbor.io/docs/2.1.0/administration/vulnerability-scanning/scan-individual-artifact" target="_top">upstream documentation</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-image-signing"></a>Image Signing</h3></div></div></div><p>By default, <code class="literal">Notary</code>, the solution for managing the content trust is installed and enabled when Harbor is installed using the Helm chart.
This allows users to store signed images in SUSE Private Registry, and in turn they have the option to use only signed images from the client applications.</p><p>To use command line tools together with the <code class="literal">Notary</code> signing support in Harbor:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enable Docker Content Trust</p><p>Set these environment variables on your client:</p><pre class="screen">export DOCKER_CONTENT_TRUST=1
export DOCKER_CONTENT_TRUST_SERVER=https://&lt;notary_fqdn&gt;</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>When <code class="literal">DOCKER_CONTENT_TRUST</code> variable is set to 1, you cannot pull unsigned images from any sources.
So for the time you want to pull some unsigned image (so you can push it signed into your local registry), it’s necessary to unset the variable, or set its value to 0.</p></div></li><li class="listitem"><p>Make sure your certificates are correctly installed on your system. Refer to: <a class="xref" href="#install-tls-security" title="Transport Layer Security (TLS) Setup">the section called “Transport Layer Security (TLS) Setup”</a>.</p></li><li class="listitem"><p>Push a signed Image to the private registry</p><pre class="screen">&gt; export DOCKER_CONTENT_TRUST_SERVER=https://notary.harbor.domain
&gt; export DOCKER_CONTENT_TRUST=1
&gt; docker push core.harbor.domain/library/alpine:latest
The push refers to repository [core.harbor.domain/library/alpine]
50644c29ef5a: Pushed
latest: digest: sha256:a15790640a6690aa1730c38cf0a440e2aa44aaca9b0e8931a9f2b0d7cc90fd65 size: 528
Signing and pushing trust metadata
You are about to create a new root signing key passphrase. This passphrase
will be used to protect the most sensitive key in your signing system. Please
choose a long, complex passphrase and be careful to keep the password and the
key file itself secure and backed up. It is highly recommended that you use a
password manager to generate the passphrase and keep it safe. There will be no
way to recover this key. You can find the key in your config directory.
Enter passphrase for new root key with ID a69b97e:
Repeat passphrase for new root key with ID a69b97e:
Enter passphrase for new repository key with ID 5419081:
Repeat passphrase for new repository key with ID 5419081:
Finished initializing "core.harbor.domain/library/alpine"
Successfully signed core.harbor.domain/library/alpine:latest</pre><p>You will be asked for the passhprases for a new root and new repository key. The root key only need to be set once.</p></li><li class="listitem"><p>Log into Harbor Portal UI, browse to the image you have just pushed and you will see an icon indicating that the image is signed</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-notary-signed-image.png" width="100%" alt="registry harbor notary signed image" /></td></tr></table></div></div></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-using-suse-private-registry-as-a-proxy-cache"></a>Using SUSE Private Registry as a Proxy Cache</h3></div></div></div><p>You can set up SUSE Private Registry to act as a caching proxy for other private or public registries. When used with Docker Hub for example, this can greatly reduce the impact that <a class="link" href="https://www.docker.com/pricing" target="_top">Docker Hub’s rate limiting policy</a> can have on your workload.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Currently, the proxy cache feature can only be used with Docker Hub and other Harbor registries.</p></div><p>Log in to the Harbor interface with an account that has administrator privileges.
First, you need to configure the remote registry as a registry endpoint under the <span class="strong"><strong>Registries</strong></span> tab.</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-add-registry-endpoint.png" width="100%" alt="registry harbor add registry endpoint" /></td></tr></table></div></div><p>The proxy cache feature can be enabled at project level, when a new project is created. Enabling the <span class="strong"><strong>Proxy Cache</strong></span> box and selecting the previously created registry endpointcauses the project to act as a caching proxy for all images pulled from the project.</p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="80%"><tr><td><img src="images/registry-harbor-add-proxy-cache-project.png" width="100%" alt="registry harbor add proxy cache project" /></td></tr></table></div></div><p>Read more about the proxy cache feature in the <a class="link" href="https://goharbor.io/docs/2.1.0/administration/configure-proxy-cache/" target="_top">upstream documentation</a>.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-appendix"></a>Appendix</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-glossary"></a>Glossary</h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">AKS</span></dt><dd><p>Azure Kubernetes Service</p></dd><dt><span class="term">ALB</span></dt><dd><p>Azure Load Balancer OR Amazon Application Load Balancer</p></dd><dt><span class="term">EKS</span></dt><dd><p>Amazon Elastic Kubernetes Service</p></dd><dt><span class="term">PVC</span></dt><dd><p>Persistent Volume Claim. Refer to: <a class="link" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim" target="_top">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim</a></p></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-limitations"></a>Limitations</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Once an authentication method is chosen and a user other than admin is added, the authentication method can not be changed without deploying a completely fresh registry.</p></li><li class="listitem"><p>High availability and scalability are not available for the internal redis and database components. Use external database and redis services to compensate this limitation.</p></li><li class="listitem"><p>Internal TLS can be enabled to secure internal connections between the SUSE Private Registry components.
This doesn’t currently cover the internal database and internal redis components.</p></li><li class="listitem"><p>Securing connections to the external Redis with TLS/SSL is currently not supported.</p></li><li class="listitem"><p>Accuracy of the Image scanning using <code class="literal">Trivy</code> might be limited.</p></li><li class="listitem"><p>GPG signing for OCI artifacts is currently not supported. The only method for signing OCI artifacts is TUF (The Update Framework), provided by the <code class="literal">Notary</code> component.</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-supported-deployment-scenarios"></a>Supported Deployment Scenarios</h3></div></div></div><p>For an exhaustive list of Helm chart deployment options, please consult the documentation included in the helm chart, by running e.g.:</p><pre class="screen">export HELM_EXPERIMENTAL_OCI=1
# download the chart from public registry
helm chart pull registry.suse.com/harbor/harbor:1.5
# export the chart to local directory
helm chart export registry.suse.com/harbor/harbor:1.5
# show the chart information
helm show readme harbor</pre><p>The following table lists the Helm chart deployment options officially supported by the SUSE Private Registry 2.1 release.</p><div class="table"><a id="supported-deployment"></a><p class="title"><strong>Table 1. Supported deployment options</strong></p><div class="table-contents"><table class="table" summary="Supported deployment options" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="center" valign="top">Deployment Option</th><th align="center" valign="top">Values Supported by Harbor</th><th align="center" valign="top">Supported by SUSE Private Registry?</th><th align="center" valign="top">Description</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Deployment tool</p></td><td align="left" valign="top"><p>Helm chart (helm v3)</p></td><td align="center" valign="top"><p>✅</p></td><td align="left" valign="top"> </td></tr><tr><td rowspan="6" align="left" valign="top"><p>K8s distribution</p></td><td align="left" valign="top"><p>SUSE CaaS Platform v4.2</p></td><td align="center" valign="top"><p>❌</p></td><td rowspan="6" align="left" valign="top"><p>Supported Kubernetes distributions on top of which the registry can be deployed as a helm chart.</p></td></tr><tr><td align="left" valign="top"><p>SUSE CaaS Platform v4.5</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>AKS (Azure K8s Services)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>EKS (Amazon)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>K3s (Rancher)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>RKE2 (Rancher)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="3" align="left" valign="top"><p>Registry Image / Chart Storage Backend</p></td><td align="left" valign="top"><p>Filesystem</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="3" align="left" valign="top"><p>Back-ends supported for storing registry objects such as container images and helm charts.</p></td></tr><tr><td align="left" valign="top"><p>Azure Blob Storage</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>AWS/S3</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="4" align="left" valign="top"><p>Frontend</p></td><td align="left" valign="top"><p>Ingress</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="4" align="left" valign="top"><p>Options available for exposing the Harbor public APIs (UI, harbor core API and notary API).</p></td></tr><tr><td align="left" valign="top"><p>LoadBalancer</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>NodePort</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>ClusterIP</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td rowspan="4" align="left" valign="top"><p>Ingress controllers</p></td><td align="left" valign="top"><p>SUSE NGINX Ingress Controller (SUSE CaaS Platform v4.5)</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="4" align="left" valign="top"><p>Ingress controllers for providing ingresses to expose services.</p></td></tr><tr><td align="left" valign="top"><p><a class="link" href="https://github.com/kubernetes/ingress-nginx" target="_top">Kubernetes ingress nginx</a> (AKS, EKS)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p><a class="link" href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html" target="_top">EKS ALB</a></p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p><a class="link" href="https://rancher.com/docs/k3s/latest/en/networking/#traefik-ingress-controller" target="_top">Traefik Ingress Controller</a> (K3s)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>TLS</p></td><td align="left" valign="top"><p>External TLS</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>TLS can be enabled for securing the connection between the harbor services (internal) as well as between its consumers (external). The certificates can be generated by the helm chart or provided in advance.</p></td></tr><tr><td align="left" valign="top"><p>Internal TLS</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Database service</p></td><td align="left" valign="top"><p>Builtin</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>Database used by Harbor. Can be deployed internally by the Harbor helm chart or can be configured to consume an external database.</p></td></tr><tr><td align="left" valign="top"><p>External</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>External database service</p></td><td align="left" valign="top"><p>Amazon RDS/PostgreSQL</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>Managed postgresql database services that can be configured as an external database service for Harbor.</p></td></tr><tr><td align="left" valign="top"><p>Azure Database for PostgreSQL</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Redis service</p></td><td align="left" valign="top"><p>Builtin</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>In-memory cache used by Harbor. Can be deployed internally by the Harbor helm chart or can be configured to consume an external redis service.</p></td></tr><tr><td align="left" valign="top"><p>External</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="3" align="left" valign="top"><p>External redis service</p></td><td align="left" valign="top"><p>Amazon ElastiCache for Redis</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="3" align="left" valign="top"><p>Managed redis cache services that can be configured as an external redis service for Harbor.</p></td></tr><tr><td align="left" valign="top"><p>Azure Cache for Redis</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>SUSE redis-ha operator</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Persistent Block Storage</p></td><td align="left" valign="top"><p>Enabled</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>Persistent storage can be enabled/disabled for the harbor stateful services:</p>
<div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Database (internal)</p></li><li class="listitem"><p>Redis (internal)</p></li><li class="listitem"><p>Registry (when using the filesystem backend)</p></li><li class="listitem"><p>Jobservice</p></li><li class="listitem"><p>Trivy</p></li></ul></div>
<p>The registry can be deployed without persistent storage, e.g. for evaluation purposes, but this deployment option is not supported.</p></td></tr><tr><td align="left" valign="top"><p>Disabled</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td rowspan="5" align="left" valign="top"><p>Persistent Block Storage Backends</p></td><td align="left" valign="top"><p>Azure Managed Disks (RWO)</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="5" align="left" valign="top"><p>Kubernetes storage backend used to provide persistent storage support.</p>
<div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>A Kubernetes storage class with RWM (<code class="literal">ReadWriteMany</code>) capabilities is required to deploy SUSE Private Registry in full HA mode.
Alternatively, a storage class with RWO (<code class="literal">ReadWriteOnce</code>) capabilities can be used to deploy the SUSE Private Registry with partial HA.
Refer to <a class="xref" href="#high-availability" title="High Availability">the section called “High Availability”</a> for more details.</p></div>
<p>† for Kubernetes storage classes supported for SUSE CaaS Platform v4.5, consult the relevant documentation at: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_storage.html" target="_top">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_storage.html</a></p></td></tr><tr><td align="left" valign="top"><p>Azure Files (RWM)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Amazon EBS (RWO)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Amazon EFS (RWM)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>SUSE CaaS Platform v4.5 supported storage backends †</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Update Strategy</p></td><td align="left" valign="top"><p>RollingUpdate</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>Registry configuration update and upgrade strategy, delivered via Helm:</p>
<div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><code class="literal">RollingUpdate</code>: Ensures overall minimal service downtime during a <code class="literal">helm upgrade</code> operation by activating the rolling update feature for all registry components that support it.
This requires a Kubernetes storage class with RWM (<code class="literal">ReadWriteMany</code>) support to be used for registry services that need shared persistent storage (jobservice and docker-registry).</p></li><li class="listitem"><p><code class="literal">Recreate</code>: Use this strategy when a Kubernetes storage class with RWM (<code class="literal">ReadWriteMany</code>) support isn’t available.
It selectively disables the rolling update feature for registry components that need shared persistent storage (jobservice and docker-registry), resulting in increased service downtime during configuration update and upgrade operations.</p></li></ul></div>
<div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>The possibility to use <code class="literal">RollingUpdate</code> strategy depends on the Persistent Volume configuration.
If the persistent volumes do not support ReadWriteMany access, using the <code class="literal">RollingUpdate</code> strategy will result in failure.</p></div></td></tr><tr><td align="left" valign="top"><p>Recreate</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Proxy</p></td><td align="left" valign="top"><p>Disabled</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="2" align="left" valign="top"><p>A proxy can be configured for replicating artifacts from/to the registries that cannot be reached directly</p></td></tr><tr><td align="left" valign="top"><p>Enabled</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td rowspan="4" align="left" valign="top"><p>High Availability for Stateless components</p></td><td align="left" valign="top"><p>portal</p></td><td align="center" valign="top"><p>✅</p></td><td rowspan="9" align="left" valign="top"><p>To achieve true HA, the number of replicas for each component needs to be set to 2 or more.
This can easily be done for stateless components, however for some stateful components (jobservice and docker-registry), a persistent block storage backend that supports the <code class="literal">ReadWriteMany</code> access mode is needed to provide shared persistent storage.</p>
<div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>HA for the internal database and internal redis is not supported by Harbor.
External database and redis services with HA support should be use to complement the HA features supported for the other components.</p></div></td></tr><tr><td align="left" valign="top"><p>core</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>nginx</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>notary</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="5" align="left" valign="top"><p>High Availability for Stateful components</p></td><td align="left" valign="top"><p>docker-registry</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>jobservice</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>trivy</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>internal database</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>internal redis</p></td><td align="center" valign="top"><p>❌</p></td></tr></tbody></table></div></div><br class="table-break" /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-supported-features"></a>Supported Features</h3></div></div></div><div class="table"><a id="supported-features"></a><p class="title"><strong>Table 2. Supported Features</strong></p><div class="table-contents"><table class="table" summary="Supported Features" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Harbor Feature</th><th align="left" valign="top">Description</th><th align="left" valign="top">Feature Specifics</th><th align="left" valign="top">Notes</th><th align="left" valign="top">Supported by SUSE Private Registry</th></tr></thead><tbody><tr><td rowspan="14" align="left" valign="middle"><p>User Authentication &amp; Authorization</p></td><td rowspan="4" align="left" valign="middle"><p>Supported backends for user authentication and authorization.</p></td><td align="left" valign="top"><p>Database</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>LDAP</p></td><td rowspan="3" align="left" valign="top"><p>To change the authentication mode from database to LDAP, OIDC or UAA no local users should be registered to the database.
If there is at least one user other than <code class="literal">admin</code> in the Harbor database, it is not possible to change the authentication mode.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>OIDC</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>UAA</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="7" align="left" valign="middle"><p>Access to resources are controlled using RBAC where the following roles can applied to users</p></td><td align="left" valign="top"><p>Limited Guest</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Can pull images but cannot push</p></li><li class="listitem"><p>Cannot see logs or the other members of a project</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Guest</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Can pull and retag images, but cannot push</p></li><li class="listitem"><p>Read-only privilege for a specified project</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Developer</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Read and write privileges for a project</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Master</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Same as Developer plus can scan images, view replications jobs, and delete images and helm charts</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>ProjectAdmin</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Same as Master plus some management privileges, such as adding and removing members</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>System Admin</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Same as ProjectAdmin plus can also list all projects, set an ordinary user as administrator, delete users and set vulnerability scan policy for all images</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Anonymous</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Has no access to private projects</p></li><li class="listitem"><p>Has read-only access to public projects</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="3" align="left" valign="top"><p>A project contains all repositories of an application. Images cannot be pushed to Harbor before a project is created. RBAC is applied to projects, so that only users with the appropriate roles can perform certain operations.</p></td><td align="left" valign="top"><p>Public</p></td><td align="left" valign="top"><p>Any user can pull images from this project. This is a convenient way for a user to share repositories with others.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Private</p></td><td align="left" valign="top"><p>Only users who are members of the project can pull images</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Quota</p></td><td align="left" valign="top"><p>Quotas are applied on projects to limit the amount of storage capacity that a project can consume.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Logging</p></td><td align="left" valign="top"><p>Registry events such as pushing and deleting images are logged and available for querying by an ordinary user (project level) and an admin user (system level)</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Logs can be filtered by:</p>
<div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Push only</p></li><li class="listitem"><p>Pull only</p></li><li class="listitem"><p>Pull and push</p></li><li class="listitem"><p>Delete only</p></li><li class="listitem"><p>All</p></li><li class="listitem"><p>Push and delete</p></li><li class="listitem"><p>Different date ranges</p></li><li class="listitem"><p>Date range and push</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Labels</p></td><td rowspan="2" align="left" valign="top"><p>Labels can be created on different levels and assigned to images. Can be used for filtering</p></td><td align="left" valign="top"><p>global</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>System admin can create/edit/delete</p></li><li class="listitem"><p>Available for all projects</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>project</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Project admin can create/edit/delete</p></li><li class="listitem"><p>Available for a specific project</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="12" align="left" valign="middle"><p>OCI Compliant Registry</p></td><td rowspan="8" align="left" valign="middle"><p>The registry must support versioning, storing and distributing the following contents</p></td><td align="left" valign="top"><p>Multi-architecture images</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Requires experimental features enabled on the Docker client</p></li><li class="listitem"><p>Multi-arch images are created using the manifests feature</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Container images</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Helm charts</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>helm 2</p></li></ul></div></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>helm 3</p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Cloud Native Application Bundles</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>CNAB is composed by a set of images and a manifest.</p></li><li class="listitem"><p>The registry must be able to understand how to present a CNAB</p></li><li class="listitem"><p><a class="link" href="https://github.com/cnabio/cnab-to-oci" target="_top">cnab-to-oci</a></p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Artifacts</p></td><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>A single or multiple files can be pushed/pulled to/from the registry</p></li><li class="listitem"><p><a class="link" href="https://github.com/deislabs/oras" target="_top">oras</a></p></li></ul></div></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Tags</p></td><td align="left" valign="top"><p>Convey useful information about a specific image or chart version/variant.
They are aliases to the ID of an image.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Repository</p></td><td align="left" valign="top"><p>A repository can hold many objects (stored as tags)</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="4" align="left" valign="top"><p>Any client that is able to consume the OCI API can interact with the registry</p></td><td align="left" valign="top"><p>Docker</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Podman/CRIO</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>Helm 2</p></td><td align="left" valign="top"><p>Helm 2 is not OCI compatible.</p></td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>Helm 3</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="3" align="left" valign="top"><p>Replication</p></td><td rowspan="3" align="left" valign="top"><p>Replication allows users to replicate resources (e.g. images and charts) between Harbor and non-Harbor registries, in both pull or push mode.</p></td><td align="left" valign="top"><p>SUSE Registry</p></td><td align="left" valign="top"><p><a class="link" href="https:registry.suse.com" target="_top">https:registry.suse.com</a></p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Docker Hub</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Docker Registry</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Content Trust</p></td><td rowspan="2" align="left" valign="top"><p>The registry must provide the ability to use digital signatures to allow client-side or runtime verification of the integrity and publisher of specific images.</p></td><td align="left" valign="top"><p>Signed Images</p></td><td align="left" valign="top"><p>Publishers can sign their images and image consumers can ensure that the images they pull are signed</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Project Level Trust</p></td><td align="left" valign="top"><p>Projects with content trust enable does not allow pulling unsigned images</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="3" align="left" valign="top"><p>Image Vulnerability Scanning</p></td><td rowspan="2" align="left" valign="top"><p>Provides static analysis of vulnerabilities in images through the following open source projects or other compatible scanners (<a class="link" href="https://github.com/anchore/harbor-scanner-adapter" target="_top">Anchore</a>, <a class="link" href="https://github.com/aquasecurity/harbor-scanner-aqua" target="_top">CSP</a>, <a class="link" href="https://github.com/dosec-cn/harbor-scanner/blob/master/README_en.md" target="_top">DoSec</a>)</p></td><td align="left" valign="top"><p>Trivy</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Clair</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>❌</p></td></tr><tr><td align="left" valign="top"><p>Artifacts subject to a CVE might not be permitted to run. CVEs whitelists enable the scanner to ignore those CVEs</p></td><td align="left" valign="top"><p>CVE Whitelists</p></td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Garbage Collection</p></td><td align="left" valign="top"><p>Deleted images does not automatically free up space. Garbage collection must be executed to free up space by removing blobs that are no longer referenced by a manifest from the file system</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>While it is running Harbor goes into read-only mode. All modifications to the registry are prohibited.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Tag Policies</p></td><td rowspan="2" align="left" valign="top"><p>As a Harbor system administrator, it is possible to define rules that govern how many artifacts of a given repository to retain, or for how long to retain certain artifacts.</p>
<p>Harbor also allows the creation of tag immutability rules at the project level, so that artifacts with certain tags cannot be pushed into Harbor if their tags match existing tags</p></td><td align="left" valign="top"><p>Retention</p></td><td align="left" valign="top"><p>Rules that govern how many artifacts of a given repository to retain, or for how long to retain certain artifacts.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Immutability</p></td><td align="left" valign="top"><p>Artifacts with certain tags cannot be pushed into Harbor if their tags match existing tags. This prevents existing artifacts from being overwritten.
Tag immutability guarantees that an immutable tagged artifact cannot be deleted, and also cannot be altered in any way such as through re-pushing, re-tagging, or replication from another target registry.</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>Robot Accounts</p></td><td align="left" valign="top"><p>Robot accounts can be used to run automated operations.
Robot accounts have the following limitations:</p>
<div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Robot accounts cannot log in to the Harbor interface.</p></li><li class="listitem"><p>Robot accounts can only perform operations by using the Docker and Helm CLIs.</p></li></ul></div></td><td align="left" valign="top"> </td><td align="left" valign="top"> </td><td align="center" valign="top"><p>✅</p></td></tr><tr><td rowspan="2" align="left" valign="top"><p>Webhooks</p></td><td rowspan="2" align="left" valign="top"><p>Webhooks allows the integration of Harbor with other tools to streamline continuous integration and development processes by sending notifications based on certain events that occur in the project</p></td><td align="left" valign="top"><p>HTTP</p></td><td rowspan="2" align="left" valign="top"><p>Webhook notifications provide information about events in JSON format and are delivered by HTTP or HTTPS POST</p></td><td align="center" valign="top"><p>✅</p></td></tr><tr><td align="left" valign="top"><p>SLACK</p></td><td align="center" valign="top"><p>✅</p></td></tr></tbody></table></div></div><br class="table-break" /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-authors"></a>Authors</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Boris Bobrov, System Engineer, SUSE</p></li><li class="listitem"><p>Christian Almeida de Oliveira, System Engineer, SUSE</p></li><li class="listitem"><p>Dirk Müller, Distinguished Engineer, SUSE</p></li><li class="listitem"><p>Markus Napp, Senior Technical Writer SUSE CaaS Platform, SUSE</p></li><li class="listitem"><p>Stefan Nica, System Engineer, SUSE</p></li><li class="listitem"><p>Flávio Ramalho, System Engineer, SUSE</p></li><li class="listitem"><p>Robert Simai, Engineering Manager, SUSE</p></li><li class="listitem"><p>Jiří Suchomel, SUSE OpenStack Cloud Engineer, SUSE</p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-legal-notice"></a>Legal Notice</h2></div></div></div><p>Copyright © 2006–2022 SUSE LLC and contributors. All rights reserved.</p><p>Permission is granted to copy, distribute and/or modify this document under the terms of
the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the
Invariant Section being this copyright notice and license. A copy of the license version 1.2
is included in the section entitled "GNU Free Documentation License".</p><p>SUSE, the SUSE logo and YaST are registered trademarks of SUSE LLC in the United States
and other countries. For SUSE trademarks, see <a class="link" href="https://www.suse.com/company/legal/" target="_top">https://www.suse.com/company/legal/</a>.</p><p>Linux is a registered trademark of Linus Torvalds. All other names or trademarks mentioned in
this document may be trademarks or registered trademarks of their respective owners.</p><p>Documents published as part of the SUSE Best Practices series have been contributed voluntarily
by SUSE employees and third parties. They are meant to serve as examples of how particular
actions can be performed. They have been compiled with utmost attention to detail.
However, this does not guarantee complete accuracy. SUSE cannot verify that actions described
in these documents do what is claimed or whether actions described have unintended consequences.
SUSE LLC, its affiliates, the authors, and the translators may not be held liable for possible errors
or the consequences thereof.</p><p>Below we draw your attention to the license under which the articles are published.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-gnu-free-documentation-license"></a>GNU Free Documentation License</h2></div></div></div><p>Copyright © 2000, 2001, 2002 Free Software Foundation, Inc.
51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p><h3><a id="id-0-preamble"></a>0. PREAMBLE</h3><p>The purpose of this License is to make a manual, textbook, or other functional and useful document "free" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially.
Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.</p><p>This License is a kind of "copyleft", which means that derivative works of the document must themselves be free in the same sense.
It complements the GNU General Public License, which is a copyleft license designed for free software.</p><p>We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does.
But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book.
We recommend this License principally for works whose purpose is instruction or reference.</p><h3><a id="id-1-applicability-and-definitions"></a>1. APPLICABILITY AND DEFINITIONS</h3><p>This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License.
Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein.
The "Document", below, refers to any such manual or work.
Any member of the public is a licensee, and is addressed as "you". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.</p><p>A "Modified Version" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.</p><p>A "Secondary Section" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document’s overall subject (or to related matters) and contains nothing that could fall directly within that overall subject.
(Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.</p><p>The "Invariant Sections" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License.
If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant.
The Document may contain zero Invariant Sections.
If the Document does not identify any Invariant Sections then there are none.</p><p>The "Cover Texts" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.</p><p>A "Transparent" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters.
A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent.
An image format is not Transparent if used for any substantial amount of text.
A copy that is not "Transparent" is called "Opaque".</p><p>Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification.
Examples of transparent image formats include PNG, XCF and JPG.
Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.</p><p>The "Title Page" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page.
For works in formats which do not have any title page as such, "Title Page" means the text near the most prominent appearance of the work’s title, preceding the beginning of the body of the text.</p><p>A section "Entitled XYZ" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.
(Here XYZ stands for a specific section name mentioned below, such as "Acknowledgements", "Dedications", "Endorsements", or "History".) To "Preserve the Title" of such a section when you modify the Document means that it remains a section "Entitled XYZ" according to this definition.</p><p>The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document.
These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.</p><h3><a id="id-2-verbatim-copying"></a>2. VERBATIM COPYING</h3><p>You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License.
You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute.
However, you may accept compensation in exchange for copies.
If you distribute a large enough number of copies you must also follow the conditions in section 3.</p><p>You may also lend copies, under the same conditions stated above, and you may publicly display copies.</p><h3><a id="id-3-copying-in-quantity"></a>3. COPYING IN QUANTITY</h3><p>If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document’s license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover.
Both covers must also clearly and legibly identify you as the publisher of these copies.
The front cover must present the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition.
Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.</p><p>If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.</p><p>If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.</p><p>It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.</p><h3><a id="id-4-modifications"></a>4. MODIFICATIONS</h3><p>You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it.
In addition, you must do these things in the Modified Version:</p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.</p></li><li class="listitem"><p>List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.</p></li><li class="listitem"><p>State on the Title page the name of the publisher of the Modified Version, as the publisher.</p></li><li class="listitem"><p>Preserve all the copyright notices of the Document.</p></li><li class="listitem"><p>Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.</p></li><li class="listitem"><p>Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.</p></li><li class="listitem"><p>Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document’s license notice.</p></li><li class="listitem"><p>Include an unaltered copy of this License.</p></li><li class="listitem"><p>Preserve the section Entitled "History", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled "History" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.</p></li><li class="listitem"><p>Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the "History" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.</p></li><li class="listitem"><p>For any section Entitled "Acknowledgements" or "Dedications", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.</p></li><li class="listitem"><p>Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.</p></li><li class="listitem"><p>Delete any section Entitled "Endorsements". Such a section may not be included in the Modified Version.</p></li><li class="listitem"><p>Do not retitle any existing section to be Entitled "Endorsements" or to conflict in title with any Invariant Section.</p></li><li class="listitem"><p>Preserve any Warranty Disclaimers.</p></li></ol></div><p>If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant.
To do this, add their titles to the list of Invariant Sections in the Modified Version’s license notice.
These titles must be distinct from any other section titles.</p><p>You may add a section Entitled "Endorsements", provided it contains nothing but endorsements of your Modified Version by various parties—​for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.</p><p>You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.
Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity.
If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.</p><p>The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.</p><h3><a id="id-5-combining-documents"></a>5. COMBINING DOCUMENTS</h3><p>You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.</p><p>The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy.
If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.</p><p>In the combination, you must combine any sections Entitled "History" in the various original documents, forming one section Entitled "History"; likewise combine any sections Entitled "Acknowledgements", and any sections Entitled "Dedications". You must delete all sections Entitled "Endorsements".</p><h3><a id="id-6-collections-of-documents"></a>6. COLLECTIONS OF DOCUMENTS</h3><p>You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.</p><p>You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.</p><h3><a id="id-7-aggregation-with-independent-works"></a>7. AGGREGATION WITH INDEPENDENT WORKS</h3><p>A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an "aggregate" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation’s users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.</p><p>If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form.
Otherwise they must appear on printed covers that bracket the whole aggregate.</p><h2><a id="id-8-translation"></a>8. TRANSLATION</h2><p>Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4.
Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections.
You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.</p><p>If a section in the Document is Entitled "Acknowledgements", "Dedications", or "History", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.</p><h3><a id="id-9-termination"></a>9. TERMINATION</h3><p>You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License.
Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p><h3><a id="id-10-future-revisions-of-this-license"></a>10. FUTURE REVISIONS OF THIS LICENSE</h3><p>The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time.
Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
See <a class="link" href="http://www.gnu.org/copyleft/" target="_top">http://www.gnu.org/copyleft/</a>.</p><p>Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this License "or any later version" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation.
If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation.</p><h3><a id="id-addendum-how-to-use-this-license-for-your-documents"></a>ADDENDUM: How to use this License for your documents</h3><pre class="screen">Copyright (c) YEAR YOUR NAME.
   Permission is granted to copy, distribute and/or modify this document
   under the terms of the GNU Free Documentation License, Version 1.2
   or any later version published by the Free Software Foundation;
   with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
   A copy of the license is included in the section entitled “GNU
   Free Documentation License”.</pre><p>If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the “
with…​Texts.”
line with this:</p><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
   Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre><p>If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation.</p><p>If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software.</p></div></div></body></html>
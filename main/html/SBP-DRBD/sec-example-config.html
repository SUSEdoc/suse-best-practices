<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Example Configurations for a Single Service</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.2" /><style type="text/css">
body { background-image: url('static/images/draft.png');
       background-repeat: no-repeat;
       background-position: top left;
       /* The following properties make the watermark "fixed" on the page. */
       /* I think that's just a bit too distracting for the reader... */
       /* background-attachment: fixed; */
       /* background-position: center center; */
     }</style><link rel="home" href="index.html" title="Data Replication across Geo Clusters via DRBD" /><link rel="up" href="index.html" title="Data Replication across Geo Clusters via DRBD" /><link rel="prev" href="sec-architecture.html" title="Cluster Architecture Overview" /><link rel="next" href="sec-interop-booth.html" title="Interoperability with Booth" /></head><body onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Example Configurations for a Single Service</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="sec-architecture.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="sec-interop-booth.html">Next</a></td></tr></table><hr /></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-example-config"></a>Example Configurations for a Single Service</h2></div></div></div><p>The example configurations below are using the following premises:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Two sites, called <span class="strong"><strong>RZ1</strong></span> and <span class="strong"><strong>RZ2</strong></span>, with two private networks <span class="strong"><strong>192.168.201.x</strong></span> and <span class="strong"><strong>192.168.202.x</strong></span>, routed to the other site</p></li><li class="listitem"><p>Four nodes, called <span class="strong"><strong>geo_rzN-M</strong></span> in four
                    combinations: <span class="strong"><strong>geo-rz1-a</strong></span> to <span class="strong"><strong>geo-rz2-b</strong></span></p></li><li class="listitem"><p>NFS is to be served; but there’s not much difference for other services</p></li><li class="listitem"><p>Nodes are using LVM, VG name is <span class="strong"><strong>volgroup</strong></span></p></li><li class="listitem"><p>The lower DRBD layer (for the HA-clusters) uses minor 0; minor 10 is used for
                    DR replication</p></li></ul></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="sec-drbd-config"></a>DRBD Configuration</h3></div></div></div><p>The following snippets show a basic DRBD configuration. These are bare-bones;
                performance-tuning options are not included here.</p><p>All three snippets can be contained in a single resource file, for example in
                    <code class="filename">/etc/drbd.d/nfs.res</code>. This is the recommended configuration,
                because keeping a single file synchronized across the four cluster nodes is easier.
                You can also consult <span class="strong"><strong>csync2</strong></span> at <a class="link" href="http://oss.linbit.com/csync2/" target="_top">http://oss.linbit.com/csync2/</a>.</p><p>If you have used the Geo Clustering Quick Start Guide at <a class="link" href="https://documentation.suse.com/sle-ha-geo/12-SP4/single-html/SLE-HA-geo-quick/#art-ha-geo-quick" target="_top">https://documentation.suse.com/sle-ha-geo/12-SP4/single-html/SLE-HA-geo-quick/#art-ha-geo-quick</a>
                to perform the basic configuration of the cluster nodes, the DRBD configuration
                files are already included in the list of files to be synchronized.</p><p>To synchronize any changes to the configuration files across both cluster nodes,
                use the following command:</p><p>
                <span class="command"><strong>root # csync2 -xv /etc/drbd.d/</strong></span>
            </p><p>If you do not have csync2, or if you do not want to use it, you will need to copy
                the DRBD configuration files manually to all the other nodes.</p><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-drbd-site-1"></a>DRBD on Site 1</h4></div></div></div><p>To configure DRBD on Site 1, you should be aware of the following
                    details:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The resource-name has the site in it, so that the complete
                            configuration can be kept in synchronization across both clusters
                            without naming conflicts</p></li><li class="listitem"><p>The nodes' <span class="strong"><strong>local</strong></span> per-node IP
                            addresses are used</p></li><li class="listitem"><p>A <span class="italic">shared-secret</span> is used to avoid
                            inadvertent wrong connections</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">uuid Program</h3><p>The <span class="package">uuid</span> program is an easy way to get unique
                                values.</p></div></li></ul></div><pre class="screen">resource nfs-lower-rz1 {
        disk            /dev/volgroup/lv-nfs;
        meta-disk       internal;
        device          /dev/drbd0;
        protocol        C;

        net {
                shared-secret   "2a9702a6-8747-11e3-9ebb-782bcbd0c11c";
        }

        on geo-rz1-a {
                address          192.168.201.111:7900;
        }
        on geo-rz1-b {
                address          192.168.201.112:7900;
        }
}</pre></div><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-drbd-site-2"></a>DRBD on Site 2</h4></div></div></div><p>Even if Site 2 is nearly identical to Site 1, you should notice the following
                    differences:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The resource name has changed</p></li><li class="listitem"><p>The node names and IP addresses are different</p></li><li class="listitem"><p>Another <span class="italic">shared-secret</span> has been
                            generated</p></li><li class="listitem"><p>The volume group and LV name can be kept in the <span class="italic">resource</span> section if identical on both
                            nodes</p></li></ul></div><pre class="screen">resource nfs-lower-rz2 {
        disk            /dev/volgroup/lv-nfs;
        meta-disk       internal;
        device          /dev/drbd0;
        protocol        C;

        net {
                shared-secret   "cd9d857d-72ef-4d10-a1de-6450d1797a2c";
        }

        on geo-rz2-a {
                address          192.168.202.111:7900;
        }
        on geo-rz2-b {
                address          192.168.202.112:7900;
        }
}</pre></div><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-drbd-across-sites"></a>DRBD Connection across Sites</h4></div></div></div><p>To configure a DRBD connection across sites, you should be aware of the
                    following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The storage disk is the HA-cluster DRBD device <span class="italic">/dev/drbd0</span></p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>You could also use <span class="italic">/dev/drbd/by-
                                        res/nfs-lower-rzN/0</span>, but that would be
                                    site-specific, and so would need to be moved into the per-site
                                    configuration (stacked on top of <span class="italic">nfs-lower-rzN</span>)</p></li></ul></div></li><li class="listitem"><p>The DRBD device drbd10 says to use minor number 10</p></li><li class="listitem"><p>Protocol A, and a higher ping-timeout are needed because of the higher
                            latency</p></li><li class="listitem"><p>A different shared-secret is used</p></li><li class="listitem"><p>Do not pass any host names, but tell DRBD to stack upon its lower
                            device; that implies that this must be Primary</p></li><li class="listitem"><p>To allow TCP/IP connections to the other site without knowing which
                            cluster node has the lower DRBD device Primary, we are using a (the)
                            service IP address</p></li></ul></div><pre class="screen">resource nfs-upper {
        disk             /dev/drbd0;
        meta-disk        internal;
        device           /dev/drbd10;
        protocol         A;

        net {
                shared-secret    "e0fbd1fe-6b0b-47db-829a-2c4ba638bf1e";
                ping-timeout     20;
        }

        stacked-on-top-of nfs-lower-rz1 {
                address          192.168.201.151:7910;
        }
        stacked-on-top-of nfs-lower-rz2 {
                address          192.168.202.151:7910;
        }
}</pre><p>Using a DRBD Proxy would involve inserting <span class="italic">proxy on
                        ...</span> sections into <span class="italic">stacked-on-top-of</span> above, and a <span class="italic">proxy {
                        ... }</span> section inside of <span class="italic">resource</span>. See LINBIT's DRBD Proxy guide at <a class="link" href="https://downloads.linbit.com/" target="_top">https://downloads.linbit.com/</a> for more details
                    regarding configuring DRBD Proxy.</p></div></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="sec-pacemaker-resources"></a>Pacemaker Resources (in crm-shell syntax)</h3></div></div></div><p>For a more in-depth look at how to configure the NFS server, see the Highly
                Available NFS Storage with DRBD and Pacemaker document included in the SUSE Linux
                Enterprise High Availability Extension documentation. To configure the necessary
                resources, use the <span class="strong"><strong>crm</strong></span> shell commands as outlined
                in the following chapters. </p><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-basic-primitives"></a>Basic Primitives</h4></div></div></div><p>Setting up the basic primitives is fairly straightforward. You need a service
                    IP, the file system, and the NFS server.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">exportfs</h3><p>It is also possible to use the <span class="italic">exportfs</span>
                        resource agents instead, and keep the NFS server running all the time. This
                        is necessary if there are multiple NFS exports that must migrate
                        independently.</p></div><pre class="screen">crm configure
primitive p-ip-nfs IPaddr2 ip=192.168.202.151 iflabel=nfs nic=eth1 cidr_netmask=24
primitive p-nfs-fs Filesystem device=/dev/drbd/by-res/nfs/0 directory=/mnt/nfs fstype=ext4
primitive p-nfs-service systemd:nfs-server</pre></div><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-multi-state-resources"></a>DRBD Pacemaker Resources, Multi-State Resources</h4></div></div></div><p>To configure the cluster resources for DRBD, it is possible to use the drbd
                    cluster script. This script will create a base resource for DRBD, as well as a
                    multi-state resource that ensures that DRBD only runs in Primary mode on a
                    single node. Multi-state resources, previously called <span class="italic">Master-Slave</span> resources, allow the instances to be in one of two
                    operating modes (called roles). The roles are called <span class="italic">master</span> and <span class="italic">slave</span>.</p><pre class="screen">crm script run drbd id=drbd-nfs drbd_resource=nfs-upper
crm script run drbd id=drbd-nfs-lower drbd_resource=nfs-lower-rz2</pre></div><div class="sect3"><div class="titlepage"><div><div><h4 class="title"><a id="sec-group-primitives"></a>Group and Basic Primitives</h4></div></div></div><p>This is mostly what you would expect from the earlier picture: the multi-state
                    equivalent of having a group consisting of <span class="italic">ms-drbd-nfs-lower:Master</span>, <span class="italic">p-ip-nfs</span>, and <span class="italic">ms-drbd-nfs:Master</span>.</p><pre class="screen">crm configure

group g-nfs p-nfs-fs p-nfs-service

colocation co-nfs-ip-with-lower inf: p-ip-nfs:Started ms-drbd-nfs-lower:Master
colocation co-nfs-g-with-upper inf: g-nfs:Started ms-drbd-nfs:Master
colocation co-nfs-upper-with-ip inf: ms-drbd-nfs:Master p-ip-nfs:Started

order o-lower-drbd-before-ip-nfs inf: ms-drbd-nfs-lower:promote p-ip-nfs:start
order o-ip-nfs-before-drbd inf: p-ip-nfs:start ms-drbd-nfs:promote
order o-drbd-nfs-before-svc inf: ms-drbd-nfs:promote g-nfs:start</pre></div></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="sec-architecture.html">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="sec-interop-booth.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Cluster Architecture Overview </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Interoperability with Booth</td></tr></table></div></body></html>
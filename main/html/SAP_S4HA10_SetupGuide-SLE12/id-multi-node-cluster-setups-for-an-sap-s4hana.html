<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Multi-Node Cluster Setups for an SAP S/4HANA</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.2" /><style type="text/css">
body { background-image: url('static/images/draft.png');
       background-repeat: no-repeat;
       background-position: top left;
       /* The following properties make the watermark "fixed" on the page. */
       /* I think that's just a bit too distracting for the reader... */
       /* background-attachment: fixed; */
       /* background-position: center center; */
     }</style><link rel="home" href="index.html" title="SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide" /><link rel="up" href="index.html" title="SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide" /><link rel="prev" href="id-administration.html" title="Administration" /><link rel="next" href="id-references.html" title="References" /></head><body onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Multi-Node Cluster Setups for an SAP S/4HANA</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="id-administration.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="id-references.html">Next</a></td></tr></table><hr /></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-multi-node-cluster-setups-for-an-sap-s4hana"></a>Multi-Node Cluster Setups for an SAP S/4HANA</h2></div></div></div><p><a id="multicluster"></a><span class="strong"><strong>Multinode cluster</strong></span> setups mean cluster configurations with more than two nodes.
Depending from the starting point it is possible to extend a two node cluster setup or directly start with
more than two nodes for an ASCS / ERS high availability setup.
The examples below will show the setting up of Multi-node cluster and the extension of an existing two node cluster pair. The major configuration changes
will be shown and the basic preparation of the new cluster member node.</p><p>The task list to set up the three node cluster is similar to the task list of the two node cluster. But some
details are described different here to get a diskless SBD setup.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-extend-an-existing-two-node-cluster-configuration"></a>Extend An Existing Two Node Cluster Configuration</h3></div></div></div><div class="orderedlist"><p class="title"><strong>Tasks</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p>Backup the current cluster</p></li><li class="listitem"><p>OS installation new node</p></li><li class="listitem"><p>Patching the existing nodes</p></li><li class="listitem"><p>Preparing the New Node’s Operating System</p></li><li class="listitem"><p>SAP preparation</p></li><li class="listitem"><p>Installing the cluster software on the new node</p></li><li class="listitem"><p>joining the cluster</p></li><li class="listitem"><p>Test the new cluster configuration</p></li></ol></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-backup-the-current-cluster"></a>Backup the Current Cluster</h4></div></div></div><p>Backup the current. cluster</p><div class="informalexample"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Backup your system</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>cluster configuration</p></li><li class="listitem"><p>corosync.conf</p></li><li class="listitem"><p>all data and configuration which are important and customized and not default</p></li></ul></div></li></ul></div><p>The system is configured as described in the SUSE Best Practices document
<span class="emphasis"><em>SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide</em></span>.</p><p><strong>Backing Up the Cluster Configuration. </strong>Go to one of the cluster nodes and safe the cluster configuration with <span class="emphasis"><em>crm</em></span> and <span class="emphasis"><em>crm report</em></span> commands:</p><pre class="screen"># crm configure save 2node.txt</pre><pre class="screen"># crm report</pre><p>Back up the existing /etc/corosync/corosync.conf and all other files which may be important for a restore.
This example is one method creating a backup. The important point is using an <span class="strong"><strong>external</strong></span> destination.</p><pre class="screen"># tar cvf /&lt;path to an <span class="strong"><strong>external</strong></span> storage&gt;/bck_2nodes_configuration.tar \
      /etc/corosync/corosync.conf \
      2node.txt \
      &lt;crm report files&gt;
      &lt;add your additional files here&gt;</pre></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-os-installation-of-the-new-cluster-node-member"></a>OS Installation of The New Cluster Node Member</h4></div></div></div><p>We recommend using automating the installation to ensure that the system setup across nodes is identical.
Make sure to document any additional steps you take beyond the automated setup. In our example, we
deploy our machines with an AutoYaST configuration file and run a post step script which does the basic
configuration.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-patching-existing-nodes"></a>Patching Existing Nodes</h4></div></div></div><p>If applicable, install the latest updates and patches on your existing nodes.
Alternatively, if you are using frozen repositories such as those provided by SUSE Manager,
add the new system to the same repositories, so they have the same patch level as your
existing nodes.</p><p>Use <span class="strong"><strong>zypper patch</strong></span> or <span class="strong"><strong>zypper update</strong></span> depending on your company’s rules.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>It is recommended installing the latest available patches to guarantee system stability and hardening.
Bug fixing and security patches help avoid unplanned outages and make the system less vulnerable.</p></li></ul></div><p>There are multiple ways:</p><pre class="screen"># zypper patch --category security
<span class="marked"> or
# zypper patch --severity important
</span> or
# zypper patch
## or
# zypper update</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-preparing-the-new-nodes-operating-system"></a>Preparing the New Node’s Operating System</h4></div></div></div><div class="example"><a id="id2235"></a><p class="title"><strong>Example 12. Check for valid DNS, time synchronization, network settings</strong></p><div class="example-contents"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Verify that DNS is working:</p><pre class="screen"># ping &lt;hostname&gt;</pre></li><li class="listitem"><p>Set up <span class="strong"><strong>NTP Client</strong></span> (this is best done with <span class="strong"><strong>yast2</strong></span>) and enable it:</p><pre class="screen"># yast ntp-client</pre></li><li class="listitem"><p>Check the network settings:</p><pre class="screen"># ip r</pre></li></ul></div></div></div><br class="example-break" /><p>You may run into trouble if there is no valid or no default route configured.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Patch the new system like the existing nodes, verify that your systems have the same patch level and
that all required reboots have been performed.</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-installing-cluster-software-on-the-new-node"></a>Installing Cluster Software on the New Node</h4></div></div></div><div class="example"><a id="id2255"></a><p class="title"><strong>Example 13. Installing packages</strong></p><div class="example-contents"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Install the pattern <span class="strong"><strong>ha_sles</strong></span> on the new cluster node:</p><pre class="screen"># zypper in -t pattern ha_sles</pre></li><li class="listitem"><p>Install the package <span class="strong"><strong>sap-suse-cluster-connector</strong></span> version 3.1.0 from the SUSE repositories:</p><pre class="screen"># zypper in sap-suse-cluster-connector</pre></li></ul></div></div></div><br class="example-break" /><p>After installing all necessary packages, compare installed package versions.</p><div class="example"><a id="id2267"></a><p class="title"><strong>Example 14. Check that all nodes have the same software packages and versions</strong></p><div class="example-contents"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>On the existing cluster nodes:</p><pre class="screen"># rpm -qa | sort &gt;rpm_valuga11.log</pre></li><li class="listitem"><p>On the new node:</p><pre class="screen"># rpm -qa | sort &gt;rpm_valuga13.log</pre></li><li class="listitem"><p>Copy the file from one node to the other and compare the two versions:</p><pre class="screen"># vimdiff rpm_valuga13.log rpm_valuga11.log</pre></li></ul></div></div></div><br class="example-break" /><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>If there any differences fix them first before you proceed.</p></div><div class="example"><a id="id2281"></a><p class="title"><strong>Example 15. Install and configure the watchdog device on the new machine.</strong></p><div class="example-contents"><p>Instead of deploying the software-based solution, preferably use a hardware-based watchdog device.
The following example uses the software device but can be easily adapted to the hardware device.</p><pre class="screen"># modprobe softdog
# echo softdog &gt; /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"</pre></div></div><br class="example-break" /><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Ensure that the new node is connected to the same SBD disk connected to the existing two nodes. Ensure that the new node has the same exact SBD configurations already exist on the two existing nodes</p></div><pre class="screen"># sbd -d /dev/disk/by-id/SUSE-Example-A-part1 dump</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-preparing-the-sap-installation-on-the-new-node"></a>Preparing the SAP Installation on the New Node</h4></div></div></div><p>With SWPM 2.0 (SP4 or later), which is part of the SL Toolset, SAP provides a new option which can perform all necessary steps to prepare a fresh
install server to be able to fit into an existing SAP system. This new option will help us to prepare a
new host which can later run either the ASCS or ERS in the cluster environment.</p><p>You need to create the directory structure that should run the SAP resource.
The SYS directory is located on an NFS share for all nodes.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Create mount points and mount NFS shares on the new added node (valuga13):</p></li></ul></div><div class="example"><a id="id2295"></a><p class="title"><strong>Example 16. Mount NFS Shares on valuga13</strong></p><div class="example-contents"><pre class="screen"># mkdir -p /sapmnt /var/lib/Landscape
# mkdir -p /usr/sap/EN2/{ASCS00,ERS10,SYS}
# mount -t nfs 192.168.1.1:/data/export/S4_HA_CLU_10/EN2/sapmnt    /sapmnt
# mount -t nfs 192.168.1.1:/data/export/S4_HA_CLU_10/EN2/SYS /usr/sap/EN2/SYS
# mount -t nfs 192.168.1.1:/Landscape /var/lib/Landscape</pre></div></div><br class="example-break" /><p>As this directory needs to be available at all time, make sure it is mounted during boot. This can be
achieved by put the information into the /etc/fstab.</p><p>In the next step the following information are required:
- profile directory
- password for SAP System Administrator
- UID for sapadm</p><pre class="screen"># cd /var/lib/Landscape/media/SAP-media/SWPM20_P9/
# ./sapinst</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>SWPM product installation path:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Installing SAP S/4HANA Server 1809  → SAP HANA DATABASE → Installation → Application Server ABAP
→ High-Availability System → Prepare Additional Host</p></li></ul></div></li><li class="listitem"><p>Use /sapmnt/EN2/profile for the profile directory</p></li><li class="listitem"><p>All passwords: &lt;use-your-secure-pwd&gt;</p></li><li class="listitem"><p>UID for sapadm: 2002</p></li></ul></div><div class="example"><a id="id2313"></a><p class="title"><strong>Example 17. Post Step Procedure after SAP Preparation Step</strong></p><div class="example-contents"><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Add the user <span class="emphasis"><em>en2adm</em></span> to the unix user group <span class="emphasis"><em>haclient</em></span>.</p><pre class="screen"># usermod -a -G haclient en2adm</pre></li><li class="listitem"><p>Create the file <span class="emphasis"><em>/usr/sap/sapservices</em></span>  or copy it from one of your existing cluster nodes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This must be done for each instance. Call sapstartsrv with parameters <span class="strong"><strong>pf=</strong></span>&lt;profile-of-the-sap-instance&gt; and <span class="strong"><strong>-reg</strong></span>.</p></div><p>The following commands register the ASCS and the ERS SAP instance:</p><pre class="screen">##  create them with sapstartsrv
# LD_LIBRARY_PATH=/usr/sap/hostctrl/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH
# /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/&lt;SID&gt;/SYS/profile/&lt;SID&gt;_ERS&lt;instanceNumberErs&gt;_&lt;virtHostNameErs&gt; -reg
# /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/&lt;SID&gt;/SYS/profile/&lt;SID&gt;_ASCS&lt;instanceNumberAscs&gt;_&lt;virtHostNameAscs&gt; -reg</pre></li><li class="listitem"><p>Alternatively copy the file</p><pre class="screen"># scp -p valuga11:/usr/sap/sapservices /usr/sap/</pre></li><li class="listitem"><p>Finally check the file</p><pre class="screen"># cat /usr/sap/sapservices
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/EN2/ASCS00/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/EN2/ASCS00/exe/sapstartsrv pf=/usr/sap/EN2/SYS/profile/EN2_ASCS00_sapen2as -D -u en2adm
LD_LIBRARY_PATH=/usr/sap/EN2/ERS10/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/EN2/ERS10/exe/sapstartsrv pf=/usr/sap/EN2/SYS/profile/EN2_ERS10_sapen2er -D -u en2adm</pre></li></ul></div></div></div><br class="example-break" /><div class="example"><a id="id2336"></a><p class="title"><strong>Example 18. Add the New Node to the Cluster</strong></p><div class="example-contents"><p>Check if the SBD device is available in case the SBD stonith method is in place for the two nodes.
If the existing cluster using a different, supported stonith mechanism check and verify them too for the
new cluster node.</p><pre class="screen"># sbd -d /dev/disk/by-id/SUSE-Example-A-part1 dump</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Joining the cluster with <span class="emphasis"><em>ha-cluster-join</em></span></p></li></ul></div><pre class="screen"># ha-cluster-join -c valuga11</pre><p>After the new node has joined into the cluster the configuration must be adapted to the new situation.
Double check if the join was successful and verify the <span class="emphasis"><em>/etc/corosync/corosync.conf</em></span></p><pre class="screen"># grep votes -n2 /etc/corosync/corosync.conf</pre><p>The values <span class="strong"><strong>expected_votes</strong></span> and <span class="strong"><strong>two_node</strong></span> should now look like this on all nodes:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>expected_votes: 3</p></li><li class="listitem"><p>two_node: 0</p></li></ul></div><p>Modify the cluster configuration and set a new colocation rule with <span class="emphasis"><em>crm</em></span>:</p><pre class="screen"># crm configure delete col_sap_EN2_no_both
# crm configure colocation ASCS00_ERS10_separated_EN2 -5000: grp_EN2_ERS10 grp_EN2_ASCS00</pre></div></div><br class="example-break" /></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-test-the-new-cluster-configuration"></a>Test the New Cluster Configuration</h4></div></div></div><p>It is highly recommended to run certain test to verify that the new configuration is working as expected.
A list of test can be found in the basic setup for the two node cluster above.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-pros-and-cons-for-odd-and-even-numbers-of-cluster-nodes"></a>Pro’s and Con’s for Odd and Even Numbers of Cluster Nodes</h3></div></div></div><p>There are certain use cases and infrastructure requirements which end up in different installation setup’s.
We will cover some advantages and disadvantages of special configuration below:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The two node cluster and two locations</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Advantage: symmetric spread of all nodes over all locations</p></li><li class="listitem"><p>Disadvantage: no diskless SBD feature allowed for all two node clusters</p></li></ul></div></li><li class="listitem"><p>The two node cluster and more than two locations</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Advantage: SBD device can be provided from there (must be HA himself)</p></li><li class="listitem"><p>Advantage: cluster could operate with three SBD devices from different locations</p></li><li class="listitem"><p>Disadvantage: no diskless SBD feature allowed for all two node clusters</p></li></ul></div></li><li class="listitem"><p>The three node cluster and two locations</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Advantage: less complex infrastructure</p></li><li class="listitem"><p>Advantage: diskless SBD feature is allowed</p></li><li class="listitem"><p>Disadvantage: "pre selected" location (two node + one node)</p></li></ul></div></li><li class="listitem"><p>The three node cluster and three locations</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>Advantage: symmetric spread of all nodes over all locations</p></li><li class="listitem"><p>Advantage: diskless SBD feature is allowed</p></li><li class="listitem"><p>Disadvantage: higher planing effort and complexity for infrastructure planning</p></li></ul></div></li></ul></div></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="id-administration.html">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="id-references.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Administration </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> References</td></tr></table></div></body></html>
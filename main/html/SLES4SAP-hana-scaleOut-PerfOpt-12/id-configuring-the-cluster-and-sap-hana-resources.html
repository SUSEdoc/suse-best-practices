<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Configuring the cluster and SAP HANA resources</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.2" /><style type="text/css">
body { background-image: url('static/images/draft.png');
       background-repeat: no-repeat;
       background-position: top left;
       /* The following properties make the watermark "fixed" on the page. */
       /* I think that's just a bit too distracting for the reader... */
       /* background-attachment: fixed; */
       /* background-position: center center; */
     }</style><link rel="home" href="index.html" title="SAP HANA System Replication Scale-Out - Performance Optimized Scenario" /><link rel="up" href="index.html" title="SAP HANA System Replication Scale-Out - Performance Optimized Scenario" /><link rel="prev" href="id-integrating-sap-hana-with-the-cluster.html" title="Integrating SAP HANA with the cluster" /><link rel="next" href="id-testing-the-cluster.html" title="Testing the cluster" /></head><body onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Configuring the cluster and SAP HANA resources</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="id-integrating-sap-hana-with-the-cluster.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="id-testing-the-cluster.html">Next</a></td></tr></table><hr /></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id-configuring-the-cluster-and-sap-hana-resources"></a>Configuring the cluster and SAP HANA resources</h2></div></div></div><p><a id="Cluster"></a>This chapter describes the configuration of the SUSE Linux Enterprise High Availability cluster. The SUSE Linux Enterprise High Availability Extension is part of SUSE Linux Enterprise Server for SAP Applications.
Further, the integration of SAP HANA System Replication with the SUSE Linux Enterprise High Availability cluster is explained. The integration is done
by using the SAPHanaSR-ScaleOut package which is also part of SUSE Linux Enterprise Server for SAP Applications.</p><div class="figure"><a id="id2323"></a><p class="title"><strong>Figure 12. <a class="xref" href="id-planning-the-installation.html#Planning">[Planning]</a> <a class="xref" href="id-setting-up-the-operating-system.html#OsSetup">[OsSetup]</a> <a class="xref" href="id-installing-the-sap-hana-databases-on-both-sites.html#SAPHanaInst">[SAPHanaInst]</a> <a class="xref" href="id-setting-up-the-sap-hana-system-replication.html#SAPHanaHsr">[SAPHanaHsr]</a> <a class="xref" href="id-integrating-sap-hana-with-the-cluster.html#Integration">[Integration]</a> Cluster <a class="xref" href="id-testing-the-cluster.html#Testing">[Testing]</a></strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td><img src="images/SAPHanaSR-ScaleOut-Plan-Phase6.svg" width="100%" alt="SAPHanaSR ScaleOut Plan Phase6" /></td></tr></table></div></div></div><br class="figure-break" /><p><span class="strong"><strong>Procedure</strong></span></p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Installation of cluster packages</p></li><li class="listitem"><p>Basic Cluster Configuration</p></li><li class="listitem"><p>Configure Cluster Properties and Resources</p></li><li class="listitem"><p>Final steps</p></li></ol></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-installing-the-cluster-packages"></a>Installing the cluster packages</h3></div></div></div><p>If not already done, install the pattern <span class="emphasis"><em>High Availability</em></span> on <span class="strong"><strong>all</strong></span> nodes.</p><p>To do so, use Zypper.</p><pre class="screen"># zypper in -t pattern ha_sles</pre><p>Now the Resource Agents for controlling the SAP HANA system replication need
to be installed at <span class="strong"><strong>all</strong></span> cluster nodes, including the majority maker.</p><pre class="screen"># zypper in SAPHanaSR-ScaleOut</pre><p>If you have the packages installed before, make sure to get the newest updates
on <span class="strong"><strong>all</strong></span> nodes</p><pre class="screen">zypper patch</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-configuring-the-basic-cluster"></a>Configuring the basic cluster</h3></div></div></div><p>After having installed the cluster packages, the next step is to set up the basic cluster framework. For convenience, use
YaST or the <span class="emphasis"><em>ha-cluster-init</em></span> script.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>It is strongly recommended to add a second corosync ring, implement unicast (UCAST)
communication and adjust the timeout values to your environment.</p></div><p><span class="strong"><strong>Prerequisites</strong></span></p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Name resolution</p></li><li class="listitem"><p>Time synchronization</p></li><li class="listitem"><p>Redundant network for cluster intercommunication</p></li><li class="listitem"><p>STONITH method</p></li></ul></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-setting-up-watchdog-for-storage-based-fencing"></a>Setting up watchdog for "Storage-based Fencing"</h4></div></div></div><p>It is recommended to use Storage-based Fencing (SBD) as central STONITH device, as done in the example at hand. Each node constantly monitors
connectivity to the storage device, and terminates itself in case the partition becomes unreachable.
Whenever SBD is used, a
correctly working watchdog is crucial. Modern systems support a hardware watchdog that needs to
be "tickled" or "fed" by a software component. The software component (usually a daemon) regularly
writes a service pulse to the watchdog. If the daemon stops feeding the watchdog, the hardware will
enforce a system restart. This protects against failures of the SBD process itself, such as dying, or
getting stuck on an I/O error.</p><div class="example"><a id="id2380"></a><p class="title"><strong>Example 28. Set up for Watchdog</strong></p><div class="example-contents"><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Access to the watchdog timer:
No other software must access the watchdog timer. Some hardware vendors ship systems management
software that uses the watchdog for system resets (for example, HP ASR daemon). Disable such
software, if watchdog is used by SBD.</p></div><p>Determine the right watchdog module. Alternatively, you can find a list of installed drivers with your
kernel version.</p><pre class="screen"># ls -l /lib/modules/$(uname -r)/kernel/drivers/watchdog</pre><p>Check if any watchdog module is already loaded.</p><pre class="screen"># lsmod | egrep "(wd|dog|i6|iT|ibm)"</pre><p>If you get a result, the system has already a loaded watchdog. If the watchdog does not match
your watchdog device, you need to unload the module.</p><p>To safely unload the module, check first if an application is using the watchdog device.</p><pre class="screen"># lsof /dev/watchdog
# rmmod &lt;wrong_module&gt;</pre><p>Enable your watchdog module and make it persistent. For the example below, <span class="emphasis"><em>softdog</em></span> has been used which has some
restrictions and should not be used as first option.</p><pre class="screen"># echo softdog &gt; /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load</pre><p>Check if the watchdog module is loaded correctly.</p><pre class="screen"># lsmod | grep dog</pre><p>Testing the watchdog can be done with a simple action. Ensure to switch of your SAP HANA
first because watchdog will force an unclean reset / shutdown of your system.</p><p>In case of a hardware watchdog a desired action is predefined after the timeout of the watchdog has
reached. If your watchdog module is loaded and not controlled by any other application, do the following:</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Triggering the watchdog without continuously updating the watchdog resets/switches off the system.
      This is the intended mechanism. The following commands will force your system to be reset/switched off.</p></div><pre class="screen"># touch /dev/watchdog</pre><p>In case the softdog module is used the following action can be performed:</p><pre class="screen"># echo 1&gt; /dev/watchdog</pre><p>After your test was successful you can implement the watchdog on all cluster members. The example below
applies to the softdog module. Replace <span class="strong"><strong>&lt;wrong_module&gt;</strong></span> by the module name queried before.</p><pre class="screen"># for i in suse{02,03,04,05,06,-mm}; do
    ssh -T $i &lt;&lt;EOSSH
        hostname
        rmmod &lt;wrong_module&gt;
        echo softdog &gt; /etc/modules-load.d/watchdog.conf
        systemctl restart systemd-modules-load
        lsmod |grep -e dog
EOSSH
done</pre></div></div><br class="example-break" /></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-setting-up-the-initial-cluster-using-ha-cluster-init"></a>Setting up the initial cluster using <span class="emphasis"><em>ha-cluster-init</em></span></h4></div></div></div><p>For more detailed information about ha-cluster-* tools, see section <span class="emphasis"><em>Overview of the Bootstrap Scripts</em></span> of the Installation and Setup Quick Start Guide
for SUSE Linux Enterprise High Availability Extension at <a class="link" href="https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-install-quick/art-ha-install-quick.html#sec-ha-inst-quick-bootstrap" target="_top">https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-install-quick/art-ha-install-quick.html#sec-ha-inst-quick-bootstrap</a></p><p>Create an initial setup by using <span class="emphasis"><em>ha-cluster-init</em></span> command. Follow the dialog
steps.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This is <span class="strong"><strong>only</strong></span> to be done on the <span class="strong"><strong>first</strong></span> cluster node. If you are using
SBD as STONITH mechanism, you need to first load the watchdog kernel module
matching your setup. In the example at hand the <span class="emphasis"><em>softdog</em></span> kernel module is used.</p></div><p>The command <span class="emphasis"><em>ha_cluster-init</em></span> configures the basic cluster framework including:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>SSH keys</p></li><li class="listitem"><p>csync2 to transfer configuration files</p></li><li class="listitem"><p>SBD (at least one device)</p></li><li class="listitem"><p>corosync (at least one ring)</p></li><li class="listitem"><p>HAWK Web interface</p></li></ul></div><pre class="screen"># ha-cluster-init -u -s &lt;sbd-device&gt;</pre><p>As requested by <span class="emphasis"><em>ha-cluster-init</em></span>, change the password of the user <span class="emphasis"><em>hacluster</em></span> on all cluster nodes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Do not forget to change the password of the user <span class="emphasis"><em>hacluster</em></span>.</p></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-configuring-the-cluster-for-all-other-cluster-nodes"></a>Configuring the cluster for all other cluster nodes</h4></div></div></div><p>The other nodes of the cluster could be integrated by starting the
command <span class="emphasis"><em>ha-cluster-join</em></span>. This command asks for the IP address or name of
the <span class="strong"><strong>first</strong></span> cluster node. Then all needed configuration files are copied over.
As a result the cluster is started on <span class="strong"><strong>all</strong></span> nodes. Do not forget the majority maker.</p><p>If you are using SBD as STONITH method, you need to activate the <span class="emphasis"><em>softdog</em></span> kernel
module matching your systems. In the example at hand the <span class="emphasis"><em>softdog</em></span> kernel module is used.</p><pre class="screen"># ha-cluster-join -c &lt;host1&gt;</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-checking-the-cluster-for-the-first-time"></a>Checking the cluster for the first time</h4></div></div></div><p>Now it is time to check and optionally start the cluster for the first time on
all nodes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>All nodes should be started in parallel. Otherwise unseen nodes might get fenced.</p></div><p>Check the cluster status with <code class="literal">crm_mon</code>. Use the option <code class="literal">-r</code> to also see
resources which are configured but stopped.</p><pre class="screen"># crm_mon -r</pre><p>The command will show the "empty" cluster and will print something like the
screen output below. The most interesting information in this output is that
there are seven nodes in the status "online" and the message "partition with quorum".</p><pre class="screen">Stack: corosync
Current DC: suse05 (version 1.1.16-4.8-77ea74d) - partition with quorum
Last updated: Mon Jun 11 16:55:04 2018
Last change: Mon Jun 11 16:53:58 2018 by root via crm_attribute on suse02

7 nodes configured
1 resource configured

Online: [ suse-mm suse01 suse02 suse03 suse04 suse05 suse06 ]

Full list of resources:

stonith-sbd     (stonith:external/sbd): Started suse-mm</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-configuring-cluster-properties-and-resources"></a>Configuring cluster properties and resources</h3></div></div></div><p>This section describes how to configure bootstrap, STONITH, resources, and constraints
using the <span class="emphasis"><em>crm</em></span> configure shell command as described in section
<span class="emphasis"><em>Configuring and Managing Cluster Resources (Command Line)</em></span> of the
SUSE Linux Enterprise High Availability Administration Guide (see <a class="link" href="https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-all/cha-ha-manual-config.html" target="_top">https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-all/cha-ha-manual-config.html</a>).</p><p>Use the command <span class="emphasis"><em>crm</em></span> to add the objects to the Cluster Resource Management
(CRM). Copy the following examples to a local file and then load the
configuration to the Cluster Information Base (CIB). The benefit is that
you have a scripted setup and a backup of your configuration.</p><p>Perform all <span class="emphasis"><em>crm</em></span> commands only on <span class="strong"><strong>one</strong></span> node, for example on machine suse01.</p><p>First write a text file with the configuration, which you load into your cluster
in a second step. This step is as follows:</p><pre class="screen"># vi crm-file&lt;XX&gt;
# crm configure load update crm-file&lt;XX&gt;</pre><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-cluster-bootstrap-and-more"></a>Cluster bootstrap and more</h4></div></div></div><p>The first example defines the cluster bootstrap options including the resource and
operation defaults.</p><p>The <code class="literal">stonith-timeout</code> should be greater than 1.2 times the SBD <code class="literal">msgwait</code> timeout.
Find more details and examples in manual page SAPHanaSR-ScaleOut_basic_cluster(7).</p><pre class="screen"># vi crm-bs.txt</pre><p>Enter the following to <span class="emphasis"><em>crm-bs.txt</em></span>:</p><pre class="screen">property $id="cib-bootstrap-options" \
              no-quorum-policy="freeze" \
              stonith-enabled="true" \
              concurrent-fencing="true" \
              stonith-action="reboot" \
              stonith-timeout="150s"
rsc_defaults $id="rsc-options" \
              resource-stickiness="1000" \
              migration-threshold="50"
op_defaults $id="op-options" \
              timeout="600"</pre><p>Now add the configuration to the cluster.</p><pre class="screen">crm configure load update crm-bs.txt</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-stonith"></a>STONITH</h4></div></div></div><p>As already explained in the requirements, STONITH is crucial for a supported cluster setup.
Without a valid fencing mechanism your cluster is unsupported.</p><p>A standard STONITH mechanism implements SBD based fencing. The SBD STONITH method is
very stable and reliable and has proved very good road capability.</p><p>You can use other fencing methods available for example from your public cloud provider.
However, it is crucial to intensively test the server fencing.</p><p>For SBD based fencing you can use one up to three SBD devices. The cluster will react differently
when an SBD device is lost. The differences and SBD fencing are explained very well in the SUSE
product documentation of the SUSE Linux Enterprise High Availability Extension available at <a class="link" href="https://documentation.suse.com/" target="_top">https://documentation.suse.com/</a>.</p><p>You need to adapt the SBD resource for the SAP HANA scale-out cluster.</p><p>As user &lt;sid&gt;adm create a file named for crm-fencing.txt.</p><div class="example"><a id="id2494"></a><p class="title"><strong>Example 29. Configure fencing</strong></p><div class="example-contents"><pre class="screen"># vi crm-fencing.txt</pre><p>Enter the following to <span class="emphasis"><em>crm-fencing.txt</em></span>:</p><pre class="screen">primitive stonith-sbd stonith:external/sbd \
	params pcmk_action_limit=-1 pcmk_delay_max=1</pre><p>Now load the configuration from the file to the cluster.</p><pre class="screen"># crm configure load update crm-fencing.txt</pre></div></div><br class="example-break" /></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-cluster-in-maintenance-mode"></a>Cluster in maintenance mode</h4></div></div></div><p>Load the configuration for the resources and the constraints
step-by-step to the cluster to explain the different parts. The
best way to avoid unexpected cluster reactions is to</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>first set the complete cluster to maintenance mode,</p></li><li class="listitem"><p>then do all needed changes and,</p></li><li class="listitem"><p>as last step, end the cluster maintenance mode.</p></li></ul></div><pre class="screen"># crm configure property maintenance-mode=true</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-saphanatopology"></a>SAPHanaTopology</h4></div></div></div><p>Next, define the group of resources needed, before the SAP HANA instances can be
started. Prepare the changes in a text file, for example <span class="emphasis"><em>crm-saphanatop.txt</em></span>,
and load these with the <span class="emphasis"><em>crm</em></span> command.</p><p>If necessary, change the <span class="strong"><strong>SID</strong></span> and <span class="strong"><strong>instance number</strong></span> (bold) to appropriate
values for your setup.</p><div class="example"><a id="id2521"></a><p class="title"><strong>Example 30. Configure SAPHanaTopology</strong></p><div class="example-contents"><pre class="screen">suse01:~ # vi crm-saphanatop.txt</pre><p>Enter the following to <span class="emphasis"><em>crm-saphanatop.txt</em></span>:</p><pre class="screen">primitive rsc_SAPHanaTop_&lt;SID&gt;_HDB&lt;Inst&gt; ocf:suse:SAPHanaTopology \
    op monitor interval="10" timeout="600" \
    op start interval="0" timeout="600" \
    op stop interval="0" timeout="300" \
    params SID="<span class="strong"><strong>&lt;SID&gt;</strong></span>" InstanceNumber="<span class="strong"><strong>&lt;Inst&gt;</strong></span>"

clone cln_SAPHanaTop_&lt;SID&gt;_HDB&lt;Inst&gt; rsc_SAPHanaTop_&lt;SID&gt;_HDB&lt;Inst&gt; \
    meta clone-node-max="1" interleave="true"</pre><pre class="screen">primitive rsc_SAPHanaTop_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> ocf:suse:SAPHanaTopology \
    op monitor interval="10" timeout="600" \
    op start interval="0" timeout="600" \
    op stop interval="0" timeout="300" \
    params SID="<span class="strong"><strong>HA1</strong></span>" InstanceNumber="<span class="strong"><strong>00</strong></span>"

clone cln_SAPHanaTop_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> rsc_SAPHanaTop_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> \
    meta clone-node-max="1" interleave="true"</pre><p>For additional information about all parameters, use the command
<code class="literal">man ocf_suse_SAPHanaTopology</code>.</p><p>Again, add the configuration to the cluster.</p><pre class="screen"># crm configure load update crm-saphanatop.txt</pre></div></div><br class="example-break" /><p>The most important parameters here are <span class="emphasis"><em>SID</em></span> (HA1) and <span class="emphasis"><em>InstanceNumber</em></span> (00),
which are self explaining in an SAP context.</p><p>Beside these parameters, the timeout values or the operations (start, monitor,
stop) are typical values to be adjusted for your environment.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-saphanacontroller"></a>SAPHanaController</h4></div></div></div><p>Next, define the group of resources needed, before the SAP HANA instances can be
started. Edit the changes in a text file, for example <code class="literal">crm-saphanacon.txt</code> and
load these with the command <code class="literal">crm</code>.</p><pre class="screen"># vi crm-saphanacon.txt</pre><div class="example"><a id="id2552"></a><p class="title"><strong>Example 31. Configure SAPHanaController</strong></p><div class="example-contents"><p>Enter the following to crm-saphanacon.txt</p><pre class="screen">primitive rsc_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt; ocf:suse:SAPHanaController \
    op start interval="0" timeout="3600" \
    op stop interval="0" timeout="3600" \
    op promote interval="0" timeout="3600" \
    op monitor interval="60" role="Master" timeout="700" \
    op monitor interval="61" role="Slave" timeout="700" \
    params SID="&lt;SID&gt;" InstanceNumber="&lt;Inst&gt;" \
        PREFER_SITE_TAKEOVER="true" \
        DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"

ms msl_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt; rsc_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt; \
        meta clone-node-max="1" master-max="1" interleave="true"</pre><p>The most important parameters here are &lt;SID&gt; (HA1) and &lt;Inst&gt;
(00), which are in the SAP context quite self explaining.
Beside these parameters, the timeout values or the operations (start, monitor,
stop) are typical tuneables.</p><pre class="screen">primitive rsc_SAPHanaCon_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> ocf:suse:SAPHanaController \
    op start interval="0" timeout="3600" \
    op stop interval="0" timeout="3600" \
    op promote interval="0" timeout="3600" \
    op monitor interval="60" role="Master" timeout="700" \
    op monitor interval="61" role="Slave" timeout="700" \
    params SID="<span class="strong"><strong>HA1</strong></span>" InstanceNumber="<span class="strong"><strong>00</strong></span>" PREFER_SITE_TAKEOVER="true" \
        DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"

ms msl_SAPHanaCon_HA1_HDB00 rsc_SAPHanaCon_HA1_HDB00 \
    meta clone-node-max="1" master-max="1" interleave="true"</pre><p>Add the configuration to the cluster.</p><pre class="screen"># crm configure load update crm-saphanacon.txt</pre></div></div><br class="example-break" /><div class="table"><a id="id2564"></a><p class="title"><strong>Table 2. Table Description of important Resource Agent parameter</strong></p><div class="table-contents"><table class="table" summary="Table Description of important Resource Agent parameter" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="left" valign="top">Name</th><th align="left" valign="top">Description</th></tr></thead><tbody><tr><td align="left" valign="top"><p>PREFER_SITE_TAKEOVER</p></td><td align="left" valign="top"><p>Defines whether RA should prefer to takeover to the secondary instance instead
of restarting the failed primary locally. Set to <span class="strong"><strong>true</strong></span> for SAPHanaSR-ScaleOut.</p></td></tr><tr><td align="left" valign="top"><p>AUTOMATED_REGISTER</p></td><td align="left" valign="top"><p>Defines whether a former primary should be automatically registered to be
secondary of the new primary. Defaults to <span class="strong"><strong>false</strong></span>.</p><p>If set to <span class="strong"><strong>false</strong></span>, the former primary must be manually registered. The cluster
will not start this SAP HANA RDBMS until it is registered to avoid double
primary up situations.</p></td></tr><tr><td align="left" valign="top"><p>DUPLICATE_PRIMARY_TIMEOUT</p></td><td align="left" valign="top"><p>Time difference needed between two primary time stamps if a dual-primary
situation occurs. If the time difference is less than the time gap, the
cluster holds one or both sites in a "WAITING" status.
This is to give an administrator the chance to react on a failover. If the complete node
of the former primary crashed, the former primary will be registered after the
time difference is passed. If "only" the SAP HANA RDBMS has crashed, then the
former primary will be registered immediately. After this registration to the
new primary, all data will be overwritten by the system replication.</p></td></tr></tbody></table></div></div><br class="table-break" /><p>Additional information about all parameters can be found with the command
<code class="literal">man ocf_suse_SAPHanaController</code>.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-the-virtual-ip-address"></a>The virtual IP address</h4></div></div></div><p>The last resource to be added to the cluster is covering the virtual IP address.
Replace the bold string with your instance number, SAP HANA system ID and the
virtual IP address.</p><div class="example"><a id="id2598"></a><p class="title"><strong>Example 32. Configure the IP Address</strong></p><div class="example-contents"><pre class="screen"># vi crm-vip.txt</pre><p>Enter the following to <span class="emphasis"><em>crm-vip.txt</em></span>:</p><pre class="screen">primitive rsc_ip_&lt;SID&gt;_HDB&lt;Inst&gt; ocf:heartbeat:IPaddr2 \
    op monitor interval="10s" timeout="20s" \
    params ip="&lt;IP&gt;"</pre><pre class="screen">primitive rsc_ip_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> ocf:heartbeat:IPaddr2 \
    op monitor interval="10s" timeout="20s" \
    params ip="<span class="strong"><strong>192.168.201.109</strong></span>"</pre><p>Load the file to the cluster.</p><pre class="screen"># crm configure load update crm-vip.txt</pre></div></div><br class="example-break" /><p>In most installations, only the parameter <span class="strong"><strong>ip</strong></span> needs to be set to the virtual
IP address to be presented to the client systems.
Use the command <code class="literal">man ocf_heartbeat_IPAddr2</code> for details on additional parameters.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-constraints"></a>Constraints</h4></div></div></div><p>The constraints are organizing the correct placement of the virtual IP
address for the client database access and the start order between the two
resource agents SAPHana and SAPHanaTopology. The rules help to remove false
positive messages from the <span class="emphasis"><em>crm_mon</em></span> command.</p><div class="example"><a id="id2617"></a><p class="title"><strong>Example 33. Configure needed constraints</strong></p><div class="example-contents"><pre class="screen"># vi crm-cs.txt</pre><p>Enter the following to <span class="emphasis"><em>crm-cs.txt</em></span>:</p><pre class="screen">colocation col_saphana_ip_&lt;SID&gt;_HDB&lt;Inst&gt; 2000: rsc_ip_&lt;SID&gt;_HDB&lt;Inst&gt;:Started \
    msl_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt;:Master

order ord_SAPHana_&lt;SID&gt;_HDB&lt;Inst&gt; Optional: cln_SAPHanaTop_&lt;SID&gt;_HDB&lt;Inst&gt; \
    msl_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt;

location SAPHanaCon_not_on_majority_maker msl_SAPHanaCon_&lt;SID&gt;_HDB&lt;Inst&gt; -inf: &lt;majority maker&gt;
location SAPHanaTop_not_on_majority_maker cln_SAPHanaTop_&lt;SID&gt;_HDB&lt;Inst&gt; -inf: &lt;majority maker&gt;</pre><pre class="screen">colocation col_saphana_ip_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> 2000: rsc_ip_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span>:Started \
    msl_SAPHanaCon_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span>:Master

order ord_SAPHana_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> Optional: cln_SAPHanaTop_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> \
    msl_SAPHanaCon_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span>

location SAPHanaCon_not_on_majority_maker msl_SAPHanaCon_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> -inf: <span class="strong"><strong>suse-mm</strong></span>
location SAPHanaTop_not_on_majority_maker cln_SAPHanaTop_<span class="strong"><strong>HA1</strong></span>_HDB<span class="strong"><strong>00</strong></span> -inf: <span class="strong"><strong>suse-mm</strong></span></pre><p>Load the file to the cluster.</p><pre class="screen"># configure load update crm-cs.txt</pre></div></div><br class="example-break" /></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id-final-steps"></a>Final Steps</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-verifying-the-communication-between-the-hook-and-the-cluster"></a>Verifying the communication between the hook and the cluster</h4></div></div></div><p>Now check if the HA/DR provider could set the appropriate
cluster attribute hana_&lt;sid&gt;_glob_srHook:</p><div class="example"><a id="id2649"></a><p class="title"><strong>Example 34. Query the srHook cluster attribute</strong></p><div class="example-contents"><pre class="screen"># crm_attribute -G  -n hana_&lt;sid&gt;_glob_srHook</pre><p>You should get an output similar to the following:</p><pre class="screen">scope=crm_config  name=hana_&lt;sid&gt;_glob_srHook value=SFAIL</pre></div></div><br class="example-break" /><p>In this case the HA/DR provider sets the attribute to SFAIL to inform the
cluster about a broken system replication.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-using-special-virtual-host-names-or-fqhn-during-the-installation-of-sap-hana"></a>Using special virtual host names or FQHN during the installation of SAP HANA</h4></div></div></div><p>If you have used special virtual host names or the fully qualified host name
(FQHN) instead of the short node name, the resource agents need to map these
names. To be able to match the short node name with the used SAP 'virtual
host name', the saphostagent needs to report the list of installed instances
correctly:</p><div class="example"><a id="id2658"></a><p class="title"><strong>Example 35. In the setup at hand the virtual host name matches the node name</strong></p><div class="example-contents"><pre class="screen"><span class="strong"><strong>suse01</strong></span>:ha1adm&gt; /usr/sap/hostctrl/exe/saphostctrl -function ListInstances
 Inst Info : HA1 - 00 - <span class="strong"><strong>suse01</strong></span> - 749, patch 418, changelist 1816226</pre></div></div><br class="example-break" /></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id-ending-the-cluster-maintenance-mode"></a>Ending the cluster maintenance mode</h4></div></div></div><p>After all changes, as last step end the cluster maintenance mode.</p><pre class="screen"># crm configure property maintenance-mode=false</pre></div></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="id-integrating-sap-hana-with-the-cluster.html">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="id-testing-the-cluster.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Integrating SAP HANA with the cluster </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Testing the cluster</td></tr></table></div></body></html>